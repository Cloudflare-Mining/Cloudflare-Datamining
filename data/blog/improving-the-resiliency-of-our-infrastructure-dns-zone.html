<div class="post-content lh-copy gray1">
	<p>In this blog post we will discuss how we made our infrastructure DNS zone more reliable by using multiple primary nameservers to leverage our own DNS product running on our edge as well as a third-party DNS provider.</p>
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2020/11/image2-16.png" class="kg-image"></figure>
	<h2 id="authoritative-nameservers">Authoritative Nameservers</h2>
	<p>You can think of an authoritative nameserver as the source of truth for the records of a given DNS zone. When a recursive resolver wants to look up a record, it will eventually need to talk to the authoritative nameserver(s) for the zone in question. If you’d like to read more on the topic, our learning center provides some <a href="https://www.cloudflare.com/learning/dns/dns-server-types/" target="_blank">additional information</a>.</p>
	<p>Here’s an example of our authoritative nameservers (replacing our actual domain with example.com):</p>
	<!--kg-card-begin: markdown-->
	<pre><code>~$ dig NS example.com +short
ns1.example.com.
ns2.example.com.
ns3.example.com.
</code></pre>
	<!--kg-card-end: markdown-->
	<p>As you can see, there are three nameservers listed. You’ll notice that the nameservers happen to reside in the same zone, but they don’t have to. Those three nameservers point to six anycasted IP addresses (3 x IPv4, 3 x IPv6) announced from our edge, comprising data centers from <a href="https://www.cloudflare.com/network" target="_blank">200+ cities</a> around the world.</p>
	<h2 id="the-problem">The Problem</h2>
	<p>We store the hostnames for all of our machines, both the ones at the edge and the ones at core data centers, in a DNS zone we refer to as our infrastructure zone. By using our own DNS product to run our infrastructure zone, we get the reliability and performance of our <a href="https://www.cloudflare.com/network/" target="_blank">Global Anycast Network</a>. However, what would happen if those nameservers became unavailable due to an issue on our edge? Eventually DNS lookups would fail, as resolvers would not be able to connect to the only authoritative nameservers configured on the zone: the ones hosted on our edge. This is the main problem we set out to solve.</p>
	<p>When an incident occurs, our engineering teams are busy investigating, debugging, and fixing the issue at hand. If they cannot resolve the hostnames of the infrastructure they need to use, it will lead to confusion and delays in resolving the incident.</p>
	<p>Imagine you are part of a team tackling an incident. As you quickly begin to investigate, you try to connect to a specific machine to gather some debugging information, but you get a DNS resolution error. What now? You know you are using the right hostname. Is this a result of the incident at hand, some side effect, or is this completely unrelated? You need to keep investigating, so you try another way. Maybe you have memorized the IP address of the machine and you can connect. More likely, you ask another resolver which still has the answer in its cache and resolve the IP that way. One thing is certain, the extra “mini-debug” step costs you precious time and detracts from debugging the real root cause.</p>
	<p>Unavailable nameservers and the problems they cause are not unique to Cloudflare, and are not specific to our use case. Even if we hosted our authoritative nameservers with a single external provider, they could also experience their own issues causing the nameservers to fail.</p>
	<p>Within the DNS community it is always understood that the more diversity, the better. Some common methods include diversifying network/routing configurations and software choices between a standard primary/secondary setup. However, no matter how resilient the network or software stack is, a single authority is still responsible for the zone. Luckily, there are ways to solve this problem!</p>
	<h2 id="our-solution-multiple-primary-nameservers">Our Solution: Multiple Primary Nameservers</h2>
	<p>Our solution was to set up our infrastructure zone with multiple primary nameservers. This type of setup is referred to as split authority, multi-primary, or primary/primary. This way, instead of using just one provider for our authoritative nameservers, we use two.</p>
	<p>We added three nameservers from our additional provider to our zone at our registrar. Using multiple primaries allows changes to our zone at each provider, to be controlled separately and completely by us. Instead of using zone transfers to keep our zone in sync, we use <a href="https://github.com/github/octodns" target="_blank">OctoDNS</a> to independently and simultaneously manage the zone at both providers. We’ll talk more about our use of OctoDNS in a bit.</p>
	<p>This setup is similar to using a primary and secondary server. The main difference is that the nameservers operate independently from one another, and do not use the usual DNS AXFR/IXFR method to keep the zone up to date. If you’d like to learn more about this type of solution, here is a great <a href="https://blog.cloudflare.com/secondary-dns-a-faster-more-resilient-way-to-serve-your-dns-records">blog post</a> by Dina Kozlov, our Product Manager for DNS, about Secondary DNS.</p>
	<p>The nameservers in our zone after adding an additional provider would look something like:</p>
	<!--kg-card-begin: markdown-->
	<pre><code>~$ dig NS example.com +short
ns1.example.com.
ns2.example.com.
ns3.example.com.
ns1.additional-provider.net.
ns2.additional-provider.net.
ns3.additional-provider.net.
</code></pre>
	<!--kg-card-end: markdown-->
	<h2 id="predictable-query-routing">Predictable Query Routing</h2>
	<p>Currently, we cannot coordinate which provider should be used when querying a record within the zone. Recursive resolvers have different methods of choosing which nameserver should be used when presented with multiple NS records. A popular choice is to use the server with the lowest RTT (round trip time). A great blog post from <a href="https://www.apnic.net/" target="_blank">APNIC</a> on <a href="https://blog.apnic.net/2019/08/16/recursive-resolver-authoritative-nameserver-selection/" target="_blank">recursive resolver authoritative nameserver selection</a> can be consulted for a detailed explanation.</p>
	<p>We needed to enforce specific routing decisions between the two authorities. Our requirement was for a weighted routing policy preferring Cloudflare based on availability checks for requests originating from our infrastructure servers. This reduces <a href="https://www.cloudflare.com/learning/cdn/glossary/round-trip-time-rtt" target="_blank">RTT</a>, since the queries originate from the same servers hosting our nameservers in the same data center, and do not need to travel further to the external provider when all is well.</p>
	<p>Our edge infrastructure servers are configured to use <a href="https://www.dnsdist.org/" target="_blank">DNSDist</a> as their primary system resolver. DNSDist load balances queries using multiple upstream recursive DNS servers (<a href="https://nlnetlabs.nl/projects/unbound/about/" target="_blank">Unbound</a>) in each data center to provide DNS resolution. This setup is used for internal DNS resolution only, and as such, it is independent from our authoritative DNS and public resolver 1.1.1.1 service.</p>
	<p>Additionally, we modified our DNSDist configuration by adding two server pools for our infrastructure zone authoritative servers, and set up active checks with weighted routes to always use the Cloudflare pool when available. With the active checking and weighted routing in place, our queries are always routed internally when available. In the event of a Cloudflare authoritative DNS failure, DNSDist will route all requests for the zone to our external provider.</p>
	<h2 id="maintaining-our-infrastructure-dns-zone">Maintaining Our Infrastructure DNS Zone</h2>
	<p>In addition to a more reliable infrastructure zone, we also wanted to further automate the provisioning of DNS records. We had been relying on an older manual tool that became quite slow handling our growing number of DNS records. In a nutshell, this tool queried our provisioning database to gather all of the machine names, and then created, deleted, or updated the required DNS records using the Cloudflare API. We had to run this tool whenever we provisioned or decommissioned &nbsp;machines that serve our customers’ requests.</p>
	<p>Part of the procedure for running this tool was to first run it in dry-run mode, and then paste the results for review in our team chat room. This review step ensured the changes the tool found were expected and safe to run, and it is something we want to keep as part of our plan to automate the process.</p>
	<p>Here’s what the old tool looked like:</p>
	<!--kg-card-begin: markdown-->
	<pre><code>$ cf-provision update-example -l ${USER}@cloudflare.com -u
Running update-example.sh -t /var/tmp/cf-provision -l <a href="#email-protected" class="__cf_email__" data-cfemail="email protected">[email&nbsp;protected]</a> -u
* Loading up possible records …

deleting: {"name":"node44.example.com","ttl":300, "type":"A","content":"192.0.2.2","proxied":false}
 rec_id:abcdefg
updating: { "name":"build-ts.example.com", "ttl":120, "type":"TXT", "content":"1596722274"} rec_id:123456 previous content: 1596712896

Result    : SUCCESS
Task      : BUILD
Dest dir  :
Started   : Thu, 06 Aug 2020 13:57:40 +0000
Finished  : Thu, 06 Aug 2020 13:58:16 +0000
Elapsed   : 36 secs
Records Changed In API  : 1 record(s) changed
Records Deleted In API  : 1 record(s) deleted
</code></pre>
	<!--kg-card-end: markdown-->
	<h3 id="zone-management-with-octodns">Zone Management with OctoDNS</h3>
	<p>As we mentioned earlier, when using a multi-primary setup for our infrastructure zone, we are required to maintain the zone data outside of traditional DNS replication. Before rolling out our own solution we wanted to see what was out there, and we stumbled upon <a href="https://github.blog/2017-04-27-enabling-split-authority-dns-with-octodns/" target="_blank">OctoDNS</a>, a project from <a href="https://github.com/" target="_blank">GitHub</a>. OctoDNS provides a set of tools that make it easy to manage your DNS records across multiple providers.</p>
	<p><a href="https://github.com/github/octodns" target="_blank">OctoDNS</a> uses a pluggable architecture. This means we can rewrite our old script as a plugin (which is called a ‘source’), and use other existing provider plugins (called ‘providers’) to interact with both the Cloudflare and our other provider’s APIs. Should we decide to sync to more external DNS providers in the future, we would just need to add them to our OctoDNS configuration. This allows our records to stay up to date, and, as an added benefit, records OctoDNS doesn’t know about will be removed during operation (for example, changes made outside of OctoDNS). This ensures that manual changes do not diverge from what is present in the provisioning database.</p>
	<p>Our goal was to keep the zone management workflow as simple as possible. At Cloudflare, we use <a href="https://www.jetbrains.com/teamcity/" target="_blank">TeamCity</a> for CI/CD, and we realized that it could not only facilitate code builds of our OctoDNS implementation, but it could also be used to deploy the zone.</p>
	<p>There are a number of benefits to using our existing TeamCity infrastructure.</p>
	<ul>
		<li>Our DevTools team can reliably manage it as a service</li>
		<li>It has granular permissions, which allow us to control who can deploy the changes</li>
		<li>It provides storage of the logs for auditing</li>
		<li>It allows easy rollback of zone revisions</li>
		<li>It integrates easily with our chat ops workflow via Google Chat webhooks</li>
	</ul>
	<p>Below is a high-level overview of how we manage our zone through OctoDNS.</p>
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2020/11/image4-10.png" class="kg-image"></figure>
	<p>There are three steps in the workflow:</p>
	<ol>
		<li><strong>Build</strong> - evaluate the sources and build the complete zone.</li>
		<li><strong>Compare</strong> - parse the built zone and compare to the records on the providers. Changes found are sent to SRE for evaluation.</li>
		<li><strong>Deploy</strong> - deploy the changes to the providers.</li>
	</ol>
	<h3 id="step-1-build">Step 1: Build</h3>
	<p><strong>Sources</strong><br>OctoDNS consumes data from our internal systems via custom source modules. So far, we have built two source modules for querying data:</p>
	<ol>
		<li>ProvAPI queries our internal provisioning API providing data center and node configuration.</li>
		<li>NetBox queries our internal <a href="https://github.com/netbox-community/netbox" target="_blank">NetBox</a> deployment providing hardware metadata.</li>
	</ol>
	<p>Additionally, static records are defined in a YAML file. These sources form the infrastructure zone source of truth.</p>
	<p><strong>Providers</strong><br>The build process establishes a staging area per execution. The YAML provider builds a static YAML file containing the complete zone. The hosts file provider is used to generate an emergency hosts file for the zone. Contents from the staging area are revision controlled in CI, which allows us to easily deploy previous versions if required.</p>
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2020/11/image1-22.png" class="kg-image"></figure>
	<h3 id="step-2-compare">Step 2: Compare</h3>
	<p>Following a successful zone build CI executes the compare build. During this phase, OctoDNS performs an octodns-sync in dry-run mode. The compare build consumes the staging YAML zone configuration, and compares the records against the records on our authoritative providers via their respective APIs. Any identified zone changes are parsed, and a summary line is generated and sent to the SRE Chat room for approval. SRE are automatically linked to the changes and associated CI build for deployment.</p>
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2020/11/image3-19.png" class="kg-image"></figure>
	<h3 id="step-3-deploy">Step 3: Deploy</h3>
	<p>The deployment CI build is access-controlled and scoped to the SRE group using our single sign-on provider. Following successful approval and peer review, an SRE can deploy the changes by executing the deploy build.</p>
	<p>The deployment process is simple; consume the YAML zone data from our staging area and deploy the changes to the zone’s authoritative providers via ‘octodns-sync --doit’. The hosts file generated for the zone is packaged and deployed, to be used in the event of complete DNS failure.</p>
	<p>Here’s an example of how the message looks. When the deploy is finished, the thread is updated to indicate which user initiated it.</p>
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2020/11/image5-6.png" class="kg-image"></figure>
	<h2 id="future-improvements">Future Improvements</h2>
	<p>In the future, we would like to automate the process further by reducing the need for approvals. There is usually no harm in adding new records, and it is done very often during the provisioning of new machines. Removing the need to approve those records would take out another step in the provisioning process, which is something we are always looking to optimize.</p>
	<p>Introducing OctoDNS and an additional provider allowed us to make our infrastructure DNS zone more reliable and easier to manage. We can now easily include new sources of record data, with OctoDNS allowing us to focus more on new and exciting projects and less on managing DNS records.</p>
</div>