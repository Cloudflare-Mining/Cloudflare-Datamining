{
	"initialReadingTime": "13",
	"locale": "en-us",
	"localesAvailable": [],
	"post": {
		"authors": [
			{
				"name": "Marek Majkowski",
				"slug": "marek-majkowski",
				"bio": null,
				"profile_image": "https://cf-assets.www.cloudflare.com/slt3lc6tev37/1JuU5qavgwVeqR8BAUrd6U/bd09672287e7cf04d4347d9a47607eb5/marek-majkowski.jpeg",
				"location": null,
				"website": null,
				"twitter": "@majek04",
				"facebook": null
			}
		],
		"excerpt": "Is it ok to have if clauses that will basically never be run? Surely, there must be some performance cost to that...",
		"feature_image": "https://cf-assets.www.cloudflare.com/slt3lc6tev37/6dkfRDk9QfZY1CJiMPuNKo/d8c303a7638bb3c1a8cc9b0bd98e25a2/branch-predictor.png",
		"featured": false,
		"html": "\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/6rKA0cXsmlXoLuoPA07BLc/33c6100db42a60a6edcb84854fd0e071/Branch-Prediction.png\" alt=\"\" class=\"kg-image\" width=\"1600\" height=\"847\" loading=\"lazy\"/>\n            \n            </figure><p>Some time ago I was looking at a hot section in our code and I saw this:</p>\n            <pre class=\"language-c++\"><code class=\"language-c++\">\n\tif (debug) {\n    \t  log(&quot;...&quot;);\n    }\n    </pre></code>\n            <p>This got me thinking. This code is in a performance critical loop and it looks like a waste - we never run with the &quot;debug&quot; flag enabled<sup>[</sup><a href=\"#footnotes\"><sup>1</sup></a><sup>].</sup> Is it ok to have <code>if</code> clauses that will basically never be run? Surely, there must be some performance cost to that...</p><h3>Just how bad is peppering the code with avoidable <code>if</code> statements?</h3><p>Back in the days the general rule was: a fully predictable branch has close to zero CPU cost.</p><p>To what extent is this true? If one branch is fine, then how about ten? A hundred? A thousand? When does adding one more <code>if</code> statement become a bad idea?</p><p>At some point the negligible cost of simple branch instructions surely adds up to a significant amount. As another example, a colleague of mine found this snippet in our production code:</p>\n            <pre class=\"language-c++\"><code class=\"language-c++\">\nconst char *getCountry(int cc) {\n\t\tif(cc == 1) return &quot;A1&quot;;\n        if(cc == 2) return &quot;A2&quot;;\n        if(cc == 3) return &quot;O1&quot;;\n        if(cc == 4) return &quot;AD&quot;;\n        if(cc == 5) return &quot;AE&quot;;\n        if(cc == 6) return &quot;AF&quot;;\n        if(cc == 7) return &quot;AG&quot;;\n        if(cc == 8) return &quot;AI&quot;;\n        ...\n        if(cc == 252) return &quot;YT&quot;;\n        if(cc == 253) return &quot;ZA&quot;;\n        if(cc == 254) return &quot;ZM&quot;;\n        if(cc == 255) return &quot;ZW&quot;;\n        if(cc == 256) return &quot;XK&quot;;\n        if(cc == 257) return &quot;T1&quot;;\n        return &quot;UNKNOWN&quot;;\n}\n        </pre></code>\n            <p>Obviously, this code could be improved<sup>[</sup><a href=\"#footnotes\"><sup>2</sup></a><sup>]</sup>. But when I thought about it more: <i>should</i> it be improved? Is there an actual performance hit of a code that consists of a series of simple branches?</p><h3>Understanding the cost of jump</h3><p>We must start our journey with a bit of theory. We want to figure out if the CPU cost of a branch increases as we add more of them. As it turns out, assessing the cost of a branch is not trivial. On modern processors it takes between one and twenty CPU cycles. There are at least four categories of control flow instructions<sup>[</sup><a href=\"#footnotes\"><sup>3</sup></a><sup>]</sup>: unconditional branch (jmp on x86), call/return, conditional branch (e.g. je on x86) taken and conditional branch not taken. The taken branches are especially problematic: without special care they are inherently costly - we&#39;ll explain this in the following section. To bring down the cost, modern CPU&#39;s try to predict the future and figure out the branch <b>target</b> before the branch is actually fully executed! This is done in a special part of the processor called the branch predictor unit (BPU).</p><p>The branch predictor attempts to figure out a destination of a branching instruction very early and with very little context. This magic happens <b>before</b> the &quot;decoder&quot; pipeline stage and the predictor has very limited data available. It only has some past history and the address of the current instruction. If you think about it - this is super powerful. Given only current instruction pointer it can assess, with very high confidence, where the target of the jump will be.</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/1K3IOo5qkOOP0hhRtwu10U/0f1b00bf1f66c601a1b2cfe7ca990c99/pasted-image-0-1.png\" alt=\"\" class=\"kg-image\" width=\"840\" height=\"613\" loading=\"lazy\"/>\n            \n            </figure><p>Source: <a href=\"https://en.wikipedia.org/wiki/Branch_predictor\">https://en.wikipedia.org/wiki/Branch_predictor</a></p><p>The BPU maintains a couple of data structures, but today we&#39;ll focus on Branch Target Buffer (BTB). It&#39;s a place where the BPU remembers the target instruction pointer of previously taken branches. The whole mechanism is much more complex, take a look a the <a href=\"http://www.ece.uah.edu/~milenka/docs/VladimirUzelac.thesis.pdf\">Vladimir Uzelac&#39;s Master thesis</a> for details about branch prediction on CPU&#39;s from 2008 era:</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/1FO1GCWdkQYR5I90qGopuH/2739fae48f53208ee8511369f3f1a453/pasted-image-0--1-.png\" alt=\"\" class=\"kg-image\" width=\"765\" height=\"601\" loading=\"lazy\"/>\n            \n            </figure><p>For the scope of this article we&#39;ll simplify and focus on the BTB only. We&#39;ll try to show how large it is and how it behaves under different conditions.</p><h3>Why is branch prediction needed?</h3><p>But first, why is branch prediction used at all? In order to get the best performance, the CPU pipeline must feed a constant flow of instructions. Consider what happens to the multi-stage CPU pipeline on a branch instruction. To illustrate let&#39;s consider the following ARM program:</p>\n            <pre class=\"language-c++\"><code class=\"language-c++\">\n\tBR label_a;\n    X1\n    ...\nlabel_a:\n \tY1\n    </pre></code>\n            <p>Assuming a simplistic CPU model, the operations would flow through the pipeline like this:</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/6LYtd8rNXEQA2X2pOPwmvd/189b8a049a74f89c13089d9b8a2fe7d6/3P6PIWN6gPAdYzP8oDgrsaOMKgUmG51zIiFhbm071cZKM276S7vRb5atpTwlKrM1lFHRYsobw8P4e-Z9t1Vb9TGeutpBe2CkMrNGruWO8yb5Qz0vZ6Qn6RbOi5Tp.png\" alt=\"\" class=\"kg-image\" width=\"1170\" height=\"654\" loading=\"lazy\"/>\n            \n            </figure><p>In the first cycle the BR instruction is fetched. This is an unconditional branch instruction changing the execution flow of the CPU. At this point it&#39;s not yet decoded, but the CPU would like to fetch another instruction already! Without a branch predictor in cycle 2 the fetch unit either has to wait or simply continues to the next instruction in memory, hoping it will be the right one.</p><p>In our example, instruction X1 is fetched even though this isn&#39;t the correct instruction to run. In cycle 4, when the branch instruction finishes the execute stage, the CPU will be able to understand the mistake, and roll back the speculated instructions before they have any effect. At this point the fetch unit is updated to correctly get the right instruction - Y1 in our case.</p><p>This situation of losing a number of cycles due to fetching code from an incorrect place is called a &quot;frontend bubble&quot;. Our theoretical CPU has a two-cycle frontend bubble when a branch target wasn’t predicted right.</p><p>In this example we see that, although the CPU does the right thing in the end, without good branch prediction it wasted effort on bad instructions. In the past, various techniques have been used to reduce this problem, such as static branch prediction and branch delay slots. But the dominant CPU designs today rely <a href=\"https://danluu.com/branch-prediction/#one-bit\">on <i>dynamic branch prediction</i></a>. This technique is able to mostly avoid the frontend bubble problem, by predicting the correct address of the next instruction even for branches that aren’t fully decoded and executed yet.</p><h3>Playing with the BTB</h3><p>Today we&#39;re focusing on the BTB - a data structure managed by the branch predictor responsible for figuring out a target of a branch. It&#39;s important to note that the BTB is distinct from and independent of the system assessing if the branch was taken or not taken. Remember, we want to figure out if a cost of a branch increases as we run more of them.</p><p>Preparing an experiment to stress only the BTB is relatively simple (<a href=\"https://xania.org/201602/bpu-part-three\">based on Matt Godbolt&#39;s work</a>). It turns out a sequence of unconditional <code>jmps</code> is totally sufficient. Consider this x86 code:</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/259y50RQfLqaFcsT4tljGb/1877e062685dd0177c2db1a054da3731/pasted-image-0--2-.png\" alt=\"\" class=\"kg-image\" width=\"900\" height=\"448\" loading=\"lazy\"/>\n            \n            </figure><p>This code stresses the BTB to an extreme - it just consists of a chain of <code>jmp +2</code> statements (i.e. literally jumping to the next instruction). In order to avoid wasting cycles on frontend pipeline bubbles, each taken jump needs a BTB hit. This branch prediction must happen very early in the CPU pipeline, before instruction decode is finished. This same mechanism is needed for any taken branch, whether it&#39;s unconditional, conditional or a function call.</p><p>The code above was run inside a test harness that measures how many CPU cycles elapse for each instruction. For example, in this run we&#39;re measuring times of dense - every two bytes - 1024 jmp instructions one after another:</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/3QofFzm1kmqFLuRrFPoyjQ/e85247358de40b0ead463299792416a1/pasted-image-0--3-.png\" alt=\"\" class=\"kg-image\" width=\"876\" height=\"510\" loading=\"lazy\"/>\n            \n            </figure><p>We’ll look at the results of experiments like this for a few different CPUs. But in this instance, it was run on a machine with an AMD EPYC 7642. Here, the cold run took 10.5 cycles per jmp, and then all subsequent runs took ~3.5 cycles per jmp. The code is prepared in such a way to make sure it&#39;s the BTB that is slowing down the first run. Take a look at the full code, there is quite some magic to warm up the L1 cache and iTLB without priming the BTB.</p><p><b>Top tip 1. On this CPU a branch instruction that is taken but not predicted, costs ~7 cycles more than one that is taken and predicted.</b> Even if the branch was unconditional.</p><h3>Density matters</h3><p>To get a full picture we also need to think about the density of jmp instructions in the code. The code above did eight jmps per 16-byte code block. This is a lot. For example, the code below contains one jmp instruction in each block of 16 bytes. Notice that the <code>nop</code> opcodes are jumped over. The block size doesn&#39;t change the number of executed instructions, only the code density:</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/2uenA2zFAkPWQZldVX7gba/7d2bdf7cb06dcd7924821749c715fac8/pasted-image-0--4-.png\" alt=\"\" class=\"kg-image\" width=\"790\" height=\"778\" loading=\"lazy\"/>\n            \n            </figure><p>Varying the jmp block size might be important. It allows us to control the placement of the jmp opcodes. Remember the BTB is indexed by instruction pointer address. Its value and its alignment might influence the placement in the BTB and help us reveal the BTB layout. Increasing the alignment will cause more nop padding to be added. The sequence of a single measured instruction - jmp in this case - and zero or more nops, I will call &quot;block&quot;, and its size &quot;block size&quot;. Notice that the larger the block size, the larger the working code size for the CPU. At larger values we might see some performance drop due to exhausting L1 cache space.</p><h3>The experiment</h3><p>Our experiment is crafted to show the performance drop depending on the number of branches, across different working code sizes. Hopefully, we will be able to prove the performance is mostly dependent on the number of blocks - and therefore the BTB size, and not the working code size.</p><p>See the <a href=\"https://github.com/cloudflare/cloudflare-blog/tree/master/2021-05-branch-prediction\">code on GitHub</a>. If you want to see the generated machine code, though, you need to run a special command. It&#39;s created procedurally by the code, customized by passed parameters. Here&#39;s an example <code>gdb</code> incantation:</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/7aAKMXz8aruwblN67UvZd7/af2eec43064b1182b6b3f2bcb462d962/pasted-image-0--5-.png\" alt=\"\" class=\"kg-image\" width=\"1197\" height=\"76\" loading=\"lazy\"/>\n            \n            </figure><p>Let&#39;s bring this experiment forward, what if we took the best times of each run - with a fully primed BTB - for varying values of jmp block sizes and number of blocks - working set size? Here you go:</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/4ZNeolS38Q90vdHPvehvVa/fffb2a76c40931e593d424dc4bb7f873/pasted-image-0--6-.png\" alt=\"\" class=\"kg-image\" width=\"1000\" height=\"600\" loading=\"lazy\"/>\n            \n            </figure><p>This is an astonishing chart. First, it&#39;s obvious something happens at the 4096 jmp mark[<a href=\"#footnotes\">4</a>] regardless of how large the jmp block sizes - how many nop&#39;s we skip over. Reading it aloud:</p><ul><li><p>On the far left, we see that if the amount of code is small enough - less than 2048 bytes (256 times a block of 8 bytes) - it&#39;s possible to hit some kind of uop/L1 cache and get ~1.5 cycles per fully predicted branch. This is amazing.</p></li><li><p>Otherwise, if you keep your hot loop to 4096 branches then, no matter how dense your code is you are likely to see ~3.4 cycles per fully predicted branch</p></li><li><p>Above 4096 branches the branch predictor gives up and the cost of each branch shoots to ~10.5 cycles per jmp. This is consistent with what we saw above - unpredicted branch on flushed BTB took ~10.5 cycles.</p></li></ul><p>Great, so what does it mean? Well, you should avoid branch instructions if you want to avoid branch misses because you have at most 4096 of fast BTB slots. This is not a very pragmatic advice though - it&#39;s not like we deliberately put many unconditional <code>jmp</code>s in real code!</p><p>There are a couple of takeaways for the discussed CPU. I repeated the experiment with an always-taken conditional branch sequence and the resulting chart looks almost identical. The only difference being the predicted taken conditional-je instruction being 2 cycles slower than unconditional jmp.</p><p>An entry to BTB is added wherever a branch is &quot;taken&quot; - that is, the jump actually happens. An unconditional &quot;jmp&quot; or always taken conditional branch, will cost a BTB slot. To get best performance make sure to not have more than 4096 taken branches in the hot loop. The good news is that branches never-taken don&#39;t take space in the BTB. We can illustrate this with another experiment:</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/5gd2JKAwYMpgk5obd4E0Hz/590527483c2896fc98376c5032737b41/pasted-image-0--7-.png\" alt=\"\" class=\"kg-image\" width=\"457\" height=\"126\" loading=\"lazy\"/>\n            \n            </figure><p>This boring code is going over not-taken <code>jne</code> followed by two nops (block size=4). Aimed with this test (jne never-taken), the previous one (jmp always-taken) and a conditional branch <code>je</code> always-taken, we can draw this chart:</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/1F2STBl6iw6OpkwqmIdlR9/656f5681f453bbd62023d160795a3519/pasted-image-0--8-.png\" alt=\"\" class=\"kg-image\" width=\"1000\" height=\"600\" loading=\"lazy\"/>\n            \n            </figure><p>First, without any surprise we can see the conditional &#39;je always-taken&#39; is getting slightly more costly than the simple unconditional <code>jmp</code>, but only after the 4096 branches mark. This makes sense, the conditional branch is resolved later in the pipeline so the frontend bubble is longer. Then take a look at the blue line hovering near zero. This is the &quot;jne never-taken&quot; line flat at 0.3 clocks / block, no matter how many blocks we run in sequence. The takeaway is clear - you can have as many never-taken branches as you want, without incurring any cost. There isn&#39;t any spike at 4096 mark, meaning BTB is not used in this case. It seems the conditional jump not seen before is guessed to be not-taken.</p><p><b>Top tip 2: conditional branches never-taken are basically free</b> - at least on this CPU.</p><p>So far we established that branches always-taken occupy BTB, branches never taken do not. How about other control flow instructions, like the <code>call</code>?</p><p>I haven&#39;t been able to find this in the literature, but it seems call/ret also need the BTB entry for best performance. I was able to illustrate this on our AMD EPYC. Let&#39;s take a look at this test:</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/fHArGj5TWvRM9AEM84NLJ/02ca744848251f7d4723c63a92c5ffc4/pasted-image-0--9-.png\" alt=\"\" class=\"kg-image\" width=\"480\" height=\"333\" loading=\"lazy\"/>\n            \n            </figure><p>This time we&#39;ll issue a number of <code>callq</code> instructions followed by <code>ret</code> - both of which should be fully predicted. The experiment is crafted so that each callq calls a unique function, to allow for retq prediction - each one returns to exactly one caller.</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/677H5BdxwzZ9B6hbebRFnB/0a53b883924daf4e65850d27c5999ee9/pasted-image-0--10-.png\" alt=\"\" class=\"kg-image\" width=\"1000\" height=\"600\" loading=\"lazy\"/>\n            \n            </figure><p>This chart confirms the theory: no matter the code density - with the exception of 64-byte block size being notably slower -  the cost of predicted call/ret starts to deteriorate after the 2048 mark. At this point the BTB is filled with call and ret predictions and can&#39;t handle any more data. This leads to an important conclusion:</p><p><b>Top tip 3. In the hot code you want to have less than 2K function calls</b> - on this CPU.</p><p>In our test CPU a sequence of fully predicted call/ret takes about 7 cycles, which is about the same as two unconditional predicted <code>jmp</code> opcodes. It&#39;s consistent with our results above.</p><p>So far we thoroughly checked AMD EPYC 7642. We started with this CPU because the branch predictor is relatively simple and the charts were easy to read. It turns out more recent CPUs are less clear.</p><h3>AMD EPYC 7713</h3><p>Newer AMD is more complex than the previous generations. Let&#39;s run the two most important experiments. First, the <code>jmp</code> one:</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/12g3Zg18eWyHIwyUuf63gj/1966b1de89c131fd46fcbcdbe6d6d725/pasted-image-0--11-.png\" alt=\"\" class=\"kg-image\" width=\"1000\" height=\"600\" loading=\"lazy\"/>\n            \n            </figure><p>For the always-taken branches case we can see a very good, sub 1 cycle, timings when the number of branches doesn&#39;t exceed 1024 and the code isn&#39;t too dense.</p><p><b>Top tip 4. On this CPU it&#39;s possible to get &lt;1 cycle per predicted jmp when the hot loop fits in ~32KiB.</b></p><p>Then there is some noise starting after the 4096 jmps mark. This is followed by a complete drop of speed at about 6000 branches. This is in line with the theory that BTB is 4096 entries long. We can speculate that some other prediction mechanism is successfully kicking in beyond that, and keeps up the performance up the ~6k mark.</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/4X5KfDDT36u9u2MGb7iu7t/f0c33bdaa2cbf42f2090f5a616008bd9/pasted-image-0--12-.png\" alt=\"\" class=\"kg-image\" width=\"1000\" height=\"600\" loading=\"lazy\"/>\n            \n            </figure><p>The call/ret chart shows a similar tale, the timings start breaking after 2048 mark, and completely fail to be predicted beyond ~3000.</p><h3>Xeon Gold 6262</h3><p>The Intel Xeon looks different from the AMD:</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/7hIriKLopeE26yIor9qGG/ed9680bc72b4843455b3b165473db63d/pasted-image-0--13-.png\" alt=\"\" class=\"kg-image\" width=\"1000\" height=\"600\" loading=\"lazy\"/>\n            \n            </figure><p>Our test shows the predicted taken branch costs 2 cycles. Intel has documented a clock penalty for very dense branching code - this explains the 4-byte block size line hovering at ~3 cycles. The branch cost breaks at the 4096 jmp mark, confirming the theory that the Intel BTB can hold 4096 entries. The 64-byte block size chart looks confusing, but really isn&#39;t. The branch cost stays at flat 2 cycles up till the 512 jmp count. Then it increases. This is caused by the internal layout of the BTB which is said to be 8-way associative. It seems with the 64-byte block size we can utilize at most half of the 4096 BTB slots.</p><p><b>Top tip 5. On Intel avoid placing your jmp/call/ret instructions at regular 64-byte intervals.</b></p><p>Then the call/ret chart:</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/1XU4ofCRG3wfjLZlwsEhBv/597d7b569487f230adb6705fe82fd324/pasted-image-0--14-.png\" alt=\"\" class=\"kg-image\" width=\"1000\" height=\"600\" loading=\"lazy\"/>\n            \n            </figure><p>Similarly, we can see the branch predictions failing after the 2048 jmp mark - in this experiment one block uses two flow control instructions: call and ret. This again confirms the BTB size of 4K entries. The 64-byte block size is generally slower due to the nop padding but also breaks faster due to the instructions alignment issue. Notice, we haven&#39;t seen this effect on AMD.</p><h3>Apple Silicon M1</h3><p>So far we saw examples of AMD and Intel server grade CPUs. How does an Apple Silicon M1 fit in this picture?</p><p>We expect it to be very different - it&#39;s designed for mobile and it&#39;s using ARM64 architecture. Let&#39;s see our two experiments:</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/bJ9lzYsIARltRCkOEJVZZ/af75a9fdbdb0324fab4e3a376e053766/pasted-image-0--15-.png\" alt=\"\" class=\"kg-image\" width=\"1000\" height=\"600\" loading=\"lazy\"/>\n            \n            </figure><p>The predicted <code>jmp</code> test shows an interesting story. First, when the code fits 4096 bytes (1024*4 or 512*8, etc) you can expect a predicted <code>jmp</code> to cost 1 clock cycle. This is an excellent score.</p><p>Beyond that, generally, you can expect a cost of 3 clock cycles per predicted jmp. This is also very good. This starts to deteriorate when the working code grows beyond ~200KiB. This is visible with block size 64 breaking at 3072 mark 3072*64=196K, and for block 32 at 6144: 6144*32=196K. At this point the prediction seems to stop working. The documentation indicates that the M1 CPU has 192 KB L1 of instruction cache - our experiment matches that.</p><p>Let&#39;s compare the &quot;predicted jmp&quot; with the &quot;unpredicted jmp&quot; chart. Take this chart with a grain of salt, because flushing the branch predictor is notoriously difficult.</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/2VCtlgVdJkSB3MFuYbR5MQ/cc906444e88a9fbf951e1a7ab84a142e/pasted-image-0--16-.png\" alt=\"\" class=\"kg-image\" width=\"1000\" height=\"600\" loading=\"lazy\"/>\n            \n            </figure><p>However, even if we don&#39;t trust the flush-bpu code (<a href=\"https://xania.org/201602/bpu-part-three\">adapted from Matt Godbolt</a>), this chart reveals two things. First, the &quot;unpredicted&quot; branch cost seems to be correlated with the branch distance. The longer the branch the costlier it is. We haven&#39;t seen such behaviour on x86 CPUs.</p><p>Then there is the cost itself. We saw a predicted sequence of branches cost, and what a supposedly-unpredicted jmp costs. In the first chart we saw that beyond ~192KiB working code, the branch predictor seems to become ineffective. The supposedly-flushed BPU seems to show the same cost. For example, the cost of a 64-byte block size jmp with a small working set size is 3 cycles. A miss is ~8 cycles. For a large working set size both times are ~8 cycles. It seems that the BTB is linked to the L1 cache state. <a href=\"https://www.realworldtech.com/forum/?threadid=159985&curpostid=160001\">Paul A. Clayton suggested</a> a possibility of such a design back in 2016.</p><p><b>Top tip 6. on M1 the predicted-taken branch generally takes 3 cycles and unpredicted but taken has varying cost, depending on jmp length. BTB is likely linked with L1 cache.</b></p><p>The call/ret chart is funny:</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/54s7q6CJamAHCjpzDHFm6V/9693c0b7ddf00b57f4db9df68b149d08/pasted-image-0--17-.png\" alt=\"\" class=\"kg-image\" width=\"1000\" height=\"600\" loading=\"lazy\"/>\n            \n            </figure><p>Like in the chart before, we can see a big benefit if hot code fits within 4096 bytes (512*4 or 256*8). Otherwise, you can count on 4-6 cycles per call/ret sequence (or, bl/ret as it&#39;s known in ARM). The chart shows funny alignment issues. It&#39;s unclear what they are caused by. Beware, comparing the numbers in this chart with x86 is unfair, since ARM <code>call</code> operation differs substantially from the x86 variant.</p><p>M1 seems pretty fast, with predicted branches usually at 3 clock cycles. Even unpredicted branches never cost more than 8 ticks in our benchmark. Call+ret sequence for dense code should fit under 5 cycles.</p><h3>Summary</h3><p>We started our journey from a piece of trivial code, and asked a basic question: how costly is  adding a never-taken <code>if</code> branch in the hot portion of code?</p><p>Then we quickly dived in very low level CPU features. By the end of this article, hopefully, an astute reader might get better intuition how a modern branch predictors works.</p><p>On x86 the hot code needs to split the BTB budget between function calls and taken branches. The BTB has only a size of 4096 entries. There are strong benefits in keeping the hot code under 16KiB.</p><p>On the other hand on M1 the BTB seems to be limited by L1 instruction cache. If you&#39;re writing super hot code, ideally it should fit 4KiB.</p><p>Finally, can you add this one more <code>if</code> statement? If it&#39;s never-taken, it&#39;s probably ok. I found no evidence that such branches incur any extra cost. But do avoid always-taken branches and function calls.</p><p><b>Sources</b></p><p>I&#39;m not the first person to investigate how BTB works. I based my experiments on:</p><ul><li><p><a href=\"http://www.ece.uah.edu/~milenka/docs/VladimirUzelac.thesis.pdf\">Vladimir Uzelac thesis</a></p></li><li><p><a href=\"https://xania.org/201602/bpu-part-three\">Matt Godbolt work</a>. The series has 5 articles.</p></li><li><p><a href=\"https://www.realworldtech.com/forum/?threadid=159985&curpostid=159985\">Travis Downs BTB questions</a> on Real World Tech</p></li><li><p><a href=\"https://stackoverflow.com/questions/38811901/slow-jmp-instruction\">various</a> <a href=\"https://stackoverflow.com/questions/51822731/why-did-intel-change-the-static-branch-prediction-mechanism-over-these-years\">stackoverflow</a> <a href=\"https://stackoverflow.com/questions/38512886/btb-size-for-haswell-sandy-bridge-ivy-bridge-and-skylake\">discussions</a>. Especially <a href=\"https://stackoverflow.com/questions/31280817/what-branch-misprediction-does-the-branch-target-buffer-detect\">this one</a> and <a href=\"https://stackoverflow.com/questions/31642902/intel-cpus-instruction-queue-provides-static-branch-prediction\">this</a></p></li><li><p><a href=\"https://www.agner.org/optimize/microarchitecture.pdf\">Agner Fog</a> microarchitecture guide has a good section on branch predictions.</p></li></ul><h3>Acknowledgements</h3><p>Thanks to <a href=\"/author/david-wragg/\">David Wragg</a> and <a href=\"https://twitter.com/danluu\">Dan Luu</a> for technical expertise and proofreading help.</p><h3>PS</h3><p>Oh, oh. But this is not the whole story! Similar research was the base to the <a href=\"https://spectreattack.com/spectre.pdf\">Spectre v2</a> attack. The attack was exploiting the little known fact that the BPU state was not cleared between context switches. With the correct technique it was possible to train the BPU - in the case of Spectre it was iBTB - and force a privileged piece of code to be speculatively executed. This, combined with a cache side-channel data leak, allowed an attacker to steal secrets from the privileged kernel. Powerful stuff.</p><p>A proposed solution was to avoid using shared BTB. This can be done in two ways: make the indirect jumps to always fail to predict, or fix the CPU to avoid sharing BTB state across isolation domains. This is a long story, maybe for another time...</p><hr/><!--kg-card-begin: html--><p><a name=\"footnotes\">Footnotes</a></p><!--kg-card-end: html--><p>1. One historical solution to this specific &#39;if debug&#39; problem is called &quot;runtime nop&#39;ing&quot;. The idea is to modify the code in runtime and patch the never-taken branch instruction with a <code>nop</code>. For example, see the &quot;ISENABLED&quot; discussion on <a href=\"https://bugzilla.mozilla.org/showbug.cgi?id=370906.\">https://bugzilla.mozilla.org/showbug.cgi?id=370906.</a></p><p>2. Fun fact: modern compilers are pretty smart. New gcc (&gt;=11) and older clang (&gt;=3.7) are able to actually optimize it quite a lot. <a href=\"https://godbolt.org/z/KWYEW3d9s\">See for yourself</a>. But, let&#39;s not get distracted by that. This article is about low level machine code branch instructions!</p><p>3. This is a simplification. There are of course more control flow instructions, like: software interrupts, syscalls, VMENTER/VMEXIT.</p><p>4. Ok, I&#39;m slightly overinterpreting the chart. Maybe the 4096 jmp mark is due to the 4096 uop cache or some instruction decoder artifact? To prove this spike is indeed BTB related I looked at Intel BPUCLEARS.EARLY and BACLEAR.CLEAR performance counters. Its value is small for block count under 4096 and large for block count greater than 5378. This is strong evidence that the performance drop is indeed caused by the BPU and likely BTB.</p>",
		"id": "2pvX64jHrEfNLMSmJO1Iv4",
		"localeList": {
			"name": "Branch predictor: How many \"if\"s are too many? Including x86 and M1 benchmarks! Config",
			"enUS": "English for Locale",
			"zhCN": "No Page for Locale",
			"zhHansCN": "No Page for Locale",
			"zhTW": "No Page for Locale",
			"frFR": "No Page for Locale",
			"deDE": "No Page for Locale",
			"itIT": "No Page for Locale",
			"jaJP": "No Page for Locale",
			"koKR": "No Page for Locale",
			"ptBR": "No Page for Locale",
			"esLA": "No Page for Locale",
			"esES": "No Page for Locale",
			"enAU": "No Page for Locale",
			"enCA": "No Page for Locale",
			"enIN": "No Page for Locale",
			"enGB": "No Page for Locale",
			"idID": "No Page for Locale",
			"ruRU": "No Page for Locale",
			"svSE": "No Page for Locale",
			"viVN": "No Page for Locale",
			"plPL": "No Page for Locale",
			"arAR": "No Page for Locale",
			"nlNL": "No Page for Locale",
			"thTH": "No Page for Locale",
			"trTR": "No Page for Locale",
			"heIL": "No Page for Locale",
			"lvLV": "No Page for Locale",
			"etEE": "No Page for Locale",
			"ltLT": "No Page for Locale"
		},
		"meta_description": "Is it ok to have if clauses that will basically never be run? Surely, there must be some performance cost to that...",
		"metadata": {
			"title": "Branch predictor: How many \"if\"s are too many? Including x86 and M1 benchmarks!",
			"description": "Is it ok to have if clauses that will basically never be run? Surely, there must be some performance cost to that...",
			"imgPreview": "https://cf-assets.www.cloudflare.com/slt3lc6tev37/7hvQ9Jzo2qJJMzPJEhg50G/91604aa5f49814df7aa1d5cefcfc0b31/branch-predictor-7sJ6qt.png"
		},
		"primary_author": {},
		"published_at": "2021-05-06T14:00:00.000+01:00",
		"slug": "branch-predictor",
		"tags": [
			{
				"id": "2UVIYusJwlvsmPYl2AvSuR",
				"name": "Deep Dive",
				"slug": "deep-dive"
			},
			{
				"id": "6lhzEBz2B56RKa4nUEAGYJ",
				"name": "Programming",
				"slug": "programming"
			},
			{
				"id": "6NELU2Vg2VAJ5fpLtAToII",
				"name": "AMD",
				"slug": "amd"
			},
			{
				"id": "7zGaJ5bHAL9yQY1lK4OHjH",
				"name": "EPYC",
				"slug": "epyc"
			},
			{
				"id": "48r7QV00gLMWOIcM1CSDRy",
				"name": "Speed & Reliability",
				"slug": "speed-and-reliability"
			}
		],
		"title": "Branch predictor: How many \"if\"s are too many? Including x86 and M1 benchmarks!",
		"updated_at": "2024-08-27T01:49:48.235Z",
		"url": "https://blog.cloudflare.com/branch-predictor"
	},
	"translations": {
		"posts.by": "By",
		"footer.gdpr": "GDPR",
		"lang_blurb1": "This post is also available in {lang1}.",
		"lang_blurb2": "This post is also available in {lang1} and {lang2}.",
		"lang_blurb3": "This post is also available in {lang1}, {lang2} and {lang3}.",
		"footer.blurb": "Cloudflare's connectivity cloud protects <a target='_blank' href='https://www.cloudflare.com/network-services/' rel='noreferrer'>entire corporate networks</a>, helps customers build <a target='_blank' href='https://workers.cloudflare.com/' rel='noreferrer'>Internet-scale applications efficiently</a>, accelerates any <a target='_blank' href='https://www.cloudflare.com/performance/accelerate-internet-applications/' rel='noreferrer'>website or Internet application</a>, <a target='_blank' href='https://www.cloudflare.com/ddos/' rel='noreferrer'>wards off DDoS attacks</a>, keeps <a target='_blank' href='https://www.cloudflare.com/application-security/' rel='noreferrer'>hackers at bay</a>, and can help you on <a target='_blank' href='https://www.cloudflare.com/products/zero-trust/' rel='noreferrer'>your journey to Zero Trust</a>.<br/><br/>Visit <a target='_blank' href='https://one.one.one.one/' rel='noreferrer'>1.1.1.1</a> from any device to get started with our free app that makes your Internet faster and safer.<br/><br/>To learn more about our mission to help build a better Internet, <a target='_blank' href='https://www.cloudflare.com/learning/what-is-cloudflare/' rel='noreferrer'>start here</a>. If you&apos;re looking for a new career direction, check out <a target='_blank' href='http://www.cloudflare.com/careers' rel='noreferrer'>our open positions</a>.",
		"footer.press": "Press",
		"header.title": "The Cloudflare Blog",
		"footer.careers": "Careers",
		"footer.company": "Company",
		"footer.support": "Support",
		"footer.the_net": "theNet",
		"footer.our_team": "Our team",
		"footer.webinars": "Webinars",
		"page.more_posts": "More posts",
		"posts.time_read": "{time} min read",
		"footer.community": "Community",
		"footer.resources": "Resources",
		"footer.solutions": "Solutions",
		"footer.trademark": "Trademark",
		"header.subscribe": "Subscribe",
		"footer.compliance": "Compliance",
		"footer.free_plans": "Free plans",
		"footer.impact_ESG": "Impact/ESG",
		"posts.follow_on_X": "Follow on X",
		"footer.help_center": "Help center",
		"footer.network_map": "Network Map",
		"header.please_wait": "Please Wait",
		"page.related_posts": "Related posts",
		"footer.case_studies": "Case Studies",
		"footer.connect_2024": "Connect 2024",
		"footer.terms_of_use": "Terms of Use",
		"footer.white_papers": "White Papers",
		"footer.cloudflare_tv": "Cloudflare TV",
		"footer.community_hub": "Community Hub",
		"footer.compare_plans": "Compare plans",
		"footer.contact_sales": "Contact Sales",
		"header.contact_sales": "Contact Sales",
		"header.email_address": "Email Address",
		"page.error.not_found": "Page not found",
		"footer.developer_docs": "Developer docs",
		"footer.privacy_policy": "Privacy Policy",
		"footer.request_a_demo": "Request a demo",
		"page.continue_reading": "Continue reading",
		"footer.analysts_report": "Analyst reports",
		"footer.for_enterprises": "For enterprises",
		"footer.getting_started": "Getting Started",
		"footer.learning_center": "Learning Center",
		"footer.project_galileo": "Project Galileo",
		"pagination.newer_posts": "Newer Posts",
		"pagination.older_posts": "Older Posts",
		"posts.social_buttons.x": "Discuss on X",
		"footer.about_cloudflare": "About Cloudflare",
		"footer.athenian_project": "Athenian Project",
		"footer.become_a_partner": "Become a partner",
		"footer.cloudflare_radar": "Cloudflare Radar",
		"footer.network_services": "Network services",
		"footer.trust_and_safety": "Trust & Safety",
		"header.get_started_free": "Get Started Free",
		"page.search.placeholder": "Search Cloudflare",
		"footer.cloudflare_status": "Cloudflare Status",
		"footer.cookie_preference": "Cookie Preferences",
		"header.valid_email_error": "Must be valid email.",
		"footer.connectivity_cloud": "Connectivity cloud",
		"footer.developer_services": "Developer services",
		"footer.investor_relations": "Investor relations",
		"page.not_found.error_code": "Error Code: 404",
		"footer.logos_and_press_kit": "Logos & press kit",
		"footer.application_services": "Application services",
		"footer.get_a_recommendation": "Get a recommendation",
		"posts.social_buttons.reddit": "Discuss on Reddit",
		"footer.sse_and_sase_services": "SSE and SASE services",
		"page.not_found.outdated_link": "You may have used an outdated link, or you may have typed the address incorrectly.",
		"footer.report_security_issues": "Report Security Issues",
		"page.error.error_message_page": "Sorry, we can't find the page you are looking for.",
		"header.subscribe_notifications": "Subscribe to receive notifications of new posts:",
		"footer.cloudflare_for_campaigns": "Cloudflare for Campaigns",
		"header.subscription_confimation": "Subscription confirmed. Thank you for subscribing!",
		"posts.social_buttons.hackernews": "Discuss on Hacker News",
		"footer.diversity_equity_inclusion": "Diversity, equity & inclusion",
		"footer.critical_infrastructure_defense_project": "Critical Infrastructure Defense Project"
	}
}