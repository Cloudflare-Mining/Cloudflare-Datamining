<div class="mb2 gray5">4 min read</div>
<div class="mt4">This post is also available in <a href="https://blog.cloudflare.com/zh-cn/cloudflare-incident-march-21-2025">简体中文</a> and <a href="https://blog.cloudflare.com/ja-jp/cloudflare-incident-march-21-2025">日本語</a>.</div><img class="mr2" src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/1zVzYYX4Zs6rRox4hJO4wJ/c64947208676753e532135f9393df5c5/BLOG-2793_1.png" alt="">
<div class="post-content lh-copy gray1">
	<p>Multiple Cloudflare services, including <a href="https://developers.cloudflare.com/r2"><u>R2 object storage</u></a>, experienced an elevated rate of errors for 1 hour and 7 minutes on March 21, 2025 (starting at 21:38 UTC and ending 22:45 UTC). During the incident window, 100% of write operations failed and approximately 35% of read operations to R2 failed globally. Although this incident started with R2, it impacted other Cloudflare services including <a href="https://www.cloudflare.com/developer-platform/products/cache-reserve"><u>Cache Reserve</u></a>, <a href="https://www.cloudflare.com/developer-platform/products/cloudflare-images"><u>Images</u></a>, <a href="https://developers.cloudflare.com/logs/edge-log-delivery"><u>Log Delivery</u></a>, <a href="https://www.cloudflare.com/developer-platform/products/cloudflare-stream"><u>Stream</u></a>, and <a href="https://developers.cloudflare.com/vectorize"><u>Vectorize</u></a>.</p>
	<p>While rotating credentials used by the R2 Gateway service (R2's API frontend) to authenticate with our storage infrastructure, the R2 engineering team inadvertently deployed the new credentials (ID and key pair) to a development instance of the service instead of production. When the old credentials were deleted from our storage infrastructure (as part of the key rotation process), the production R2 Gateway service did not have access to the new credentials. This ultimately resulted in R2’s Gateway service not being able to authenticate with our storage backend. There was no data loss or corruption that occurred as part of this incident: any in-flight uploads or mutations that returned successful HTTP status&nbsp;codes were persisted.</p>
	<p>Once the root cause was identified and we realized we hadn’t deployed the new credentials to the production R2 Gateway service, we deployed the updated credentials and service availability was restored.&nbsp;</p>
	<p>This incident happened because of human error and lasted longer than it should have because we didn’t have proper visibility into which credentials were being used by the Gateway Worker to authenticate with our storage infrastructure.&nbsp;</p>
	<p>We’re deeply sorry for this incident and the disruption it may have caused to you or your users. We hold ourselves to a high standard and this is not acceptable. This blog post exactly explains the impact, what happened and when, and the steps we are taking to make sure this failure (and others like it) doesn’t happen again.</p>
	<div class="flex anchor relative">
		<h2 id="what-was-impacted">What was impacted?</h2>
		<a href="https://blog.cloudflare.com/#what-was-impacted" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p><b>The primary incident window occurred between 21:38 UTC and 22:45 UTC.</b></p>
	<p>The following table details the specific impact to R2 and Cloudflare services that depend on, or interact with, R2:</p>
	<style type="text/css">
		.tg {
			border-collapse: collapse;
			border-spacing: 0;
			margin: 0px auto;
		}

		.tg td {
			border-color: black;
			border-style: solid;
			border-width: 1px;
			font-family: Arial, sans-serif;
			font-size: 14px;
			overflow: hidden;
			padding: 10px 5px;
			word-break: normal;
		}

		.tg th {
			border-color: black;
			border-style: solid;
			border-width: 1px;
			font-family: Arial, sans-serif;
			font-size: 14px;
			font-weight: normal;
			overflow: hidden;
			padding: 10px 5px;
			word-break: normal;
		}

		.tg .tg-fymr {
			border-color: inherit;
			font-weight: bold;
			text-align: left;
			vertical-align: top
		}

		.tg .tg-0pky {
			border-color: inherit;
			text-align: left;
			vertical-align: top
		}

		@media screen and (max-width: 767px) {
			.tg {
				width: auto !important;
			}

			.tg col {
				width: auto !important;
			}

			.tg-wrap {
				overflow-x: auto;
				-webkit-overflow-scrolling: touch;
				margin: auto 0px;
			}
		}
	</style>
	<div class="tg-wrap">
		<table class="tg">
			<thead>
				<tr>
					<th class="tg-fymr"><span style="font-weight:bold;font-style:normal;text-decoration:none;color:#000;background-color:transparent">Product/Service</span></th>
					<th class="tg-0pky"><span style="font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent">Impact</span></th>
				</tr>
			</thead>
			<tbody>
				<tr>
					<td class="tg-fymr"><span style="font-style:normal;text-decoration:none;color:#000;background-color:transparent">R2</span></td>
					<td class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">All customers using Cloudflare R2 would have experienced an elevated error rate during the primary incident window. Specifically:</span><br><br><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">* Object write operations had a 100% error rate.</span><br><br><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">* Object reads had an approximate error rate of 35% globally. Individual customer error rate varied during this window depending on access patterns. Customers accessing public assets through custom domains would have seen a reduced error rate as cached object reads were not impacted.</span><br><br><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">* Operations involving metadata only (e.g., head and list operations) were not impacted.</span><br><br><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">There was no data loss or risk to data integrity within R2's storage subsystem. This incident was limited to a temporary authentication issue between R2's API frontend and our storage infrastructure.</span></td>
				</tr>
				<tr>
					<td class="tg-fymr"><span style="font-style:normal;text-decoration:none;color:#000;background-color:transparent">Billing</span></td>
					<td class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">Billing uses R2 to store customer invoices. During the primary incident window, customers may have experienced errors when attempting to download/access past Cloudflare invoices.</span></td>
				</tr>
				<tr>
					<td class="tg-fymr"><span style="font-style:normal;text-decoration:none;color:#000;background-color:transparent">Cache Reserve</span></td>
					<td class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">Cache Reserve customers observed an increase in requests to their origin during the incident window as an increased percentage of reads to R2 failed. This resulted in an increase in requests to origins to fetch assets unavailable in Cache Reserve during this period.</span><br><br><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">User-facing requests for assets to sites with Cache Reserve did not observe failures as cache misses failed over to the origin.</span></td>
				</tr>
				<tr>
					<td class="tg-fymr"><span style="font-style:normal;text-decoration:none;color:#000;background-color:transparent">Email Security</span></td>
					<td class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">Email Security depends on R2 for customer-facing metrics. During the primary incident window, customer-facing metrics would not have updated.</span></td>
				</tr>
				<tr>
					<td class="tg-fymr"><span style="font-style:normal;text-decoration:none;color:#000;background-color:transparent">Images</span></td>
					<td class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">All (100% of) uploads failed during the primary incident window. Successful delivery of stored images dropped to approximately 25%.</span></td>
				</tr>
				<tr>
					<td class="tg-fymr"><span style="font-style:normal;text-decoration:none;color:#000;background-color:transparent">Key Transparency Auditor</span></td>
					<td class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">All (100% of) operations failed during the primary incident window due to dependence on R2 writes and/or reads. Once the incident was resolved, service returned to normal operation immediately.</span></td>
				</tr>
				<tr>
					<td class="tg-fymr"><span style="font-style:normal;text-decoration:none;color:#000;background-color:transparent">Log Delivery</span></td>
					<td class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">Log delivery (for Logpush and Logpull) was delayed during the primary incident window, resulting in significant delays (up to 70 minutes) in log processing. All logs were delivered after incident resolution.</span></td>
				</tr>
				<tr>
					<td class="tg-fymr"><span style="font-style:normal;text-decoration:none;color:#000;background-color:transparent">Stream</span></td>
					<td class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">All (100% of) uploads failed during the primary incident window. Successful Stream video segment delivery dropped to 94%. Viewers may have seen video stalls every minute or so, although actual impact would have varied.</span><br><br><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">Stream Live was down during the primary incident window as it depends on object writes.</span></td>
				</tr>
				<tr>
					<td class="tg-fymr"><span style="font-style:normal;text-decoration:none;color:#000;background-color:transparent">Vectorize</span></td>
					<td class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">Queries and operations against Vectorize indexes were impacted during the incident window. During the incident window, Vectorize customers would have seen an increased error rate for read queries to indexes and all (100% of) insert and upsert operation failed as Vectorize depends on R2 for persistent storage.</span></td>
				</tr>
			</tbody>
		</table>
	</div>
	<div class="flex anchor relative">
		<h2 id="incident-timeline">Incident timeline</h2>
		<a href="https://blog.cloudflare.com/#incident-timeline" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p><b>All timestamps referenced are in Coordinated Universal Time (UTC).</b></p>
	<style type="text/css">
		.tg {
			border-collapse: collapse;
			border-spacing: 0;
			margin: 0px auto;
		}

		.tg td {
			border-color: black;
			border-style: solid;
			border-width: 1px;
			font-family: Arial, sans-serif;
			font-size: 14px;
			overflow: hidden;
			padding: 10px 5px;
			word-break: normal;
		}

		.tg th {
			border-color: black;
			border-style: solid;
			border-width: 1px;
			font-family: Arial, sans-serif;
			font-size: 14px;
			font-weight: normal;
			overflow: hidden;
			padding: 10px 5px;
			word-break: normal;
		}

		.tg .tg-1wig {
			font-weight: bold;
			text-align: left;
			vertical-align: top
		}

		.tg .tg-0lax {
			text-align: left;
			vertical-align: top
		}

		@media screen and (max-width: 767px) {
			.tg {
				width: auto !important;
			}

			.tg col {
				width: auto !important;
			}

			.tg-wrap {
				overflow-x: auto;
				-webkit-overflow-scrolling: touch;
				margin: auto 0px;
			}
		}
	</style>
	<div class="tg-wrap">
		<table class="tg" style="undefined;table-layout: fixed; width: 762px">
			<colgroup>
				<col style="width: 185.88889px">
				<col style="width: 575.88889px">
			</colgroup>
			<thead>
				<tr>
					<th class="tg-0lax"><span style="font-weight:bold;font-style:normal;text-decoration:none;color:#000;background-color:transparent">Time</span></th>
					<th class="tg-1wig"><span style="font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent">Event</span></th>
				</tr>
			</thead>
			<tbody>
				<tr>
					<td class="tg-1wig"><span style="font-style:normal;text-decoration:none;color:#000;background-color:transparent">Mar 21, 2025 - 19:49 UTC</span></td>
					<td class="tg-0lax"><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">The R2 engineering team started the credential rotation process. A new set of credentials (ID and key pair) for storage infrastructure was created. Old credentials were maintained to avoid downtime during credential change over.</span></td>
				</tr>
				<tr>
					<td class="tg-1wig"><span style="font-style:normal;text-decoration:none;color:#000;background-color:transparent">Mar 21, 2025 - 20:19 UTC</span></td>
					<td class="tg-0lax"><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">Set updated production secret (</span><span style="font-weight:400;font-style:normal;text-decoration:none;color:#188038;background-color:transparent">wrangler secret put</span><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">) and executed </span><span style="font-weight:400;font-style:normal;text-decoration:none;color:#188038;background-color:transparent">wrangler deploy</span><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent"> command to deploy R2 Gateway service with updated credentials. </span><br><br><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">Note: We later discovered the </span><span style="font-weight:400;font-style:normal;text-decoration:none;color:#188038;background-color:transparent">--env</span><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent"> parameter was inadvertently omitted for both Wrangler commands. This resulted in credentials being deployed to the Worker assigned to the </span><span style="font-weight:400;font-style:normal;text-decoration:none;color:#188038;background-color:transparent">default</span><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent"> environment instead of the Worker assigned to the </span><span style="font-weight:400;font-style:normal;text-decoration:none;color:#188038;background-color:transparent">production</span><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent"> environment.</span></td>
				</tr>
				<tr>
					<td class="tg-1wig"><span style="font-style:normal;text-decoration:none;color:#000;background-color:transparent">Mar 21, 2025 - 20:20 UTC</span></td>
					<td class="tg-0lax"><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">The R2 Gateway service Worker assigned to the </span><span style="font-weight:400;font-style:normal;text-decoration:none;color:#188038;background-color:transparent">default</span><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent"> environment is now using the updated storage infrastructure credentials.</span><br><br><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">Note: This was the wrong Worker, the </span><span style="font-weight:400;font-style:normal;text-decoration:none;color:#188038;background-color:transparent">production</span><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent"> environment should have been explicitly set. But, at this point, we incorrectly believed the credentials were updated on the correct production Worker.</span></td>
				</tr>
				<tr>
					<td class="tg-1wig"><span style="font-style:normal;text-decoration:none;color:#000;background-color:transparent">Mar 21, 2025 - 20:37 UTC</span></td>
					<td class="tg-0lax"><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">Old credentials were removed from our storage infrastructure to complete the credential rotation process.</span></td>
				</tr>
				<tr>
					<td class="tg-1wig"><span style="font-style:normal;text-decoration:none;color:#000;background-color:transparent">Mar 21, 2025 - 21:38 UTC</span></td>
					<td class="tg-1wig"><span style="font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent">– IMPACT BEGINS –</span><br><br><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">R2 availability metrics begin to show signs of service degradation. The impact to R2 availability metrics was gradual and not immediately obvious because there was a delay in the propagation of the previous credential deletion to storage infrastructure.</span></td>
				</tr>
				<tr>
					<td class="tg-1wig"><span style="font-style:normal;text-decoration:none;color:#000;background-color:transparent">Mar 21, 2025 - 21:45 UTC</span></td>
					<td class="tg-0lax"><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">R2 global availability alerts are triggered (indicating 2% of error budget burn rate).</span><br><br><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">The R2 engineering team began looking at operational dashboards and logs to understand impact.</span></td>
				</tr>
				<tr>
					<td class="tg-1wig"><span style="font-style:normal;text-decoration:none;color:#000;background-color:transparent">Mar 21, 2025 - 21:50 UTC</span></td>
					<td class="tg-0lax"><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">Internal incident declared.</span></td>
				</tr>
				<tr>
					<td class="tg-1wig"><span style="font-style:normal;text-decoration:none;color:#000;background-color:transparent">Mar 21, 2025 - 21:51 UTC</span></td>
					<td class="tg-0lax"><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">R2 engineering team observes gradual but consistent decline in R2 availability metrics for both read and write operations. Operations involving metadata only (e.g., head and list operations) were not impacted.</span><br><br><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">Given gradual decline in availability metrics, R2 engineering team suspected a potential regression in propagation of new credentials in storage infrastructure.</span></td>
				</tr>
				<tr>
					<td class="tg-1wig"><span style="font-style:normal;text-decoration:none;color:#000;background-color:transparent">Mar 21, 2025 - 22:05 UTC</span></td>
					<td class="tg-0lax"><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">Public incident status page published.</span></td>
				</tr>
				<tr>
					<td class="tg-1wig"><span style="font-style:normal;text-decoration:none;color:#000;background-color:transparent">Mar 21, 2025 - 22:15 UTC</span></td>
					<td class="tg-0lax"><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">R2 engineering team created a new set of credentials (ID and key pair) for storage infrastructure in an attempt to force re-propagation.</span><br><br><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">Continued monitoring operational dashboards and logs.</span></td>
				</tr>
				<tr>
					<td class="tg-1wig"><span style="font-style:normal;text-decoration:none;color:#000;background-color:transparent">Mar 21, 2025 - 22:20 UTC</span></td>
					<td class="tg-0lax"><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">R2 engineering team saw no improvement in availability metrics. Continued investigating other potential root causes.</span></td>
				</tr>
				<tr>
					<td class="tg-1wig"><span style="font-style:normal;text-decoration:none;color:#000;background-color:transparent">Mar 21, 2025 - 22:30 UTC</span></td>
					<td class="tg-0lax"><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">R2 engineering team deployed a new set of credentials (ID and key pair) to R2 Gateway service Worker. This was to validate whether there was an issue with the credentials we had pushed to gateway service.</span><br><br><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">Environment parameter was still omitted in the </span><span style="font-weight:400;font-style:normal;text-decoration:none;color:#188038;background-color:transparent">deploy</span><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent"> and </span><span style="font-weight:400;font-style:normal;text-decoration:none;color:#188038;background-color:transparent">secret put</span><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent"> commands, so this deployment was still to the wrong non-production Worker.</span></td>
				</tr>
				<tr>
					<td class="tg-1wig"><span style="font-style:normal;text-decoration:none;color:#000;background-color:transparent">Mar 21, 2025 - 22:36 UTC</span></td>
					<td class="tg-1wig"><span style="font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent">– ROOT CAUSE IDENTIFIED –</span><br><br><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">The R2 engineering team discovered that credentials had been deployed to a non-production Worker by reviewing production Worker release history.</span></td>
				</tr>
				<tr>
					<td class="tg-1wig"><span style="font-style:normal;text-decoration:none;color:#000;background-color:transparent">Mar 21, 2025 - 22:45 UTC</span></td>
					<td class="tg-1wig"><span style="font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent">– IMPACT ENDS –</span><br><br><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">Deployed credentials to correct production Worker. R2 availability recovered.</span></td>
				</tr>
				<tr>
					<td class="tg-1wig"><span style="font-style:normal;text-decoration:none;color:#000;background-color:transparent">Mar 21, 2025 - 22:54 UTC</span></td>
					<td class="tg-0lax"><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">The incident is considered resolved.</span></td>
				</tr>
			</tbody>
		</table>
	</div>
	<div class="flex anchor relative">
		<h2 id="analysis">Analysis</h2>
		<a href="https://blog.cloudflare.com/#analysis" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>R2’s architecture is primarily composed of three parts: R2 production gateway Worker (serves requests from S3 API, REST API, Workers API), metadata service, and storage infrastructure (stores encrypted object data).</p>
	<figure class="kg-card kg-image-card">
		<img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/1G9gfdEE4RIuMIUeNz42RN/25d1b4cca187a4a24c600a43ba51fb71/BLOG-2793_2.png" alt="BLOG-2793 2" class="kg-image" width="1840" height="1000" loading="lazy">
	</figure>
	<p>The R2 Gateway Worker uses credentials (ID and key pair) to securely authenticate with our distributed storage infrastructure. We rotate these credentials regularly as a best practice security precaution.</p>
	<p>Our key rotation process involves the following high-level steps:</p>
	<ol>
		<li>
			<p>Create a new set of credentials (ID and key pair) for our storage infrastructure. At this point, the old credentials are maintained to avoid downtime during credential change over.</p>
		</li>
		<li>
			<p>Set the new credential secret for the R2 production gateway Worker using the <code>wrangler secret put</code> command.</p>
		</li>
		<li>
			<p>Set the new updated credential ID as an environment variable in the R2 production gateway Worker using the <code>wrangler deploy</code> command. At this point, new storage credentials start being used by the gateway Worker.</p>
		</li>
		<li>
			<p>Remove previous credentials from our storage infrastructure to complete the credential rotation process.</p>
		</li>
		<li>
			<p>Monitor operational dashboards and logs to validate change over.</p>
		</li>
	</ol>
	<p>The R2 engineering team uses <a href="https://developers.cloudflare.com/workers/wrangler/environments"><u>Workers environments</u></a> to separate production and development environments for the R2 Gateway Worker. Each environment defines a separate isolated Cloudflare Worker with separate environment variables and secrets.&nbsp;</p>
	<p>Critically, both <code>wrangler secret put</code> and <code>wrangler deploy</code> commands default to the default environment if the --env command line parameter is not included. In this case, due to human error, we inadvertently omitted the --env parameter and deployed the new storage credentials to the wrong Worker (<code>default</code> environment instead of <code>production</code>). To correctly deploy storage credentials to the production R2 Gateway Worker, we need to specify <code>--env production</code>.</p>
	<p>The action we took on step 4 above to remove the old credentials from our storage infrastructure caused authentication errors, as the R2 Gateway production Worker still had the old credentials. This is ultimately what resulted in degraded availability.</p>
	<p>The decline in R2 availability metrics was gradual and not immediately obvious because there was a delay in the propagation of the previous credential deletion to storage infrastructure. This accounted for a delay in our initial discovery of the problem. Instead of relying on availability metrics after updating the old set of credentials, we should have explicitly validated which token was being used by the R2 Gateway service to authenticate with R2's storage infrastructure.</p>
	<p>Overall, the impact on read availability was significantly mitigated by our intermediate cache that sits in front of storage and continued to serve requests.</p>
	<div class="flex anchor relative">
		<h2 id="resolution">Resolution</h2>
		<a href="https://blog.cloudflare.com/#resolution" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>Once we identified the root cause, we were able to resolve the incident quickly by deploying the new credentials to the production R2 Gateway Worker. This resulted in an immediate recovery of R2 availability.</p>
	<div class="flex anchor relative">
		<h2 id="next-steps">Next steps</h2>
		<a href="https://blog.cloudflare.com/#next-steps" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>This incident happened because of human error and lasted longer than it should have because we didn’t have proper visibility into which credentials were being used by the R2 Gateway Worker to authenticate with our storage infrastructure.</p>
	<p>We have taken immediate steps to prevent this failure (and others like it) from happening again:</p>
	<ul>
		<li>
			<p>Added logging tags that include the suffix of the credential ID the R2 Gateway Worker uses to authenticate with our storage infrastructure. With this change, we can explicitly confirm which credential is being used.</p>
		</li>
		<li>
			<p>Related to the above step, our internal processes now require explicit confirmation that the suffix of the new token ID matches logs from our storage infrastructure before deleting the previous token.</p>
		</li>
		<li>
			<p>Require that key rotation takes place through our hotfix release tooling instead of relying on manual wrangler command entry which introduces human error. Our hotfix release deploy tooling explicitly enforces the environment configuration and contains other safety checks.</p>
		</li>
		<li>
			<p>While it’s been an implicit standard that this process involves at least two humans to validate the changes ahead as we progress, we’ve updated our relevant SOPs (standard operating procedures) to include this explicitly.</p>
		</li>
		<li>
			<p><b>In Progress</b>: Extend our existing closed loop health check system that monitors our endpoints to test new keys, automate reporting of their status through our alerting platform, and ensure global propagation prior to releasing the gateway Worker.</p>
		</li>
		<li>
			<p><b>In Progress</b>: To expedite triage on any future issues with our distributed storage endpoints, we are updating our <a href="https://www.cloudflare.com/learning/performance/what-is-observability">observability</a> platform to include views of upstream success rates that bypass caching to give clearer indication of issues serving requests for any reason.</p>
		</li>
	</ul>
	<p>The list above is not exhaustive: as we work through the above items, we will likely uncover other improvements to our systems, controls, and processes that we’ll be applying to improve R2’s resiliency, on top of our business-as-usual efforts. We are confident that this set of changes will prevent this failure, and related credential rotation failure modes, from occurring again. Again, we sincerely apologize for this incident and deeply regret any disruption it has caused you or your users.</p>
</div>