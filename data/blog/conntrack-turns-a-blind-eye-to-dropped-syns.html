<div class="post-content lh-copy gray1">
	<h2 id="intro">Intro</h2>
	<p>We have been working with conntrack, the connection tracking layer in the Linux kernel, for years. And yet, despite the collected know-how, questions about its inner workings occasionally come up. When they do, it is hard to resist the temptation to go digging for answers.</p>
	<p>One such question popped up while writing <a href="https://blog.cloudflare.com/conntrack-tales-one-thousand-and-one-flows/">the previous blog post on conntrack</a>:</p>
	<blockquote>‚ÄúWhy are there no entries in the conntrack table for SYN packets dropped by the firewall?‚Äù</blockquote>
	<p>Ready for a deep dive into the network stack? Let‚Äôs find out.</p>
	<figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://lh5.googleusercontent.com/tqbla_C4bF9esDdiQKx-12wXVI3xKv6IDUklAgB0zu6G4KiC3ziZeCJkSgUWechnpaCnFBL6XY_glt1HNaXDqURrRm6ttta7ciHiG8vidp7x6Th0eQUqXQF4Ure1QsnrAlK36qQ35f66ztX1VA" class="kg-image">
		<figcaption><a href="https://pixabay.com/photos/female-diver-sea-scuba-diving-4829158/" target="_blank"><em>Image</em></a><em> by </em><a href="https://pixabay.com/users/chulmin1700-15022416/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=4829158" target="_blank"><em>chulmin park</em></a><em> from </em><a href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=4829158" target="_blank"><em>Pixabay</em></a></figcaption>
	</figure>
	<p>We already know from <a href="https://blog.cloudflare.com/conntrack-tales-one-thousand-and-one-flows/">last time</a> that conntrack is in charge of tracking incoming and outgoing network traffic. By running conntrack -L we can inspect existing network flows, or as conntrack calls them, connections.</p>
	<p>So if we spin up a <a href="https://github.com/cloudflare/cloudflare-blog/blob/master/2021-03-conntrack-syn-drop/Vagrantfile" target="_blank">toy VM</a>, <em>connect</em> to it over SSH, and inspect the contents of the conntrack table, we will see‚Ä¶</p>
	<!--kg-card-begin: markdown-->
	<pre><code class="language-ssh">$ vagrant init fedora/33-cloud-base
$ vagrant up
‚Ä¶
$ vagrant ssh
Last login: Sun Jan 31 15:08:02 2021 from 192.168.122.1
[vagrant@ct-vm ~]$ sudo conntrack -L
conntrack v1.4.5 (conntrack-tools): 0 flow entries have been shown.
</code></pre>
	<!--kg-card-end: markdown-->
	<p>‚Ä¶ nothing!</p>
	<p>Even though the conntrack kernel module is loaded:</p>
	<!--kg-card-begin: markdown-->
	<pre><code class="language-ssh">[vagrant@ct-vm ~]$ lsmod | grep '^nf_conntrack\b'
nf_conntrack          163840  1 nf_conntrack_netlink
</code></pre>
	<!--kg-card-end: markdown-->
	<p>Hold on a minute. Why is the SSH connection to the VM not listed in conntrack entries? SSH is working. With each keystroke we are sending packets to the VM. But conntrack doesn‚Äôt register it.</p>
	<p>Isn‚Äôt conntrack an integral part of the network stack that sees every packet passing through it? ü§î</p>
	<figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.cloudflare.com/content/images/2021/03/image1-5.png" class="kg-image">
		<figcaption>Based on an <a href="https://commons.wikimedia.org/wiki/File:Netfilter-packet-flow.svg" target="_blank"><em>image</em></a><em> by </em><a href="https://commons.wikimedia.org/wiki/User_talk:Jengelh" target="_blank"><em>Jan Engelhardt</em></a><em> CC BY-SA 3.0</em></figcaption>
	</figure>
	<p>Clearly everything we learned about conntrack last time is not the whole story.</p>
	<h2 id="calling-into-conntrack">Calling into conntrack</h2>
	<p>Our little experiment with SSH‚Äôing into a VM begs the question ‚Äî how does conntrack actually get notified about network packets passing through the stack?</p>
	<p>We can walk <a href="https://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data/" target="_blank">the receive path step by step</a> and we won‚Äôt find any direct calls into the conntrack code in either the IPv4 or IPv6 stack. Conntrack does not interface with the network stack directly.</p>
	<p>Instead, it relies on the Netfilter framework, and its set of hooks baked into in the stack:</p>
	<!--kg-card-begin: markdown-->
	<pre><code class="language-ssh">int ip_rcv(struct sk_buff *skb, struct net_device *dev, ‚Ä¶)
{
    ‚Ä¶
    return NF_HOOK(NFPROTO_IPV4, NF_INET_PRE_ROUTING,
               net, NULL, skb, dev, NULL,
               ip_rcv_finish);
}
</code></pre>
	<!--kg-card-end: markdown-->
	<p>Netfilter users, like conntrack, can register callbacks with it. Netfilter will then run all registered callbacks when its hook processes a network packet.</p>
	<p>For the INET family, that is IPv4 and IPv6, there are five Netfilter hooks &nbsp;to choose from:</p>
	<figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.cloudflare.com/content/images/2021/03/image5-1.png" class="kg-image">
		<figcaption>Based on <a href="https://thermalcircle.de/doku.php?id=blog:linux:nftables_packet_flow_netfilter_hooks_detail" target="_blank"><em>Nftables - Packet flow and Netfilter hooks in detail</em></a><em>, </em><a href="https://thermalcircle.de/" target="_blank"><em>thermalcircle.de</em></a><em>, </em><a href="https://creativecommons.org/licenses/by-sa/4.0/deed.en" target="_blank"><em>CC BY-SA 4.0</em></a></figcaption>
	</figure>
	<p>Which ones does conntrack use? We will get to that in a moment.</p>
	<p>First, let‚Äôs focus on the trigger. What makes conntrack register its callbacks with Netfilter?</p>
	<p>The SSH connection doesn‚Äôt show up in the conntrack table just because the module is loaded. We already saw that. This means that conntrack doesn‚Äôt register its callbacks with Netfilter at module load time.</p>
	<p>Or at least, it doesn't do it by <em>default</em>. Since Linux v5.1 (May 2019) the conntrack module has the <a href="https://github.com/torvalds/linux/commit/ba3fbe663635ae7b33a2d972c5d2def036258e42" target="_blank">enable_hooks parameter</a>, which causes conntrack to register its callbacks on load:</p>
	<!--kg-card-begin: markdown-->
	<pre><code class="language-ssh">[vagrant@ct-vm ~]$ modinfo nf_conntrack
‚Ä¶
parm:           enable_hooks:Always enable conntrack hooks (bool)
</code></pre>
	<!--kg-card-end: markdown-->
	<p>Going back to our toy VM, let‚Äôs try to reload the conntrack module with enable_hooks set:</p>
	<!--kg-card-begin: markdown-->
	<pre><code class="language-ssh">[vagrant@ct-vm ~]$ sudo rmmod nf_conntrack_netlink nf_conntrack
[vagrant@ct-vm ~]$ sudo modprobe nf_conntrack enable_hooks=1
[vagrant@ct-vm ~]$ sudo conntrack -L
tcp      6 431999 ESTABLISHED src=192.168.122.204 dst=192.168.122.1 sport=22 dport=34858 src=192.168.122.1 dst=192.168.122.204 sport=34858 dport=22 [ASSURED] mark=0 secctx=system_u:object_r:unlabeled_t:s0 use=1
conntrack v1.4.5 (conntrack-tools): 1 flow entries have been shown.
[vagrant@ct-vm ~]$
</code></pre>
	<!--kg-card-end: markdown-->
	<p>Nice! The conntrack table now contains an entry for our SSH session.</p>
	<p>The Netfilter hook notified conntrack about SSH session packets passing through the stack.</p>
	<p>Now that we know how conntrack gets called, we can go back to our question ‚Äî can we observe a TCP SYN packet dropped by the firewall with conntrack?</p>
	<h2 id="listing-netfilter-hooks">Listing Netfilter hooks</h2>
	<p>That is easy to check:</p>
	<!--kg-card-begin: markdown-->
	<ol>
		<li>Add a rule to drop anything coming to port tcp/2570<sup>2</sup></li>
	</ol>
	<!--kg-card-end: markdown-->
	<!--kg-card-begin: markdown-->
	<pre><code class="language-ssh">[vagrant@ct-vm ~]$ sudo iptables -t filter -A INPUT -p tcp --dport 2570 -j DROP
</code></pre>
	<!--kg-card-end: markdown-->
	<p>2) Connect to the VM on port tcp/2570 from the outside</p>
	<!--kg-card-begin: markdown-->
	<pre><code class="language-ssh">host $ nc -w 1 -z 192.168.122.204 2570
</code></pre>
	<!--kg-card-end: markdown-->
	<p>3) List conntrack table entries</p>
	<!--kg-card-begin: markdown-->
	<pre><code class="language-ssh">[vagrant@ct-vm ~]$ sudo conntrack -L
tcp      6 431999 ESTABLISHED src=192.168.122.204 dst=192.168.122.1 sport=22 dport=34858 src=192.168.122.1 dst=192.168.122.204 sport=34858 dport=22 [ASSURED] mark=0 secctx=system_u:object_r:unlabeled_t:s0 use=1
conntrack v1.4.5 (conntrack-tools): 1 flow entries have been shown.
</code></pre>
	<!--kg-card-end: markdown-->
	<p>No new entries. Conntrack didn‚Äôt record a new flow for the dropped SYN.</p>
	<p>But did it process the SYN packet? To answer that we have to find out which callbacks conntrack registered with Netfilter.</p>
	<p>Netfilter keeps track of callbacks registered for each hook in instances of <code>struct nf_hook_entries</code>. We can reach these objects through the Netfilter state (<code>struct netns_nf</code>), which lives inside network namespace (<code>struct net</code>).</p>
	<!--kg-card-begin: markdown-->
	<pre><code class="language-ssh">struct netns_nf {
    ‚Ä¶
    struct nf_hook_entries __rcu *hooks_ipv4[NF_INET_NUMHOOKS];
    struct nf_hook_entries __rcu *hooks_ipv6[NF_INET_NUMHOOKS];
    ‚Ä¶
}
</code></pre>
	<!--kg-card-end: markdown-->
	<p><code>struct nf_hook_entries</code>, if you look at its <a href="https://elixir.bootlin.com/linux/v5.10.14/source/include/linux/netfilter.h#L101" target="_blank">definition</a>, is a bit of an exotic construct. A glance at how the object size is calculated during its <a href="https://elixir.bootlin.com/linux/v5.10.14/source/net/netfilter/core.c#L50" target="_blank">allocation</a> gives a hint about its memory layout:</p>
	<!--kg-card-begin: markdown-->
	<pre><code class="language-ssh">    struct nf_hook_entries *e;
    size_t alloc = sizeof(*e) +
               sizeof(struct nf_hook_entry) * num +
               sizeof(struct nf_hook_ops *) * num +
               sizeof(struct nf_hook_entries_rcu_head);
</code></pre>
	<!--kg-card-end: markdown-->
	<p>It‚Äôs an element count, followed by two arrays glued together, and some <a href="https://en.wikipedia.org/wiki/Read-copy-update" target="_blank">RCU</a>-related state which we‚Äôre going to ignore. The two arrays have the same size, but hold different kinds of values.</p>
	<p>We can walk the second array, holding pointers to struct nf_hook_ops, to discover the registered callbacks and their priority. Priority determines the invocation order.</p>
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2021/03/image3-3.png" class="kg-image"></figure>
	<p>With <a href="https://github.com/osandov/drgn" target="_blank">drgn</a>, a programmable C debugger tailored for the Linux kernel, we can locate the Netfilter state in kernel memory, and walk its contents relatively easily. Given we know what we are looking for.</p>
	<!--kg-card-begin: markdown-->
	<pre><code class="language-ssh">[vagrant@ct-vm ~]$ sudo drgn
drgn 0.0.8 (using Python 3.9.1, without libkdumpfile)
‚Ä¶
&gt;&gt;&gt; pre_routing_hook = prog['init_net'].nf.hooks_ipv4[0]
&gt;&gt;&gt; for i in range(0, pre_routing_hook.num_hook_entries):
...     pre_routing_hook.hooks[i].hook
...
(nf_hookfn *)ipv4_conntrack_defrag+0x0 = 0xffffffffc092c000
(nf_hookfn *)ipv4_conntrack_in+0x0 = 0xffffffffc093f290
&gt;&gt;&gt;
</code></pre>
	<!--kg-card-end: markdown-->
	<p>Neat! We have a way to access Netfilter state.</p>
	<p>Let‚Äôs take it to the next level and list all registered callbacks for each Netfilter hook (using <a href="https://github.com/cloudflare/cloudflare-blog/blob/master/2021-03-conntrack-syn-drop/tools/list-nf-hooks" target="_blank">less than 100 lines of Python</a>):</p>
	<!--kg-card-begin: markdown-->
	<pre><code class="language-ssh">[vagrant@ct-vm ~]$ sudo /vagrant/tools/list-nf-hooks
ü™ù ipv4 PRE_ROUTING
       -400 ‚Üí ipv4_conntrack_defrag     ‚òú conntrack callback
       -300 ‚Üí iptable_raw_hook
       -200 ‚Üí ipv4_conntrack_in         ‚òú conntrack callback
       -150 ‚Üí iptable_mangle_hook
       -100 ‚Üí nf_nat_ipv4_in

ü™ù ipv4 LOCAL_IN
       -150 ‚Üí iptable_mangle_hook
          0 ‚Üí iptable_filter_hook
         50 ‚Üí iptable_security_hook
        100 ‚Üí nf_nat_ipv4_fn
 2147483647 ‚Üí ipv4_confirm
‚Ä¶
</code></pre>
	<!--kg-card-end: markdown-->
	<p>The output from our script shows that conntrack has two callbacks registered with the <code>PRE_ROUTING</code> hook - <code>ipv4_conntrack_defrag</code> and <code>ipv4_conntrack_in</code>. But are they being called?</p>
	<figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.cloudflare.com/content/images/2021/03/image4-3.png" class="kg-image">
		<figcaption>Based on <a href="https://thermalcircle.de/doku.php?id=blog:linux:nftables_packet_flow_netfilter_hooks_detail" target="_blank"><em>Netfilter PRE_ROUTING hook</em></a><em>, </em><a href="https://thermalcircle.de/" target="_blank"><em>thermalcircle.de</em></a><em>, </em><a href="https://creativecommons.org/licenses/by-sa/4.0/deed.en" target="_blank"><em>CC BY-SA 4.0</em></a></figcaption>
	</figure>
	<h2 id="tracing-conntrack-callbacks">Tracing conntrack callbacks</h2>
	<p>We expect that when the Netfilter <code>PRE_ROUTING</code> hook processes a TCP SYN packet, it will invoke <code>ipv4_conntrack_defrag</code> and then <code>ipv4_conntrack_in</code> callbacks.</p>
	<p>To confirm it we will put to use the tracing powers of <a href="https://ebpf.io/" target="_blank">BPF üêù</a>. BPF programs can run on entry to functions. These kinds of programs are known as BPF kprobes. In our case we will attach BPF kprobes to conntrack callbacks.</p>
	<p>Usually, when working with BPF, we would write the BPF program in C and use <code>clang -target bpf</code> to compile it. However, for tracing it will be much easier to use <a href="https://bpftrace.org/" target="_blank">bpftrace</a>. With bpftrace we can write <a href="https://github.com/cloudflare/cloudflare-blog/blob/master/2021-03-conntrack-syn-drop/tools/trace-conntrack-prerouting.bt" target="_blank">our BPF kprobe program</a> in a <a href="https://github.com/iovisor/bpftrace/blob/master/docs/reference_guide.md" target="_blank">high-level language</a> inspired by <a href="https://en.wikipedia.org/wiki/AWK" target="_blank">AWK</a>:</p>
	<!--kg-card-begin: markdown-->
	<pre><code class="language-ssh">kprobe:ipv4_conntrack_defrag,
kprobe:ipv4_conntrack_in
{
    $skb = (struct sk_buff *)arg1;
    $iph = (struct iphdr *)($skb-&gt;head + $skb-&gt;network_header);
    $th = (struct tcphdr *)($skb-&gt;head + $skb-&gt;transport_header);

    if ($iph-&gt;protocol == 6 /* IPPROTO_TCP */ &amp;&amp;
        $th-&gt;dest == 2570 /* htons(2570) */ &amp;&amp;
        $th-&gt;syn == 1) {
        time("%H:%M:%S ");
        printf("%s:%u &gt; %s:%u tcp syn %s\n",
               ntop($iph-&gt;saddr),
               (uint16)($th-&gt;source &lt;&lt; 8) | ($th-&gt;source &gt;&gt; 8),
               ntop($iph-&gt;daddr),
               (uint16)($th-&gt;dest &lt;&lt; 8) | ($th-&gt;dest &gt;&gt; 8),
               func);
    }
}
</code></pre>
	<!--kg-card-end: markdown-->
	<p>What does this program do? It is roughly an equivalent of a tcpdump filter:</p>
	<!--kg-card-begin: markdown-->
	<pre><code class="language-ssh">dst port 2570 and tcp[tcpflags] &amp; tcp-syn != 0
</code></pre>
	<!--kg-card-end: markdown-->
	<p>But only for packets passing through conntrack <code>PRE_ROUTING</code> callbacks.</p>
	<p>(If you haven‚Äôt used bpftrace, it comes with an excellent <a href="https://github.com/iovisor/bpftrace/blob/master/docs/reference_guide.md" target="_blank">reference guide</a> and gives you the ability to explore kernel data types on the fly with <code>bpftrace -lv 'struct iphdr'</code>.)</p>
	<p>Let‚Äôs run the tracing program while we connect to the VM from the outside (<code>nc -z 192.168.122.204 2570</code>):</p>
	<!--kg-card-begin: markdown-->
	<pre><code class="language-ssh">[vagrant@ct-vm ~]$ sudo bpftrace /vagrant/tools/trace-conntrack-prerouting.bt
Attaching 3 probes...
Tracing conntrack prerouting callbacks... Hit Ctrl-C to quit
13:22:56 192.168.122.1:33254 &gt; 192.168.122.204:2570 tcp syn ipv4_conntrack_defrag
13:22:56 192.168.122.1:33254 &gt; 192.168.122.204:2570 tcp syn ipv4_conntrack_in
^C

[vagrant@ct-vm ~]$
</code></pre>
	<!--kg-card-end: markdown-->
	<p>Conntrack callbacks have processed the TCP SYN packet destined to tcp/2570.</p>
	<p>But if conntrack saw the packet, why is there no corresponding flow entry in the conntrack table?</p>
	<h2 id="going-down-the-rabbit-hole">Going down the rabbit hole</h2>
	<p>What actually happens inside the conntrack <code>PRE_ROUTING</code> callbacks?</p>
	<p>To find out, we can trace the call chain that starts on entry to the conntrack callback. The <code>function_graph</code> tracer built into the <a href="https://lwn.net/Articles/365835/" target="_blank">Ftrace</a> framework is perfect for this task.</p>
	<p>But because all incoming traffic goes through the <code>PRE_ROUTING</code> hook, including our SSH connection, our trace will be polluted with events from SSH traffic. To avoid that, let‚Äôs switch from SSH access to a serial console.</p>
	<p>When using <a href="https://github.com/vagrant-libvirt/vagrant-libvirt" target="_blank">libvirt</a> as the Vagrant provider, you can connect to the serial console with <code>virsh</code>:</p>
	<!--kg-card-begin: markdown-->
	<pre><code class="language-ssh">host $ virsh -c qemu:///session list
 Id   Name                State
-----------------------------------
 1    conntrack_default   running

host $ virsh -c qemu:///session console conntrack_default
Once connected to the console and logged into the VM, we can record the call chain using the trace-cmd wrapper for Ftrace:
[vagrant@ct-vm ~]$ sudo trace-cmd start -p function_graph -g ipv4_conntrack_defrag -g ipv4_conntrack_in
  plugin 'function_graph'
[vagrant@ct-vm ~]$ # ‚Ä¶ connect from the host with `nc -z 192.168.122.204 2570` ‚Ä¶
[vagrant@ct-vm ~]$ sudo trace-cmd stop
[vagrant@ct-vm ~]$ sudo cat /sys/kernel/debug/tracing/trace
# tracer: function_graph
#
# CPU  DURATION                  FUNCTION CALLS
# |     |   |                     |   |   |   |
 1)   1.219 us    |  finish_task_switch();
 1)   3.532 us    |  ipv4_conntrack_defrag [nf_defrag_ipv4]();
 1)               |  ipv4_conntrack_in [nf_conntrack]() {
 1)               |    nf_conntrack_in [nf_conntrack]() {
 1)   0.573 us    |      get_l4proto [nf_conntrack]();
 1)               |      nf_ct_get_tuple [nf_conntrack]() {
 1)   0.487 us    |        nf_ct_get_tuple_ports [nf_conntrack]();
 1)   1.564 us    |      }
 1)   0.820 us    |      hash_conntrack_raw [nf_conntrack]();
 1)   1.255 us    |      __nf_conntrack_find_get [nf_conntrack]();
 1)               |      init_conntrack.constprop.0 [nf_conntrack]() {  ‚ù∑
 1)   0.427 us    |        nf_ct_invert_tuple [nf_conntrack]();
 1)               |        __nf_conntrack_alloc [nf_conntrack]() {      ‚ù∂
                             ‚Ä¶ 
 1)   3.680 us    |        }
                           ‚Ä¶ 
 1) + 15.847 us   |      }
                         ‚Ä¶ 
 1) + 34.595 us   |    }
 1) + 35.742 us   |  }
 ‚Ä¶
[vagrant@ct-vm ~]$
</code></pre>
	<!--kg-card-end: markdown-->
	<p>What catches our attention here is the allocation, <a href="https://elixir.bootlin.com/linux/v5.10.14/source/net/netfilter/nf_conntrack_core.c#L1471" target="_blank">__nf_conntrack_alloc()</a> (‚ù∂), inside <code>init_conntrack() (‚ù∑). __nf_conntrack_alloc()</code> creates a <a href="https://elixir.bootlin.com/linux/v5.10.14/source/include/net/netfilter/nf_conntrack.h#L58" target="_blank">struct nf_conn</a> object which represents a tracked connection.</p>
	<!--kg-card-begin: markdown-->
	<p>This object is not created in vain. <a href="https://elixir.bootlin.com/linux/v5.10.14/source/net/netfilter/nf_conntrack_core.c#L1633" target="_blank">A glance</a> at <code>init_conntrack()</code> <a href="https://elixir.bootlin.com/linux/v5.10.14/source/net/netfilter/nf_conntrack_core.c#L1633" target="_blank">source</a> shows that it is pushed onto a list of unconfirmed connections<sup>3</sup>.</p>
	<!--kg-card-end: markdown-->
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2021/03/image6-1.png" class="kg-image"></figure>
	<p>What does it mean that a connection is unconfirmed? As <a href="https://manpages.debian.org/buster/conntrack/conntrack.8.en.html" target="_blank">conntrack(8) man page</a> explains:</p>
	<!--kg-card-begin: markdown-->
	<pre><code>unconfirmed:
       This table shows new entries, that are not yet inserted into the
       conntrack table. These entries are attached to packets that  are
       traversing  the  stack, but did not reach the confirmation point
       at the postrouting hook.
</code></pre>
	<!--kg-card-end: markdown-->
	<p>Perhaps we have been looking for our flow in the wrong table? Does the unconfirmed table have a record for our dropped TCP SYN?</p>
	<h2 id="pulling-the-rabbit-out-of-the-hat">Pulling the rabbit out of the hat</h2>
	<p>I have bad news‚Ä¶</p>
	<!--kg-card-begin: markdown-->
	<pre><code class="language-ssh">[vagrant@ct-vm ~]$ sudo conntrack -L unconfirmed
conntrack v1.4.5 (conntrack-tools): 0 flow entries have been shown.
[vagrant@ct-vm ~]$
</code></pre>
	<!--kg-card-end: markdown-->
	<p>The flow is not present in the unconfirmed table. We have to dig deeper.</p>
	<p>Let‚Äôs for a moment assume that a <code>struct nf_conn</code> object was added to the <code>unconfirmed</code> list. If the list is now empty, then the object must have been removed from the list before we inspected its contents.</p>
	<p>Has an entry been removed from the <code>unconfirmed</code> table? What function removes entries from the <code>unconfirmed</code> table?</p>
	<p>It turns out that <code>nf_ct_add_to_unconfirmed_list()</code> which <code>init_conntrack()</code> invokes, has its opposite defined just right beneath it - <code>nf_ct_del_from_dying_or_unconfirmed_list()</code>.</p>
	<p>It is worth a shot to check if this function is being called, and if so, from where. For that we can again use a BPF tracing program, attached to function entry. However, this time our program will record a kernel stack trace:</p>
	<!--kg-card-begin: markdown-->
	<pre><code class="language-ssh">kprobe:nf_ct_del_from_dying_or_unconfirmed_list { @[kstack()] = count(); exit(); }
</code></pre>
	<!--kg-card-end: markdown-->
	<p>With <code>bpftrace</code> running our one-liner, we connect to the VM from the host with <code>nc</code> as before:</p>
	<!--kg-card-begin: markdown-->
	<pre><code class="language-ssh">[vagrant@ct-vm ~]$ sudo bpftrace -e 'kprobe:nf_ct_del_from_dying_or_unconfirmed_list { @[kstack()] = count(); exit(); }'
Attaching 1 probe...

@[
    nf_ct_del_from_dying_or_unconfirmed_list+1 ‚ùπ
    destroy_conntrack+78
    nf_conntrack_destroy+26
    skb_release_head_state+78
    kfree_skb+50 ‚ù∏
    nf_hook_slow+143 ‚ù∑
    ip_local_deliver+152 ‚ù∂
    ip_sublist_rcv_finish+87
    ip_sublist_rcv+387
    ip_list_rcv+293
    __netif_receive_skb_list_core+658
    netif_receive_skb_list_internal+444
    napi_complete_done+111
    ‚Ä¶
]: 1

[vagrant@ct-vm ~]$
</code></pre>
	<!--kg-card-end: markdown-->
	<p>Bingo. The conntrack delete function was called, and the captured stack trace shows that on local delivery path (‚ù∂), where <code>LOCAL_IN</code> Netfilter hook runs (‚ù∑), the packet is destroyed (‚ù∏). Conntrack must be getting called when <a href="https://elixir.bootlin.com/linux/v5.10.14/source/include/linux/skbuff.h#L713" target="_blank">sk_buff</a> (the packet and its metadata) is destroyed. This causes conntrack to remove the unconfirmed flow entry (‚ùπ).</p>
	<p>It makes sense. After all we have a <code>DROP</code> rule in the <code>filter/INPUT</code> chain. And that <code>iptables -j DROP</code> rule has a significant side effect. It cleans up an entry in the conntrack <code>unconfirmed</code> table!</p>
	<p>This explains why we can‚Äôt observe the flow in the <code>unconfirmed</code> table. It lives for only a very short period of time.</p>
	<p>Not convinced? You don‚Äôt have to take my word for it. I will prove it with a dirty trick!</p>
	<h2 id="making-the-rabbit-disappear-or-actually-appear">Making the rabbit disappear, or actually appear</h2>
	<p>If you recall the output from <code>list-nf-hooks</code> that we‚Äôve seen earlier, there is another conntrack callback there - <code>ipv4_confirm</code>, which I have ignored:</p>
	<!--kg-card-begin: markdown-->
	<pre><code class="language-ssh">[vagrant@ct-vm ~]$ sudo /vagrant/tools/list-nf-hooks
‚Ä¶
ü™ù ipv4 LOCAL_IN
       -150 ‚Üí iptable_mangle_hook
          0 ‚Üí iptable_filter_hook
         50 ‚Üí iptable_security_hook
        100 ‚Üí nf_nat_ipv4_fn
 2147483647 ‚Üí ipv4_confirm              ‚òú another conntrack callback
‚Ä¶ 
</code></pre>
	<!--kg-card-end: markdown-->
	<p><code>ipv4_confirm</code> is ‚Äúthe confirmation point‚Äù mentioned in the <a href="https://manpages.debian.org/buster/conntrack/conntrack.8.en.html" target="_blank">conntrack(8) man page</a>. When a flow gets confirmed, it is moved from the <code>unconfirmed</code> table to the main <code>conntrack</code> table.</p>
	<p>The callback is registered with a ‚Äúweird‚Äù priority ‚Äì 2,147,483,647. It‚Äôs the maximum positive value of a 32-bit signed integer can hold, and at the same time, the lowest possible priority a callback can have.</p>
	<p>This ensures that the <code>ipv4_confirm</code> callback runs last. We want the flows to graduate from the <code>unconfirmed</code> table to the main <code>conntrack</code> table only once we know the corresponding packet has made it through the firewall.</p>
	<p>Luckily for us, it is possible to have more than one callback registered with the same priority. In such cases, the order of registration matters. We can put that to use. Just for educational purposes.</p>
	<p>Good old <code>iptables</code> won‚Äôt be of much help here. Its Netfilter callbacks have hard-coded priorities which we can‚Äôt change. But <code>nftables</code>, the <code>iptables</code> <a href="https://developers.redhat.com/blog/2016/10/28/what-comes-after-iptables-its-successor-of-course-nftables/" target="_blank">successor</a>, is much more flexible in this regard. With <code>nftables</code> we can create a rule chain with arbitrary priority.</p>
	<p>So this time, let‚Äôs use nftables to install a filter rule to drop traffic to port tcp/2570. The trick, though, is to register our chain before conntrack registers itself. This way our filter will run <em>last</em>.</p>
	<p>First, delete the tcp/2570 drop rule in iptables and unregister conntrack.</p>
	<!--kg-card-begin: markdown-->
	<pre><code class="language-ssh">vm # iptables -t filter -F
vm # rmmod nf_conntrack_netlink nf_conntrack
</code></pre>
	<!--kg-card-end: markdown-->
	<p>Then add tcp/2570 drop rule in <code>nftables</code>, with lowest possible priority.</p>
	<!--kg-card-begin: markdown-->
	<pre><code class="language-ssh">vm # nft add table ip my_table
vm # nft add chain ip my_table my_input { type filter hook input priority 2147483647 \; }
vm # nft add rule ip my_table my_input tcp dport 2570 counter drop
vm # nft -a list ruleset
table ip my_table { # handle 1
        chain my_input { # handle 1
                type filter hook input priority 2147483647; policy accept;
                tcp dport 2570 counter packets 0 bytes 0 drop # handle 4
        }
}
</code></pre>
	<!--kg-card-end: markdown-->
	<p>Finally, re-register conntrack hooks.</p>
	<!--kg-card-begin: markdown-->
	<pre><code class="language-ssh">vm # modprobe nf_conntrack enable_hooks=1
</code></pre>
	<!--kg-card-end: markdown-->
	<p>The registered callbacks for the <code>LOCAL_IN</code> hook now look like this:</p>
	<!--kg-card-begin: markdown-->
	<pre><code class="language-ssh">vm # /vagrant/tools/list-nf-hooks
‚Ä¶
ü™ù ipv4 LOCAL_IN
       -150 ‚Üí iptable_mangle_hook
          0 ‚Üí iptable_filter_hook
         50 ‚Üí iptable_security_hook
        100 ‚Üí nf_nat_ipv4_fn
 2147483647 ‚Üí ipv4_confirm, nft_do_chain_ipv4
‚Ä¶
</code></pre>
	<!--kg-card-end: markdown-->
	<p>What happens if we connect to port tcp/2570 now?</p>
	<!--kg-card-begin: markdown-->
	<pre><code class="language-ssh">vm # conntrack -L
tcp      6 115 SYN_SENT src=192.168.122.1 dst=192.168.122.204 sport=54868 dport=2570 [UNREPLIED] src=192.168.122.204 dst=192.168.122.1 sport=2570 dport=54868 mark=0 secctx=system_u:object_r:unlabeled_t:s0 use=1
conntrack v1.4.5 (conntrack-tools): 1 flow entries have been shown.
</code></pre>
	<!--kg-card-end: markdown-->
	<p>We have fooled conntrack üí•</p>
	<p>Conntrack promoted the flow from the <code>unconfirmed</code> to the main <code>conntrack</code> table despite the fact that the firewall dropped the packet. We can observe it.</p>
	<h2 id="outro">Outro</h2>
	<!--kg-card-begin: markdown-->
	<p>Conntrack processes every received packet<sup>4</sup> and creates a flow for it. A flow entry is always created even if the packet is dropped shortly after. The flow might never be promoted to the main conntrack table and can be short lived.</p>
	<!--kg-card-end: markdown-->
	<p>However, this blog post is not really about conntrack. Its internals have been covered by <a href="https://www.usenix.org/publications/login/june-2006-volume-31-number-3/netfilters-connection-tracking-system" target="_blank">magazines</a>, <a href="https://www.semanticscholar.org/paper/Netfilter-Connection-Tracking-and-NAT-Boye/3f3c09dbc2a13c4840bb4a148753bb528493b607" target="_blank">papers</a>, <a href="https://books.google.pl/books?id=RpsQAwAAQBAJ&amp;lpg=PP1&amp;pg=PA253#v=onepage&amp;q&amp;f=false" target="_blank">books</a>, and on other blogs long before. We probably could have learned elsewhere all that has been shown here.</p>
	<p>For us, conntrack was really just an excuse to demonstrate various ways to discover the inner workings of the Linux network stack. As good as any other.</p>
	<p>Today we have powerful introspection tools like <a href="https://github.com/osandov/drgn" target="_blank">drgn</a>, <a href="https://bpftrace.org/" target="_blank">bpftrace</a>, or <a href="https://lwn.net/Articles/365835/" target="_blank">Ftrace</a>, and a <a href="https://elixir.bootlin.com/" target="_blank">cross referencer</a> to plow through the source code, at our fingertips. They help us look under the hood of a live operating system and gradually deepen our understanding of its workings.</p>
	<p>I have to warn you, though. Once you start digging into the kernel, it is hard to stop‚Ä¶</p>
	<!--kg-card-begin: markdown-->
	<p>...........<br>
		<sup>1</sup>Actually since <a href="https://kernelnewbies.org/Linux_5.10#Networking" target="_blank">Linux v5.10</a> (Dec 2020) there is an additional Netfilter hook for the INET family named <code>NF_INET_INGRESS</code>. The new hook type allows users to attach nftables chains to the Traffic Control ingress hook.<br>
		<sup>2</sup>Why did I pick this port number? Because 2570 = 0x0a0a. As we will see later, this saves us the trouble of converting between the network byte order and the host byte order.<br>
		<sup>3</sup>To be precise, there are multiple lists of unconfirmed connections. One per each CPU. This is a common pattern in the kernel. Whenever we want to prevent CPUs from contending for access to a shared state, we give each CPU a private instance of the state.<br>
		<sup>4</sup>Unless we explicitly exclude it from being tracked with <code>iptables -j NOTRACK</code>.
	</p>
	<!--kg-card-end: markdown-->
</div>