{
	"locale": "en-us",
	"localesAvailable": [
		"zh-cn",
		"zh-tw",
		"fr-fr",
		"de-de",
		"ja-jp",
		"ko-kr",
		"es-es"
	],
	"post": {
		"authors": [
			{
				"name": "Jesse Kipp",
				"slug": "jesse",
				"bio": null,
				"profile_image": "https://cf-assets.www.cloudflare.com/slt3lc6tev37/8yRqemLc8lLhE9Uwzd7G4/579fa890128819dc440469199f5cdd92/jesse.jpg",
				"location": null,
				"website": null,
				"twitter": null,
				"facebook": null
			},
			{
				"name": "Celso Martinho",
				"slug": "celso",
				"bio": "From when Mosaic took over Gopher. Engineering Director at Cloudflare.",
				"profile_image": "https://cf-assets.www.cloudflare.com/slt3lc6tev37/2pzgat1zmt1oF1byi7hskH/7e3b95ac25469b2640929caf376fa4c8/celso.png",
				"location": "Portugal, Lisbon",
				"website": "https://celso.io/",
				"twitter": "@celso",
				"facebook": null
			}
		],
		"excerpt": "Workers AI now supports streaming text responses for the LLM models in our catalog, including Llama-2, using server-sent events",
		"feature_image": "https://cf-assets.www.cloudflare.com/slt3lc6tev37/E30qOKdqjVdLW01aB7qXk/c817ec2b0007aae5e904564c11e34df7/workers-ai-streaming.png",
		"featured": false,
		"html": "<p></p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/6hqH5G1qi0RIrmIsdkb1Ql/b3ddf916e909fb740a067fe7ade898ff/pasted-image-0--3--2.png\" alt=\"Streaming LLMs and longer context lengths available in Workers AI\" class=\"kg-image\" width=\"1600\" height=\"901\" loading=\"lazy\"/>\n            \n            </figure><p>Workers AI is our serverless GPU-powered inference platform running on top of Cloudflare’s global network. It provides a growing catalog of off-the-shelf models that run seamlessly with Workers and enable developers to build powerful and scalable AI applications in minutes. We’ve already seen developers doing amazing things with Workers AI, and we can’t wait to see what they do as we continue to expand the platform. To that end, today we’re excited to announce some of our most-requested new features: streaming responses for all <a href=\"https://www.cloudflare.com/learning/ai/what-is-large-language-model/\">Large Language Models</a> (LLMs) on Workers AI, larger context and sequence windows, and a full-precision <a href=\"https://developers.cloudflare.com/workers-ai/models/llm/\">Llama-2</a> model variant.</p><p>If you’ve used ChatGPT before, then you’re familiar with the benefits of response streaming, where responses flow in token by token. LLMs work internally by generating responses sequentially using a process of repeated inference — the full output of a LLM model is essentially a sequence of hundreds or thousands of individual prediction tasks. For this reason, while it only takes a few milliseconds to generate a single token, generating the full response takes longer, on the order of seconds. The good news is we can start displaying the response as soon as the first tokens are generated, and append each additional token until the response is complete. This yields a much better experience for the end user —  displaying text incrementally as it&#39;s generated not only provides instant responsiveness, but also gives the end-user time to read and interpret the text.</p><p>As of today, you can now use response streaming for any LLM model in our catalog, including the very popular <a href=\"https://developers.cloudflare.com/workers-ai/models/llm/\">Llama-2 model</a>. Here’s how it works.</p><h3>Server-sent events: a little gem in the browser API</h3><p><a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events\">Server-sent events</a> are easy to use, simple to implement on the server side, standardized, and broadly available across many platforms natively or as a polyfill. Server-sent events fill a niche of handling a stream of updates from the server, removing the need for the boilerplate code that would otherwise be necessary to handle the event stream.</p><!--kg-card-begin: html--><style type=\"text/css\">\n.tg  {border-collapse:collapse;border-color:#ccc;border-spacing:0;}\n.tg td{background-color:#fff;border-color:#ccc;border-style:solid;border-width:1px;color:#333;\n  font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg th{background-color:#f0f0f0;border-color:#ccc;border-style:solid;border-width:1px;color:#333;\n  font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg .tg-lt9p{background-color:#F3F3F3;text-align:left;vertical-align:top}\n.tg .tg-9qck{background-color:#F3F3F3;font-weight:bold;text-align:left;vertical-align:top}\n.tg .tg-0lax{text-align:left;vertical-align:top}\n</style>\n<table class=\"tg\" width=\"100%\">\n<thead>\n  <tr>\n    <th class=\"tg-lt9p\"></th>\n    <th class=\"tg-9qck\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">Easy-to-use</span></th>\n    <th class=\"tg-9qck\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">Streaming</span></th>\n    <th class=\"tg-9qck\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">Bidirectional</span></th>\n  </tr>\n</thead>\n<tbody>\n  <tr>\n    <td class=\"tg-9qck\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">fetch</span></td>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">✅</span></td>\n    <td class=\"tg-0lax\"></td>\n    <td class=\"tg-0lax\"></td>\n  </tr>\n  <tr>\n    <td class=\"tg-9qck\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">Server-sent events</span></td>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">✅</span></td>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">✅</span></td>\n    <td class=\"tg-0lax\"></td>\n  </tr>\n  <tr>\n    <td class=\"tg-9qck\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">Websockets</span></td>\n    <td class=\"tg-0lax\"></td>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">✅</span></td>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">✅</span></td>\n  </tr>\n</tbody>\n</table><!--kg-card-end: html--><p><sup>Comparing fetch, server-sent events, and websockets</sup></p><p>To get started using streaming on Workers AI’s text generation models with server-sent events, set the “stream” parameter to true in the input of request. This will change the response format and <code>mime-type</code> to <code>text/event-stream</code>.</p><p>Here’s an example of using streaming with the <a href=\"https://developers.cloudflare.com/workers-ai/get-started/rest-api/\">REST API</a>:</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">curl -X POST \\\n\"https://api.cloudflare.com/client/v4/accounts/<account>/ai/run/@cf/meta/llama-2-7b-chat-int8\" \\\n-H \"Authorization: Bearer <token>\" \\\n-H \"Content-Type:application/json\" \\\n-d '{ \"prompt\": \"where is new york?\", \"stream\": true }'\n\ndata: {\"response\":\"New\"}\n\ndata: {\"response\":\" York\"}\n\ndata: {\"response\":\" is\"}\n\ndata: {\"response\":\" located\"}\n\ndata: {\"response\":\" in\"}\n\ndata: {\"response\":\" the\"}\n\n...\n\ndata: [DONE]</pre></code>\n            <p>And here’s an example using a Worker script:</p>\n            <pre class=\"language-typescript\"><code class=\"language-typescript\">import { Ai } from \"@cloudflare/ai\";\nexport default {\n    async fetch(request, env, ctx) {\n        const ai = new Ai(env.AI, { sessionOptions: { ctx: ctx } });\n        const stream = await ai.run(\n            \"@cf/meta/llama-2-7b-chat-int8\",\n            { prompt: \"where is new york?\", stream: true  }\n        );\n        return new Response(stream,\n            { headers: { \"content-type\": \"text/event-stream\" } }\n        );\n    }\n}</pre></code>\n            <p>If you want to consume the output event-stream from this Worker in a browser page, the client-side JavaScript is something like:</p>\n            <pre class=\"language-typescript\"><code class=\"language-typescript\">const source = new EventSource(\"/worker-endpoint\");\nsource.onmessage = (event) => {\n    if(event.data==\"[DONE]\") {\n        // SSE spec says the connection is restarted\n        // if we don't explicitly close it\n        source.close();\n        return;\n    }\n    const data = JSON.parse(event.data);\n    el.innerHTML += data.response;\n}</pre></code>\n            <p>You can use this simple code with any simple HTML page, complex SPAs using React or other Web frameworks.</p><p>This creates a much more interactive experience for the user, who now sees the page update as the response is incrementally created, instead of waiting with a spinner until the entire response sequence has been generated. Try it out streaming on <a href=\"https://ai.cloudflare.com\">ai.cloudflare.com</a>.</p>\n            <figure class=\"kg-card kg-image-card kg-width-wide\">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/6VIIO6crNIkpaz8hG9n8jg/a7105f439c60510ad8465b248ea699b8/llama-streaming.gif\" alt=\"\" class=\"kg-image\" width=\"1518\" height=\"610\" loading=\"lazy\"/>\n            \n            </figure><p>Workers AI supports streaming text responses for the <a href=\"https://developers.cloudflare.com/workers-ai/models/llm/\">Llama-2</a> model and any future LLM models we are adding to our catalog.</p><p>But this is not all.</p><h3>Higher precision, longer context and sequence lengths</h3><p>Another top request we heard from our community after the launch of Workers AI was for longer questions and answers in our Llama-2 model. In LLM terminology, this translates to higher context length (the number of tokens the model takes as input before making the prediction) and higher sequence length (the number of tokens the model generates in the response.)</p><p>We’re listening, and in conjunction with streaming, today we are adding a higher 16-bit full-precision Llama-2 variant to the catalog, and increasing the context and sequence lengths for the existing 8-bit version.</p><!--kg-card-begin: html--><style type=\"text/css\">\n.tg  {border-collapse:collapse;border-spacing:0;}\n.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg .tg-9qck{background-color:#F3F3F3;font-weight:bold;text-align:left;vertical-align:top}\n.tg .tg-0lax{text-align:left;vertical-align:top}\n</style>\n<table class=\"tg\" width=\"100%\">\n<thead>\n  <tr>\n    <th class=\"tg-9qck\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">Model</span></th>\n    <th class=\"tg-9qck\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">Context length (in)</span></th>\n    <th class=\"tg-9qck\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">Sequence length (out)</span></th>\n  </tr>\n</thead>\n<tbody>\n  <tr>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">@cf/meta/llama-2-7b-chat-int8</span></td>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">2048 (768 before)</span></td>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">1800 (256 before)</span></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">@cf/meta/llama-2-7b-chat-fp16</span></td>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">3072</span></td>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">2500</span></td>\n  </tr>\n</tbody>\n</table><!--kg-card-end: html--><p>Streaming, higher precision, and longer context and sequence lengths provide a better user experience and enable new, richer applications using large language models in Workers AI.</p><p>Check the Workers AI <a href=\"https://developers.cloudflare.com/workers-ai\">developer documentation</a> for more information and options. If you have any questions or feedback about Workers AI, please come see us in the <a href=\"https://community.cloudflare.com/\">Cloudflare Community</a> and the <a href=\"https://discord.gg/cloudflaredev\">Cloudflare Discord</a>.If you are interested in machine learning and serverless AI, the Cloudflare Workers AI team is building a global-scale platform and tools that enable our customers to run fast, low-latency inference tasks on top of our network. Check our <a href=\"https://www.cloudflare.com/careers/jobs/\">jobs page</a> for opportunities.</p>",
		"id": "4RWvzttPkO6JoYsMwoovJ8",
		"localeList": {
			"name": "Streaming and longer context lengths for LLMs on Workers AI Config",
			"enUS": "English for Locale",
			"zhCN": "Translated for Locale",
			"zhHansCN": "No Page for Locale",
			"zhTW": "Translated for Locale",
			"frFR": "Translated for Locale",
			"deDE": "Translated for Locale",
			"itIT": "No Page for Locale",
			"jaJP": "Translated for Locale",
			"koKR": "Translated for Locale",
			"ptBR": "No Page for Locale",
			"esLA": "No Page for Locale",
			"esES": "Translated for Locale",
			"enAU": "No Page for Locale",
			"enCA": "No Page for Locale",
			"enIN": "No Page for Locale",
			"enGB": "No Page for Locale",
			"idID": "No Page for Locale",
			"ruRU": "No Page for Locale",
			"svSE": "No Page for Locale",
			"viVN": "No Page for Locale",
			"plPL": "No Page for Locale",
			"arAR": "No Page for Locale",
			"nlNL": "No Page for Locale",
			"thTH": "No Page for Locale",
			"trTR": "No Page for Locale",
			"heIL": "No Page for Locale",
			"lvLV": "No Page for Locale",
			"etEE": "No Page for Locale",
			"ltLT": "No Page for Locale"
		},
		"meta_description": "Workers AI now supports streaming text responses for the LLM models in our catalog, including Llama-2, using server-sent events",
		"metadata": {
			"title": "Streaming and longer context lengths for LLMs on Workers AI",
			"description": "Workers AI now supports streaming text responses for the LLM models in our catalog, including Llama-2, using server-sent events",
			"imgPreview": "https://cf-assets.www.cloudflare.com/slt3lc6tev37/4IroqQ9dI46SGkhQvNb3XF/11ca44697e7c583d80cb88513d99a176/workers-ai-streaming-nnHfyN.png"
		},
		"primary_author": {},
		"published_at": "2023-11-14T14:00:33.000+00:00",
		"reading_time": 4,
		"slug": "workers-ai-streaming",
		"tags": [
			{
				"id": "1Wf1Dpb2AFicG44jpRT29y",
				"name": "Workers AI",
				"slug": "workers-ai"
			},
			{
				"id": "6hbkItfupogJP3aRDAq6v8",
				"name": "Cloudflare Workers",
				"slug": "workers"
			},
			{
				"id": "3JAY3z7p7An94s6ScuSQPf",
				"name": "Developer Platform",
				"slug": "developer-platform"
			},
			{
				"id": "78aSAeMjGNmCuetQ7B4OgU",
				"name": "JavaScript",
				"slug": "javascript"
			},
			{
				"id": "5cye1Bh5KxFh3pKSnX8Dsy",
				"name": "Serverless",
				"slug": "serverless"
			},
			{
				"id": "2FQK880QI5lKEUCjVHBber",
				"name": "1.1.1.1",
				"slug": "1-1-1-1"
			}
		],
		"title": "Streaming and longer context lengths for LLMs on Workers AI",
		"updated_at": "2024-08-27T01:02:59.650Z",
		"url": "https://blog.cloudflare.com/workers-ai-streaming"
	},
	"translations": {
		"posts.by": "By",
		"footer.gdpr": "GDPR",
		"lang_blurb1": "This post is also available in {lang1}.",
		"lang_blurb2": "This post is also available in {lang1} and {lang2}.",
		"lang_blurb3": "This post is also available in {lang1}, {lang2} and {lang3}.",
		"footer.blurb": "Cloudflare's connectivity cloud protects <a target='_blank' href='https://www.cloudflare.com/network-services/' rel='noreferrer'>entire corporate networks</a>, helps customers build <a target='_blank' href='https://workers.cloudflare.com/' rel='noreferrer'>Internet-scale applications efficiently</a>, accelerates any <a target='_blank' href='https://www.cloudflare.com/performance/accelerate-internet-applications/' rel='noreferrer'>website or Internet application</a>, <a target='_blank' href='https://www.cloudflare.com/ddos/' rel='noreferrer'>wards off DDoS attacks</a>, keeps <a target='_blank' href='https://www.cloudflare.com/application-security/' rel='noreferrer'>hackers at bay</a>, and can help you on <a target='_blank' href='https://www.cloudflare.com/products/zero-trust/' rel='noreferrer'>your journey to Zero Trust</a>.<br/><br/>Visit <a target='_blank' href='https://one.one.one.one/' rel='noreferrer'>1.1.1.1</a> from any device to get started with our free app that makes your Internet faster and safer.<br/><br/>To learn more about our mission to help build a better Internet, <a target='_blank' href='https://www.cloudflare.com/learning/what-is-cloudflare/' rel='noreferrer'>start here</a>. If you&apos;re looking for a new career direction, check out <a target='_blank' href='http://www.cloudflare.com/careers' rel='noreferrer'>our open positions</a>.",
		"footer.press": "Press",
		"header.title": "The Cloudflare Blog",
		"footer.careers": "Careers",
		"footer.company": "Company",
		"footer.support": "Support",
		"footer.the_net": "theNet",
		"footer.our_team": "Our team",
		"footer.webinars": "Webinars",
		"page.more_posts": "More posts",
		"posts.time_read": "{time} min read",
		"footer.community": "Community",
		"footer.resources": "Resources",
		"footer.solutions": "Solutions",
		"footer.trademark": "Trademark",
		"header.subscribe": "Subscribe",
		"footer.compliance": "Compliance",
		"footer.free_plans": "Free plans",
		"footer.impact_ESG": "Impact/ESG",
		"posts.follow_on_X": "Follow on X",
		"footer.help_center": "Help center",
		"footer.network_map": "Network Map",
		"header.please_wait": "Please Wait",
		"page.related_posts": "Related posts",
		"footer.case_studies": "Case Studies",
		"footer.connect_2024": "Connect 2024",
		"footer.terms_of_use": "Terms of Use",
		"footer.white_papers": "White Papers",
		"footer.cloudflare_tv": "Cloudflare TV",
		"footer.community_hub": "Community Hub",
		"footer.compare_plans": "Compare plans",
		"footer.contact_sales": "Contact Sales",
		"header.contact_sales": "Contact Sales",
		"header.email_address": "Email Address",
		"page.error.not_found": "Page not found",
		"footer.developer_docs": "Developer docs",
		"footer.privacy_policy": "Privacy Policy",
		"footer.request_a_demo": "Request a demo",
		"page.continue_reading": "Continue reading",
		"footer.analysts_report": "Analyst reports",
		"footer.for_enterprises": "For enterprises",
		"footer.getting_started": "Getting Started",
		"footer.learning_center": "Learning Center",
		"footer.project_galileo": "Project Galileo",
		"pagination.newer_posts": "Newer Posts",
		"pagination.older_posts": "Older Posts",
		"posts.social_buttons.x": "Discuss on X",
		"footer.about_cloudflare": "About Cloudflare",
		"footer.athenian_project": "Athenian Project",
		"footer.become_a_partner": "Become a partner",
		"footer.cloudflare_radar": "Cloudflare Radar",
		"footer.network_services": "Network services",
		"footer.trust_and_safety": "Trust & Safety",
		"header.get_started_free": "Get Started Free",
		"page.search.placeholder": "Search Cloudflare",
		"footer.cloudflare_status": "Cloudflare Status",
		"footer.cookie_preference": "Cookie Preferences",
		"header.valid_email_error": "Must be valid email.",
		"footer.connectivity_cloud": "Connectivity cloud",
		"footer.developer_services": "Developer services",
		"footer.investor_relations": "Investor relations",
		"page.not_found.error_code": "Error Code: 404",
		"footer.logos_and_press_kit": "Logos & press kit",
		"footer.application_services": "Application services",
		"footer.get_a_recommendation": "Get a recommendation",
		"posts.social_buttons.reddit": "Discuss on Reddit",
		"footer.sse_and_sase_services": "SSE and SASE services",
		"page.not_found.outdated_link": "You may have used an outdated link, or you may have typed the address incorrectly.",
		"footer.report_security_issues": "Report Security Issues",
		"page.error.error_message_page": "Sorry, we can't find the page you are looking for.",
		"header.subscribe_notifications": "Subscribe to receive notifications of new posts:",
		"footer.cloudflare_for_campaigns": "Cloudflare for Campaigns",
		"header.subscription_confimation": "Subscription confirmed. Thank you for subscribing!",
		"posts.social_buttons.hackernews": "Discuss on Hacker News",
		"footer.diversity_equity_inclusion": "Diversity, equity & inclusion",
		"footer.critical_infrastructure_defense_project": "Critical Infrastructure Defense Project"
	}
}