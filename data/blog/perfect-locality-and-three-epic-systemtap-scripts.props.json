{
	"locale": "en-us",
	"localesAvailable": [],
	"post": {
		"authors": [
			{
				"name": "Marek Majkowski",
				"slug": "marek-majkowski",
				"bio": null,
				"profile_image": "https://cf-assets.www.cloudflare.com/slt3lc6tev37/1JuU5qavgwVeqR8BAUrd6U/bd09672287e7cf04d4347d9a47607eb5/marek-majkowski.jpeg",
				"location": null,
				"website": null,
				"twitter": "@majek04",
				"facebook": null
			}
		],
		"excerpt": "In a recent blog post we discussed epoll behavior causing uneven load among NGINX worker processes. We suggested a work around - the REUSEPORT socket option.",
		"feature_image": "https://cf-assets.www.cloudflare.com/slt3lc6tev37/4yl7HZNXJGxmQzQxGckHZc/742f0586cf5dca5b8c64dffacec0cb05/perfect-locality-and-three-epic-systemtap-scripts.jpg",
		"featured": false,
		"html": "<p>In a recent blog post we discussed <a href=\"/the-sad-state-of-linux-socket-balancing/\">epoll behavior causing uneven load</a> among NGINX worker processes. We suggested a work around - the REUSEPORT socket option. It changes the queuing from &quot;combined queue model&quot; aka Waitrose (formally: <a href=\"http://people.revoledu.com/kardi/tutorial/Queuing/MMs-Queuing-System.html\">M/M/s</a>), to a dedicated accept queue per worker aka &quot;the Tesco superstore model&quot; (formally: <a href=\"http://people.revoledu.com/kardi/tutorial/Queuing/MMs-Queuing-System.html\">M/M/1</a>). With this setup the load is spread more evenly, but in certain conditions the latency distribution might suffer.</p><p>After reading that piece, a colleague of mine, John, said: <i>&quot;Hey Marek, don&#39;t forget that REUSEPORT has an additional advantage: it can improve packet locality! Packets can avoid being passed around CPUs!&quot;</i></p><p>John had a point. Let&#39;s dig into this step by step.</p><p>In this blog post we&#39;ll explain the REUSEPORT socket option, how it can help with packet locality and its performance implications. We&#39;ll show three advanced SystemTap scripts which we used to help us understand and measure the packet locality.</p><h3>A shared queue</h3><p>The standard BSD socket API model is rather simple. In order to receive new TCP connections a program calls bind() and then listen() on a fresh socket. This will create a single accept queue. Programs can share the file descriptor - pointing to one kernel data structure - among multiple processes to spread the load. As we&#39;ve <a href=\"/the-sad-state-of-linux-socket-balancing/\">seen in a previous blog post</a> connections might not be distributed perfectly. Still, this allows programs to scale up processing power from a limited single-process, single-CPU design.</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/71jaXye3QUzA1zqSVuZRmQ/9d31ec81e744545d84f94f8cab40d268/mms-1.jpeg.jpeg\" alt=\"\" class=\"kg-image\" width=\"640\" height=\"247\" loading=\"lazy\"/>\n            \n            </figure><p>Modern network cards split the inbound packets across multiple RX queues, allowing multiple CPUs to share interrupt and packet processing load. Unfortunately in the standard BSD API the new connections will all be funneled back to single accept queue, causing a potential bottleneck.</p><h3>Introducing REUSEPORT</h3><p>This bottleneck was identified at Google, where a reported application was dealing with 40,000 connections per second. Google kernel hackers fixed it by <a href=\"https://lwn.net/Articles/542629/\">adding a TCP support for SO_REUSEPORT socket option</a> in Linux kernel 3.9.</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/5B2G4N5rR8U4TrLkekzX9g/67f08c9d1cf24164efc93fad47f21656/mm1-1.jpeg.jpeg\" alt=\"\" class=\"kg-image\" width=\"640\" height=\"243\" loading=\"lazy\"/>\n            \n            </figure><p>REUSEPORT allows the application to set multiple accept queues on a single TCP listen port. This removes the central bottleneck and enables the CPUs to do more work in parallel.</p><h3>REUSEPORT locality</h3><p>Initially there was no way to influence the load balancing algorithm. While REUSEPORT allowed setting up a dedicated accept queue per each worker process, it wasn&#39;t possible to influence what packets would go into them. New connections flowing into the network stack would be distributed using only the usual 5-tuple hash. Packets from any of the RX queues, hitting any CPU, might flow into any of the accept queues.</p><p>This changed in Linux kernel 4.4 with the introduction of the <a href=\"https://patchwork.ozlabs.org/patch/528071/\">SO_INCOMING_CPU settable socket option</a>. Now a userspace program could add a hint to make the packets received on a specific CPU go to a specific accept queue. With this improvement the accept queue won&#39;t need to be shared across multiple cores, improving CPU cache locality and fixing lock contention issues.</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/3qVKKTNlhoSvrjKN2peAYH/fc129300f7a9532d951b1700c8414744/mm1-local-1.jpeg.jpeg\" alt=\"\" class=\"kg-image\" width=\"640\" height=\"238\" loading=\"lazy\"/>\n            \n            </figure><p>There are other benefits - with proper tuning it is possible to keep the processing of packets belonging to entire connections local. Think about it like that: if a SYN packet was received on some CPU it is likely that further packets for this connection will also be delivered to the same CPU<a href=\"#fn1\">[1]</a>. Therefore, making sure the worker on the same CPU called the accept() has strong advantages. With the right tuning all processing of the connection might be performed on a single CPU. This can help keep the CPU cache warm, reduce cross-CPU interrupts and boost the performance of memory allocation algorithms.</p><p>SO_INCOMING_CPU interface is pretty rudimentary and was deemed unsuitable for more complex usage. It was superseded by the more powerful <a href=\"https://lwn.net/Articles/675043/\">SO_ATTACH_REUSEPORT_CBPF option</a> (and it&#39;s extended variant: SO_ATTACH_REUSEPORT_EBPF) in kernel 4.6. These flags allow a program to specify a fully functional BPF program as a load balancing algorithm.</p><p>Beware that the introduction of SO_ATTACH_REUSEPORT_[CE]BPF broke SO_INCOMING_CPU. Nowadays there isn&#39;t a choice - you have to use the BPF variants to get the intended behavior.</p><h3>Setting CBPF on NGINX</h3><p>NGINX in &quot;reuseport&quot; mode doesn&#39;t set the advanced socket options increasing packet locality. John suggested that improving packet locality is beneficial for performance. We must verify such a bold claim!</p><p>We wanted to play with setting couple of SO_ATTACH_REUSEPORT_CBPF BPF scripts. We didn&#39;t want to hack the NGINX sources though. After some tinkering we decided it would be easier to write a SystemTap script to set the option from <i>outside</i> the server process. This turned out to be a big mistake!</p><p>After plenty of work, numerous kernel panics caused by our buggy scripts (running in &quot;guru&quot; mode), we finally managed to get it into working order. The SystemTap script that calls &quot;setsockopt&quot; with right parameters. It&#39;s one of the most complex scripts we&#39;ve written so far. Here it is:</p><ul><li><p><a href=\"https://github.com/cloudflare/cloudflare-blog/blob/master/2017-11-perfect-locality/setcbpf.stp\">setcbpf.stp</a></p></li></ul><p>We tested it on kernel 4.9. It sets the following CBPF (classical BPF) load balancing program on the REUSEPORT socket group. Sockets received on Nth CPU will be passed to Nth member of the REUSEPORT group:</p>\n            <pre class=\"language-.text\"><code class=\"language-.text\">A = #cpu\nA = A % &lt;reuseport group size&gt;\nreturn A</pre></code>\n            <p>The SystemTap script takes three parameters: pid, file descriptor and REUSEPORT group size. To figure out the pid of a process and a file descriptor number use the &quot;ss&quot; tool:</p>\n            <pre class=\"language-.text\"><code class=\"language-.text\">$ ss -4nlp -t &#039;sport = :8181&#039; | sort\nLISTEN  0   511    *:8181  *:*   users:((&quot;nginx&quot;,pid=29333,fd=3),...\nLISTEN  0   511    *:8181  *:*   ...\n...</pre></code>\n            <p>In this listing we see that pid=29333 fd=3 points to REUSEPORT descriptor bound to port tcp/8181. On our test machine we have 24 logical CPUs (including HT) and we run 12 NGINX workers - the group size is 12. Example invocation of the script:</p>\n            <pre class=\"language-.text\"><code class=\"language-.text\">$ sudo stap -g setcbpf.stp 29333 3 12</pre></code>\n            <h3>Measuring performance</h3><p>Unfortunately on Linux it&#39;s pretty hard to verify if setting CBPF actually does anything. To understand what&#39;s going on we wrote another SystemTap script. It hooks into a process and prints all successful invocations of the accept() function, including the CPU on which the connection was delivered to kernel, and current CPU - on which the application is running. The idea is simple - if they match, we&#39;ll have good locality!</p><p>The script:</p><ul><li><p><a href=\"https://github.com/cloudflare/cloudflare-blog/blob/master/2017-11-perfect-locality/accept.stp\">accept.stp</a></p></li></ul><p>Before setting the CBPF socket option on the server, we saw this output:</p>\n            <pre class=\"language-.text\"><code class=\"language-.text\">$ sudo stap -g accept.stp nginx|grep &quot;cpu=#12&quot;\ncpu=#12 pid=29333 accept(3) -&gt; fd=30 rxcpu=#19\ncpu=#12 pid=29333 accept(3) -&gt; fd=31 rxcpu=#21\ncpu=#12 pid=29333 accept(3) -&gt; fd=32 rxcpu=#16\ncpu=#12 pid=29333 accept(3) -&gt; fd=33 rxcpu=#22\ncpu=#12 pid=29333 accept(3) -&gt; fd=34 rxcpu=#19\ncpu=#12 pid=29333 accept(3) -&gt; fd=35 rxcpu=#21\ncpu=#12 pid=29333 accept(3) -&gt; fd=37 rxcpu=#16</pre></code>\n            <p>We can see accept()s done from a worker on CPU #12 returning client sockets received on some other CPUs like: #19, #21, #16 and so on.</p><p>Now, let&#39;s run CBPF and see the results:</p>\n            <pre class=\"language-.text\"><code class=\"language-.text\">$ sudo stap -g setcbpf.stp `pidof nginx -s` 3 12\n[+] Pid=29333 fd=3 group_size=12 setsockopt(SO_ATTACH_REUSEPORT_CBPF)=0\n\n$ sudo stap -g accept.stp nginx|grep &quot;cpu=#12&quot;\ncpu=#12 pid=29333 accept(3) -&gt; fd=30 rxcpu=#12\ncpu=#12 pid=29333 accept(3) -&gt; fd=31 rxcpu=#12\ncpu=#12 pid=29333 accept(3) -&gt; fd=32 rxcpu=#12\ncpu=#12 pid=29333 accept(3) -&gt; fd=33 rxcpu=#12\ncpu=#12 pid=29333 accept(3) -&gt; fd=34 rxcpu=#12\ncpu=#12 pid=29333 accept(3) -&gt; fd=35 rxcpu=#12\ncpu=#12 pid=29333 accept(3) -&gt; fd=36 rxcpu=#12</pre></code>\n            <p>Now the situation is perfect. All accept()s called from the NGINX worker pinned to CPU #12 got client sockets received on the same CPU.</p><p>But does it actually help with the performance?</p><p>Sadly: no. We&#39;ve run a number of tests (using the setup introduced in previous blog post) but we weren&#39;t able to record any significant performance difference. Compared to other costs incurred by running a high level HTTP server, a couple of microseconds shaved by keeping connections local to a CPU doesn&#39;t seem to make a measurable difference.</p><h3>Measuring packet locality</h3><p>But no, we didn&#39;t give up!</p><p>Not being able to measure an end to end performance gain, we decided to try another approach. Why not try to measure packet locality itself!</p><p>Measuring locality is tricky. In certain circumstances a packet can cross multiple CPUs on its way down the networking stack. Fortunately we can simplify the problem. Let&#39;s define &quot;packet locality&quot; as the probability of a packet (to be specific: the Linux sock_buff data structure, skb) being allocated and freed on the same CPU.</p><p>For this, we wrote yet another SystemTap script:</p><ul><li><p><a href=\"https://github.com/cloudflare/cloudflare-blog/blob/master/2017-11-perfect-locality/locality.stp\">locality.stp</a></p></li></ul><p>When run without the CBPF option the script gave us this results:</p>\n            <pre class=\"language-.text\"><code class=\"language-.text\">$ sudo stap -g locality.stp 12\nrx= 21%   29kpps tx=  9%  24kpps\nrx=  8%  130kpps tx=  8% 131kpps\nrx= 11%  132kpps tx=  9% 126kpps\nrx= 10%  128kpps tx=  8% 127kpps\nrx= 10%  129kpps tx=  8% 126kpps\nrx= 11%  132kpps tx=  9% 127kpps\nrx= 11%  129kpps tx= 10% 128kpps\nrx= 10%  130kpps tx=  9% 127kpps\nrx= 12%   94kpps tx=  8%  90kpps</pre></code>\n            <p>During our test the HTTP server received about 130,000 packets per second and transmitted about as much. 10-11% of the received and 8-10% of the transmitted packets had good locality - were allocated and freed on the same CPU.</p><p>Achieving good locality is not that easy. On the RX side, this means the packet must be received on the same CPU as the application that will read() it. On the transmission side it&#39;s even trickier. In case of TCP, a piece of data must all: be sent() by application, get transmitted, and receive back an ACK from the other party, all on the same CPU.</p><p>We performed a bit of tuning, which included inspecting:</p><ul><li><p>number of RSS queues and their interrupts being pinned to right CPUs</p></li><li><p>the indirection table</p></li><li><p>correct XPS settings on the TX path</p></li><li><p>NGINX workers being pinned to right CPUs</p></li><li><p>NGINX using the REUSEPORT bind option</p></li><li><p>and finally setting CBPF on the REUSEPORT sockets</p></li></ul><p>We were able to achieve almost perfect locality! With all tweaks done the script output looked better:</p>\n            <pre class=\"language-.text\"><code class=\"language-.text\">$ sudo stap -g locality.stp 12\nrx= 99%   18kpps tx=100%  12kpps\nrx= 99%  118kpps tx= 99% 115kpps\nrx= 99%  132kpps tx= 99% 129kpps\nrx= 99%  138kpps tx= 99% 136kpps\nrx= 99%  140kpps tx=100% 134kpps\nrx= 99%  138kpps tx= 99% 135kpps\nrx= 99%  139kpps tx=100% 137kpps\nrx= 99%  139kpps tx=100% 135kpps\nrx= 99%   77kpps tx= 99%  74kpps</pre></code>\n            <p>Now the test runs at 138,000 packets per second received and transmitted. The packets have a whopping 99% packet locality.</p><p>As for performance difference in practice - it&#39;s too small to measure. Even though we received about 7% more packets, the end-to-end tests didn&#39;t show a meaningful speed boost.</p><h3>Conclusion</h3><p>We weren&#39;t able to prove definitely if improving packet locality actually improves performance for a high-level TCP application like an HTTP server. In hindsight it makes sense - the added benefit is minuscule compared to the overhead of running an HTTP server, especially with logic in a high level language like Lua.</p><p>This hasn&#39;t stopped us from having fun! We (myself, Gilberto Bertin and David Wragg) wrote three pretty cool SystemTap scripts, which are super useful when debugging Linux packet locality. They may come handy for demanding users, for example running high performance UDP servers or doing high frequency trading.</p><p>Most importantly - in the process we learned a lot about the Linux networking stack. We got to practice writing CBPF scripts, and learned how to measure locality with hackish SystemTap scripts. We got reminded of the obvious - out of the box Linux is remarkably well tuned.</p><hr/><p><i>Dealing with the internals of Linux and NGINX sound interesting? Join our </i><a href=\"https://boards.greenhouse.io/cloudflare/jobs/589572\"><i>world famous team</i></a><i> in London, Austin, San Francisco and our elite office in Warsaw, Poland</i>.</p><hr/><hr/><ol><li><p>We are not taking into account aRFS - accelerated RFS. <a href=\"#fnref1\">↩︎</a></p></li></ol>",
		"id": "1FApU5GLrcapOqChAGtO0E",
		"localeList": {
			"name": "Perfect locality and three epic SystemTap scripts Config",
			"enUS": "English for Locale",
			"zhCN": "No Page for Locale",
			"zhHansCN": "No Page for Locale",
			"zhTW": "No Page for Locale",
			"frFR": "No Page for Locale",
			"deDE": "No Page for Locale",
			"itIT": "No Page for Locale",
			"jaJP": "No Page for Locale",
			"koKR": "No Page for Locale",
			"ptBR": "No Page for Locale",
			"esLA": "No Page for Locale",
			"esES": "No Page for Locale",
			"enAU": "No Page for Locale",
			"enCA": "No Page for Locale",
			"enIN": "No Page for Locale",
			"enGB": "No Page for Locale",
			"idID": "No Page for Locale",
			"ruRU": "No Page for Locale",
			"svSE": "No Page for Locale",
			"viVN": "No Page for Locale",
			"plPL": "No Page for Locale",
			"arAR": "No Page for Locale",
			"nlNL": "No Page for Locale",
			"thTH": "No Page for Locale",
			"trTR": "No Page for Locale",
			"heIL": "No Page for Locale",
			"lvLV": "No Page for Locale",
			"etEE": "No Page for Locale",
			"ltLT": "No Page for Locale"
		},
		"meta_description": null,
		"metadata": {
			"imgPreview": ""
		},
		"primary_author": {},
		"published_at": "2017-11-07T10:15:00.000+00:00",
		"reading_time": 7,
		"slug": "perfect-locality-and-three-epic-systemtap-scripts",
		"tags": [
			{
				"id": "5NpgoTJYJjhgjSLaY7Gt3p",
				"name": "TCP",
				"slug": "tcp"
			},
			{
				"id": "6lhzEBz2B56RKa4nUEAGYJ",
				"name": "Programming",
				"slug": "programming"
			},
			{
				"id": "3FBpuRfF7HUFga2Z5jgAFf",
				"name": "NGINX",
				"slug": "nginx"
			}
		],
		"title": "Perfect locality and three epic SystemTap scripts",
		"updated_at": "2024-08-27T02:26:57.489Z",
		"url": "https://blog.cloudflare.com/perfect-locality-and-three-epic-systemtap-scripts"
	},
	"translations": {
		"posts.by": "By",
		"footer.gdpr": "GDPR",
		"lang_blurb1": "This post is also available in {lang1}.",
		"lang_blurb2": "This post is also available in {lang1} and {lang2}.",
		"lang_blurb3": "This post is also available in {lang1}, {lang2} and {lang3}.",
		"footer.blurb": "Cloudflare's connectivity cloud protects <a target='_blank' href='https://www.cloudflare.com/network-services/' rel='noreferrer'>entire corporate networks</a>, helps customers build <a target='_blank' href='https://workers.cloudflare.com/' rel='noreferrer'>Internet-scale applications efficiently</a>, accelerates any <a target='_blank' href='https://www.cloudflare.com/performance/accelerate-internet-applications/' rel='noreferrer'>website or Internet application</a>, <a target='_blank' href='https://www.cloudflare.com/ddos/' rel='noreferrer'>wards off DDoS attacks</a>, keeps <a target='_blank' href='https://www.cloudflare.com/application-security/' rel='noreferrer'>hackers at bay</a>, and can help you on <a target='_blank' href='https://www.cloudflare.com/products/zero-trust/' rel='noreferrer'>your journey to Zero Trust</a>.<br/><br/>Visit <a target='_blank' href='https://one.one.one.one/' rel='noreferrer'>1.1.1.1</a> from any device to get started with our free app that makes your Internet faster and safer.<br/><br/>To learn more about our mission to help build a better Internet, <a target='_blank' href='https://www.cloudflare.com/learning/what-is-cloudflare/' rel='noreferrer'>start here</a>. If you&apos;re looking for a new career direction, check out <a target='_blank' href='http://www.cloudflare.com/careers' rel='noreferrer'>our open positions</a>.",
		"footer.press": "Press",
		"header.title": "The Cloudflare Blog",
		"footer.careers": "Careers",
		"footer.company": "Company",
		"footer.support": "Support",
		"footer.the_net": "theNet",
		"footer.our_team": "Our team",
		"footer.webinars": "Webinars",
		"page.more_posts": "More posts",
		"posts.time_read": "{time} min read",
		"footer.community": "Community",
		"footer.resources": "Resources",
		"footer.solutions": "Solutions",
		"footer.trademark": "Trademark",
		"header.subscribe": "Subscribe",
		"footer.compliance": "Compliance",
		"footer.free_plans": "Free plans",
		"footer.impact_ESG": "Impact/ESG",
		"posts.follow_on_X": "Follow on X",
		"footer.help_center": "Help center",
		"footer.network_map": "Network Map",
		"header.please_wait": "Please Wait",
		"page.related_posts": "Related posts",
		"footer.case_studies": "Case Studies",
		"footer.connect_2024": "Connect 2024",
		"footer.terms_of_use": "Terms of Use",
		"footer.white_papers": "White Papers",
		"footer.cloudflare_tv": "Cloudflare TV",
		"footer.community_hub": "Community Hub",
		"footer.compare_plans": "Compare plans",
		"footer.contact_sales": "Contact Sales",
		"header.contact_sales": "Contact Sales",
		"header.email_address": "Email Address",
		"page.error.not_found": "Page not found",
		"footer.developer_docs": "Developer docs",
		"footer.privacy_policy": "Privacy Policy",
		"footer.request_a_demo": "Request a demo",
		"page.continue_reading": "Continue reading",
		"footer.analysts_report": "Analyst reports",
		"footer.for_enterprises": "For enterprises",
		"footer.getting_started": "Getting Started",
		"footer.learning_center": "Learning Center",
		"footer.project_galileo": "Project Galileo",
		"pagination.newer_posts": "Newer Posts",
		"pagination.older_posts": "Older Posts",
		"posts.social_buttons.x": "Discuss on X",
		"footer.about_cloudflare": "About Cloudflare",
		"footer.athenian_project": "Athenian Project",
		"footer.become_a_partner": "Become a partner",
		"footer.cloudflare_radar": "Cloudflare Radar",
		"footer.network_services": "Network services",
		"footer.trust_and_safety": "Trust & Safety",
		"header.get_started_free": "Get Started Free",
		"page.search.placeholder": "Search Cloudflare",
		"footer.cloudflare_status": "Cloudflare Status",
		"footer.cookie_preference": "Cookie Preferences",
		"header.valid_email_error": "Must be valid email.",
		"footer.connectivity_cloud": "Connectivity cloud",
		"footer.developer_services": "Developer services",
		"footer.investor_relations": "Investor relations",
		"page.not_found.error_code": "Error Code: 404",
		"footer.logos_and_press_kit": "Logos & press kit",
		"footer.application_services": "Application services",
		"footer.get_a_recommendation": "Get a recommendation",
		"posts.social_buttons.reddit": "Discuss on Reddit",
		"footer.sse_and_sase_services": "SSE and SASE services",
		"page.not_found.outdated_link": "You may have used an outdated link, or you may have typed the address incorrectly.",
		"footer.report_security_issues": "Report Security Issues",
		"page.error.error_message_page": "Sorry, we can't find the page you are looking for.",
		"header.subscribe_notifications": "Subscribe to receive notifications of new posts:",
		"footer.cloudflare_for_campaigns": "Cloudflare for Campaigns",
		"header.subscription_confimation": "Subscription confirmed. Thank you for subscribing!",
		"posts.social_buttons.hackernews": "Discuss on Hacker News",
		"footer.diversity_equity_inclusion": "Diversity, equity & inclusion",
		"footer.critical_infrastructure_defense_project": "Critical Infrastructure Defense Project"
	}
}