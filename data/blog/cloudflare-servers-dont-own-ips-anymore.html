<div class="mb2 gray5">12 min read</div>
<div class="post-content lh-copy gray1">
	<!--kg-card-begin: markdown-->
	<p><em><small>This post is also available in<a href="https://blog.cloudflare.com/zh-cn/cloudflare-servers-dont-own-ips-anymore-zh-cn">简体中文</a>, <a href="https://blog.cloudflare.com/zh-tw/cloudflare-servers-dont-own-ips-anymore-zh-tw">繁體中文</a>, <a href="https://blog.cloudflare.com/ja-jp/cloudflare-servers-dont-own-ips-anymore-ja-jp">日本語</a>, <a href="https://blog.cloudflare.com/de-de/cloudflare-servers-dont-own-ips-anymore-de-de">Deutsch</a>, <a href="https://blog.cloudflare.com/fr-fr/cloudflare-servers-dont-own-ips-anymore-fr-fr">Français</a>, <a href="https://blog.cloudflare.com/es-es/cloudflare-servers-dont-own-ips-anymore-es-es">Español</a>, <a href="https://blog.cloudflare.com/pt-br/cloudflare-servers-dont-own-ips-anymore-pt-br">Português</a> and <a href="https://blog.cloudflare.com/ru-ru/cloudflare-servers-dont-own-ips-anymore-ru-ru">Pусский</a>.</small></em></p>
	<!--kg-card-end: markdown-->
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2022/11/image11-3.png" class="kg-image" alt="Cloudflare servers don't own IPs anymore – so how do they connect to the Internet?" loading="lazy"></figure>
	<p>A lot of Cloudflare's technology is well documented. For example, how we handle traffic between the eyeballs (clients) and our servers has been discussed many times on this blog: “<a href="https://blog.cloudflare.com/a-brief-anycast-primer">A brief primer on anycast (2011)</a>”, "<a href="https://blog.cloudflare.com/cloudflares-architecture-eliminating-single-p">Load Balancing without Load Balancers (2013)</a>", "<a href="https://blog.cloudflare.com/path-mtu-discovery-in-practice">Path MTU discovery in practice (2015)</a>", &nbsp;"<a href="https://blog.cloudflare.com/unimog-cloudflares-edge-load-balancer">Cloudflare's edge load balancer (2020)</a>", "<a href="https://blog.cloudflare.com/tubular-fixing-the-socket-api-with-ebpf">How we fixed the BSD socket API (2022)</a>".</p>
	<p>However, we have rarely talked about the second part of our networking setup — how our servers fetch the content from the Internet. In this blog we’re going to cover this gap. We'll discuss how we manage Cloudflare IP addresses used to retrieve the data from the Internet, how our egress network design has evolved and how we optimized it for best use of available IP space.</p>
	<p>Brace yourself. We have a lot to cover.</p>
	<h3 id="terminology-first-">Terminology first!</h3>
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2022/11/image8-4.png" class="kg-image" alt="" loading="lazy"></figure>
	<p>Each Cloudflare server deals with many kinds of networking traffic, but two rough categories stand out:</p>
	<ul>
		<li><em>Internet sourced traffic</em> - Inbound connections initiated by eyeball to our servers. In the context of this blog post we'll call these "<strong>ingress</strong> connections".</li>
		<li><em>Cloudflare sourced traffic</em> - Outgoing connections initiated by our servers to other hosts on the Internet. For brevity, we'll call these "<strong>egress</strong> connections".</li>
	</ul>
	<p>The egress part, while rarely discussed on this blog, is critical for our operation. Our servers must initiate outgoing connections to get their jobs done! Like:</p>
	<ul>
		<li>In our CDN product, before the content is cached, it's fetched from the origin servers. See "<a href="https://blog.cloudflare.com/how-we-built-pingora-the-proxy-that-connects-cloudflare-to-the-internet">Pingora, the proxy that connects Cloudflare to the Internet (2022)</a>", <a href="https://blog.cloudflare.com/argo-v2">Argo</a> and <a href="https://blog.cloudflare.com/tiered-cache-smart-topology">Tiered Cache</a>.</li>
		<li>For the <a href="https://www.cloudflare.com/products/cloudflare-spectrum" target="_blank">Spectrum</a> product, each ingress TCP connection results in one egress connection.</li>
		<li><a href="https://workers.cloudflare.com" target="_blank">Workers</a> often run multiple subrequests to construct an HTTP response. Some of them might be querying servers to the Internet.</li>
		<li>We also operate client-facing forward proxy products - like WARP and Cloudflare Gateway. These proxies deal with eyeball connections destined to the Internet. Our servers need to establish connections to the Internet on behalf of our users.</li>
	</ul>
	<p>And so on.</p>
	<h3 id="anycast-on-ingress-unicast-on-egress">Anycast on ingress, unicast on egress</h3>
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2022/11/image9-3.png" class="kg-image" alt="" loading="lazy"></figure>
	<p>Our ingress network architecture is very different from the egress one. On ingress, the connections sourced from the Internet are handled exclusively by our anycast IP ranges. Anycast is a technology where each of our data centers "announces" and can handle the same IP ranges. With many destinations possible, how does the Internet know where to route the packets? Well, the eyeball packets are routed towards the closest data center based on Internet BGP metrics, often it's also geographically the closest one. Usually, the BGP routes don't change much, and each eyeball IP can be expected to be routed to a single data center.</p>
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2022/11/image10-2.png" class="kg-image" alt="" loading="lazy"></figure>
	<p>However, while anycast works well in the ingress direction, it can't operate on egress. Establishing an outgoing connection from an anycast IP won't work. Consider the response packet. It's likely to be routed back to a wrong place - a data center geographically closest to the sender, not necessarily the source data center!</p>
	<p>For this reason, until recently, we established outgoing connections in a straightforward and conventional way: each server was given its own unicast IP address. "Unicast IP" means there is only one server using that address in the world. Return packets will work just fine and get back exactly to the right server identified by the unicast IP.</p>
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2022/11/image5-16.png" class="kg-image" alt="" loading="lazy"></figure>
	<h3 id="segmenting-traffic-based-on-egress-ip">Segmenting traffic based on egress IP</h3>
	<p>Originally connections sourced by Cloudflare were mostly HTTP fetches going to origin servers on the Internet. As our product line grew, so did the variety of traffic. The most notable example is <a href="https://blog.cloudflare.com/1111-warp-better-vpn">our WARP app</a>. For WARP, our servers operate a forward proxy, and handle the traffic sourced by end-user devices. It's done without the same degree of intermediation as in our CDN product. This creates a problem. Third party servers on the Internet — like the origin servers — must be able to distinguish between connections coming from Cloudflare services and our WARP users. Such traffic segmentation is traditionally done by using different IP ranges for different traffic types (although recently we introduced more robust techniques like <a href="https://developers.cloudflare.com/ssl/origin-configuration/authenticated-origin-pull" target="_blank">Authenticated Origin Pulls</a>).</p>
	<p>To work around the trusted vs untrusted traffic pool differentiation problem, we added an untrusted WARP IP address to each of our servers:</p>
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2022/11/image4-30.png" class="kg-image" alt="" loading="lazy"></figure>
	<h3 id="country-tagged-egress-ip-addresses">Country tagged egress IP addresses</h3>
	<p>It quickly became apparent that trusted vs untrusted weren't the only tags needed. For WARP service we also need country tags. For example, United Kingdom based WARP users expect the bbc.com website to just work. However, the BBC restricts many of its services to people just in the UK.</p>
	<p>It does this by <em>geofencing</em> — using a database mapping public IP addresses to countries, and allowing only the UK ones. Geofencing is widespread on today's Internet. To avoid geofencing issues, we need to choose specific egress addresses tagged with an appropriate country, depending on WARP user location. Like many other parties on the Internet, we tag our egress IP space with country codes and publish it as a geofeed (like <a href="https://mask-api.icloud.com/egress-ip-ranges.csv" target="_blank">this one</a>). Notice, the published geofeed is just data. The fact that an IP is tagged as say UK does not mean it is served from the UK, it just means the operator wants it to be geolocated to the UK. Like many things on the Internet, it is based on trust.</p>
	<p>Notice, at this point we have three independent geographical tags:</p>
	<ul>
		<li>the country tag of the WARP user - the eyeball connecting IP</li>
		<li>the location of the data center the eyeball connected to</li>
		<li>the country tag of the egressing IP</li>
	</ul>
	<p>For best service, we want to choose the egressing IP so that its country tag matches the country from the eyeball IP. But egressing from a specific country tagged IP is challenging: our data centers serve users from all over the world, potentially from many countries! Remember: due to anycast we don't directly control the ingress routing. Internet geography doesn’t always match physical geography. For example our London data center receives traffic not only from users in the United Kingdom, but also from Ireland, and Saudi Arabia. As a result, our servers in London need many WARP egress addresses associated with many countries:</p>
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2022/11/image2-52.png" class="kg-image" alt="" loading="lazy"></figure>
	<p>Can you see where this is going? The problem space just explodes! Instead of having one or two egress IP addresses for each server, now we require dozens, and IPv4 addresses aren't cheap. With this design, we need many addresses per server, and we operate thousands of servers. This architecture becomes very expensive.</p>
	<h3 id="is-anycast-a-problem">Is anycast a problem?</h3>
	<p>Let me recap: with anycast ingress we don't control which data center the user is routed to. Therefore, each of our data centers must be able to egress from an address with any conceivable tag. Inside the data center we also don't control which server the connection is routed to. There are potentially many tags, many data centers, and many servers inside a data center.</p>
	<p>Maybe the problem is the ingress architecture? Perhaps it's better to use a traditional networking design where a specific eyeball is routed with DNS to a specific data center, or even a server?</p>
	<p>That's one way of thinking, but we decided against it. We like our anycast on ingress. It brings us many advantages:</p>
	<ul>
		<li><strong>Performance</strong>: with anycast, by definition, the eyeball is routed to the closest (by BGP metrics) data center. This is usually the fastest data center for a given user.</li>
		<li><strong>Automatic failover</strong>: if one of our data centers becomes unavailable, the traffic will be instantly, automatically re-routed to the next best place.</li>
		<li><strong>DDoS resilience</strong>: during a denial of service attack or a traffic spike, the load is automatically balanced across many data centers, significantly reducing the impact.</li>
		<li><strong>Uniform software</strong>: The functionality of every data center and of every server inside a data center is identical. We use the same software stack on all the servers around the world. Each machine can perform any action, for any product. This enables easy debugging and good scalability.</li>
	</ul>
	<p>For these reasons we'd like to keep the anycast on ingress. We decided to solve the issue of egress address cardinality in some other way.</p>
	<h3 id="solving-a-million-dollar-problem">Solving a million dollar problem</h3>
	<p>Out of the thousands of servers we operate, every single one should be able to use an egress IP with any of the possible tags. It's easiest to explain our solution by first showing two extreme designs.</p>
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2022/11/image6-10.png" class="kg-image" alt="" loading="lazy"></figure>
	<p><strong>Each server owns all the needed IPs:</strong> each server has all the specialized egress IPs with the needed tags.</p>
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2022/11/image12-1.png" class="kg-image" alt="" loading="lazy"></figure>
	<p><strong>One server owns the needed IP:</strong> a specialized egress IP with a specific tag lives in one place, other servers forward traffic to it.</p>
	<p>Both options have pros and cons:</p><!--kg-card-begin: html-->
	<table>
		<thead>
			<tr>
				<th>Specialized IP on every server</th>
				<th>Specialized IP on one server</th>
			</tr>
		</thead>
		<tbody>
			<tr>
				<td>Super expensive $$$, every server needs many IP addresses.</td>
				<td>Cheap $, only one specialized IP needed for a tag.</td>
			</tr>
			<tr>
				<td>Egress always local - fast</td>
				<td>Egress almost always forwarded - slow</td>
			</tr>
			<tr>
				<td>Excellent reliability - every server is independent</td>
				<td>Poor reliability - introduced chokepoints</td>
			</tr>
		</tbody>
	</table><!--kg-card-end: html-->
	<h3 id="there-s-a-third-way">There's a third way</h3>
	<p>We've been thinking hard about this problem. Frankly, the first extreme option of having every needed IP available locally on every Cloudflare server is not totally unworkable. This is, roughly, what we were able to pull off for IPv6. With IPv6, access to the large needed IP space is not a problem.</p>
	<p>However, in IPv4 neither option is acceptable. The first offers fast and reliable egress, but requires great cost — the IPv4 addresses needed are expensive. The second option uses the smallest possible IP space, so it's cheap, but compromises on performance and reliability.</p>
	<p>The solution we devised is a compromise between the extremes. The rough idea is to change the assignment unit. Instead of assigning one /32 IPv4 address for each server, we devised a method of assigning a /32 IP per data center, and then sharing it among physical servers.</p><!--kg-card-begin: html-->
	<table>
		<thead>
			<tr>
				<th>Specialized IP on every server</th>
				<th>Specialized IP per data center</th>
				<th>Specialized IP on one server</th>
			</tr>
		</thead>
		<tbody>
			<tr>
				<td>Super expensive $$$</td>
				<td>Reasonably priced $$</td>
				<td>Cheap $</td>
			</tr>
			<tr>
				<td>Egress always local - fast</td>
				<td>Egress always local - fast</td>
				<td>Egress almost always forwarded - slow</td>
			</tr>
			<tr>
				<td>Excellent reliability - every server is independent</td>
				<td>Excellent reliability - every server is independent</td>
				<td>Poor reliability - many choke points</td>
			</tr>
		</tbody>
	</table><!--kg-card-end: html-->
	<h3 id="sharing-an-ip-inside-data-center">Sharing an IP inside data center</h3>
	<p>The idea of sharing an IP among servers is not new. Traditionally this can be achieved by Source-NAT on a router. Sadly, the sheer number of egress IP's we need and the size of our operation, prevents us from relying on stateful firewall / NAT at the router level. We also dislike shared state, so we're not fans of distributed NAT installations.</p>
	<p>What we chose instead, is splitting an egress IP across servers by <strong>a port range</strong>. For a given egress IP, each server owns a small portion of available source ports - a port slice.</p>
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2022/11/image1-68.png" class="kg-image" alt="" loading="lazy"></figure>
	<p>When return packets arrive from the Internet, we have to route them back to the correct machine. For this task we've customized "Unimog" - our L4 XDP-based load balancer - ("<a href="https://blog.cloudflare.com/unimog-cloudflares-edge-load-balancer">Unimog, Cloudflare's load balancer (2020)</a>") and it's working flawlessly.</p>
	<p>With a port slice of say 2,048 ports, we can share one IP among 31 servers. However, there is always a possibility of running out of ports. To address this, we've worked hard to be able to reuse the egress ports efficiently. See the "<a href="https://blog.cloudflare.com/how-to-stop-running-out-of-ephemeral-ports-and-start-to-love-long-lived-connections">How to stop running out of ports (2022)</a>", "<a href="https://lpc.events/event/16/contributions/1349" target="_blank">How to share IPv4 addresses (2022)</a>" and our <a href="https://cloudflare.tv/event/oZKxMJg4" target="_blank">Cloudflare.TV segment</a>.</p>
	<p>This is pretty much it. Each server is aware which IP addresses and port slices it owns. For inbound routing Unimog inspects the ports and dispatches the packets to appropriate machines.</p>
	<h3 id="sharing-a-subnet-between-data-centers">Sharing a subnet between data centers</h3>
	<p>This is not the end of the story though, we haven't discussed how we can route a single /32 address into a datacenter. Traditionally, in the public Internet, it's only possible to route subnets with granularity of /24 or 256 IP addresses. In our case this would lead to great waste of IP space.</p>
	<p>To solve this problem and improve the utilization of our IP space, we deployed our egress ranges as... <strong>anycast</strong>! With that in place, we customized Unimog and taught it to forward the packets over our <a href="https://blog.cloudflare.com/cloudflare-backbone-internet-fast-lane">backbone network</a> to the right data center. Unimog maintains a database like this:</p><!--kg-card-begin: markdown-->
	<pre><code>198.51.100.1 - forward to LHR
198.51.100.2 - forward to CDG
198.51.100.3 - forward to MAN
...
</code></pre>
	<!--kg-card-end: markdown-->
	<p>With this design, it doesn't matter to which data center return packets are delivered. Unimog can always fix it and forward the data to the right place. Basically, while at the BGP layer we are using anycast, due to our design, semantically an IP identifies a datacenter and an IP and port range identify a specific machine. It behaves almost like a unicast.</p>
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2022/11/image7-6.png" class="kg-image" alt="" loading="lazy"></figure>
	<p>We call this technology stack "<strong>soft-unicast</strong>" and it feels magical. It's like we did unicast in software over anycast in the BGP layer.</p>
	<h3 id="soft-unicast-is-indistinguishable-from-magic">Soft-unicast is indistinguishable from magic</h3>
	<p>With this setup we can achieve significant benefits:</p>
	<ul>
		<li>We are able to share a /32 egress IP amongst <strong>many servers</strong>.</li>
		<li>We can spread a single subnet across <strong>many data centers</strong>, and change it easily on the fly. This allows us to fully use our egress IPv4 ranges.</li>
		<li>We can <strong>group similar IP addresses</strong> together. For example, all the IP addresses tagged with the "UK" tag might form a single continuous range. This reduces the size of the published geofeed.</li>
		<li>It's easy for us to <strong>onboard new egress IP ranges</strong>, like customer IP's. This is useful for some of our products, like <a href="https://www.cloudflare.com/products/zero-trust" target="_blank">Cloudflare Zero Trust</a>.</li>
	</ul>
	<p>All this is done at sensible cost, at no loss to performance and reliability:</p>
	<ul>
		<li>Typically, the user is able to egress directly from the closest datacenter, providing the <strong>best possible performance</strong>.</li>
		<li>Depending on the actual needs we can allocate or release the IP addresses. This gives us <strong>flexibility with the IP</strong> cost management, we don't need to overspend upfront.</li>
		<li>Since we operate multiple egress IP addresses in different locations, the <strong>reliability is not compromised</strong>.</li>
	</ul>
	<h3 id="the-true-location-of-our-ip-addresses-is-the-cloud-">The true location of our IP addresses is: “the cloud”</h3>
	<p>While soft-unicast allows us to gain great efficiency, we've hit some issues. Sometimes we get a question "Where does this IP physically exist?". But it doesn't have an answer! Our egress IPs don't exist physically anywhere. From a BGP standpoint our egress ranges are anycast, so they live everywhere. Logically each address is used in one data center at a time, but we can move it around on demand.</p>
	<h3 id="content-delivery-networks-misdirect-users">Content Delivery Networks misdirect users</h3>
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2022/11/image3-43.png" class="kg-image" alt="" loading="lazy"></figure>
	<p>As another example of problems, here's one issue we've hit with third party CDNs. As we mentioned before, there are three country tags in our pipeline:</p>
	<ul>
		<li>The country tag of the IP eyeball is connecting from.</li>
		<li>The location of our data center.</li>
		<li>The country tag of the IP addresses we chose for the egress connections.</li>
	</ul>
	<p>The fact that our egress address is tagged as "UK" doesn't always mean it actually is being used in the UK. We’ve had cases when a UK-tagged WARP user, due to the maintenance of our LHR data center, was routed to Paris. A popular CDN performed a reverse-lookup of our egress IP, found it tagged as "UK", and directed the user to a London CDN server. This is generally OK... but we actually egressed from Paris at the time. This user ended up routing packets from their home in the UK, to Paris, and back to the UK. This is bad for performance.</p>
	<p>We address this issue by performing DNS requests in the egressing data center. For DNS we use IP addresses tagged with the location of the <strong>data center</strong>, not the intended geolocation for the user. This generally fixes the problem, but sadly, there are still some exceptions.</p>
	<h3 id="the-future-is-here">The future is here</h3>
	<p>Our 2021 experiments with <a href="https://blog.cloudflare.com/addressing-agility">Addressing Agility</a> proved we have plenty of opportunity to innovate with the addressing of the ingress. Soft-unicast shows us we can achieve great flexibility and density on the egress side.</p>
	<p>With each new product, the number of tags we need on the egress grows - from traffic trustworthiness, product category to geolocation. As the pool of usable IPv4 addresses shrinks, we can be sure there will be more innovation in the space. Soft-unicast is our solution, but for sure it's not our last development.</p>
	<p>For now though, it seems like we're moving away from traditional unicast. Our egress IP's really don't exist in a fixed place anymore, and some of our servers don't even own a true unicast IP nowadays.</p>
</div>