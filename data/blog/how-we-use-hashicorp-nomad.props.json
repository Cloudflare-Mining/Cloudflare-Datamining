{
	"initialReadingTime": "7",
	"locale": "en-us",
	"localesAvailable": [],
	"post": {
		"authors": [
			{
				"name": "Thomas Lefebvre",
				"slug": "thomas",
				"bio": null,
				"profile_image": "https://cf-assets.www.cloudflare.com/slt3lc6tev37/6olw7cJNP8SN0qnNsDbwOf/630e6539be032aded6cf0c0d3025dbec/thomas.jpg",
				"location": null,
				"website": null,
				"twitter": null,
				"facebook": null
			}
		],
		"excerpt": "A walkthrough on how we are improving the reliability of our management services running in each data center by using Nomad for dynamic task scheduling.",
		"feature_image": "https://cf-assets.www.cloudflare.com/slt3lc6tev37/2iPuvq238von3vtCh1gd0d/61ecc8a2651263449beb6389e188b85c/how-we-use-hashicorp-nomad.png",
		"featured": false,
		"html": "<p>In this blog post, we will walk you through the reliability model of services running in our more than 200 edge cities worldwide. Then, we will go over how deploying a new dynamic task scheduling system, HashiCorp Nomad, helped us improve the availability of services in each of those data centers, covering how we deployed Nomad and the challenges we overcame along the way. Finally, we will show you both how we currently use Nomad and how we are planning on using it in the future.</p><h2>Reliability model of services running in each data center</h2><p>For this blog post, we will distinguish between two different categories of services running in each data center:</p><ul><li><p><b>Customer-facing services</b>: all of our stack of products that our customers use, such as caching, <a href=\"https://www.cloudflare.com/learning/ddos/glossary/web-application-firewall-waf/\">WAF</a>, DDoS protection, rate-limiting, load-balancing, etc.</p></li><li><p><b>Management services</b>: software required to operate the data center, that is not in the direct request path of customer traffic.</p></li></ul><h3>Customer-facing services</h3><p>The reliability model of our customer-facing services is to run them on all machines in each data center. This works well as it allows each data center’s capacity to scale dynamically by adding more machines.</p><p>Scaling is especially made easy thanks to our dynamic load balancing system, Unimog, which runs on each machine. Its role is to continuously re-balance traffic based on current resource usage and to check the health of services. This helps provide resiliency to individual machine failures and ensures resource usage is close to identical on all machines.</p><p>As an example, here is the CPU usage over a day in one of our data centers where each time series represents one machine and the different colors represent different generations of hardware. Unimog keeps all machines processing traffic and at roughly the same CPU utilization.</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/1zlf1rrhRjEVE9t65RwNsR/474b5fe9a5a1264dd38d545472e83937/image2-1.png\" alt=\"\" class=\"kg-image\" width=\"872\" height=\"186\" loading=\"lazy\"/>\n            \n            </figure><h3>Management services</h3><p>Some of our larger data centers have a substantial number of machines, but sometimes we need to reliably run just a single or a few instances of a management service in each location.</p><p>There are currently a couple of options to do this, each have their own pros and cons:</p><ol><li><p>Deploying the service to all machines in the data center:</p><ul><li><p><b>Pro</b>: it ensures the service’s reliability</p></li><li><p><b>Con</b>: it unnecessarily uses resources which could have been used to serve customer traffic and is not cost-effective</p></li></ul></li><li><p>Deploying the service to a static handful of machines in each data center:</p><ul><li><p><b>Pro</b>: it is less wasteful of resources and more cost-effective</p></li><li><p><b>Con</b>: it runs the risk of service unavailability when those handful of machines unexpectedly fail</p></li></ul></li></ol><p>A third, more viable option, is to use dynamic task scheduling so that only the right amount of resources are used while ensuring reliability.</p><h3>A need for more dynamic task scheduling</h3><p>Having to pick between two suboptimal reliability model options for management services we want running in each data center was not ideal.</p><p>Indeed, some of those services, even though they are not in the request path, are required to continue operating the data center. If the machines running those services become unavailable, in some cases we have to temporarily disable the data center while recovering them. Doing so automatically re-routes users to the next available data center and doesn’t cause disruption. In fact, the entire Cloudflare network is designed to operate with data centers being disabled and brought back automatically. But it’s optimal to route end users to a data center near them so we want to minimize any data center level downtime.</p><p>This led us to realize we needed a system to ensure a certain number of instances of a service were running in each data center, regardless of which physical machine ends up running it.</p><p>Customer-facing services run on all machines in each data center and do not need to be onboarded to that new system. On the other hand, services currently running on a fixed subset of machines with sub-optimal reliability guarantees and services which don’t need to run on all machines are good candidates for onboarding.</p><h3>Our pick: HashiCorp Nomad</h3><p>Armed with our set of requirements, we conducted some research on candidate solutions.</p><p>While Kubernetes was another option, we decided to use HashiCorp’s <a href=\"https://www.nomadproject.io/\">Nomad</a> for the following reasons:</p><ul><li><p>Satisfies our initial requirement, which was reliably running a single instance of a binary with resource isolation in each data center.</p></li><li><p>Has few dependencies and a straightforward integration with Consul. Consul is another piece of HashiCorp software we had already deployed in each datacenter. It provides distributed key-value storage and service discovery capabilities.</p></li><li><p>Is lightweight (single Go binary), easy to deploy and provision new clusters which is a plus when deploying as many clusters as we have data centers.</p></li><li><p>Has a modular task driver (part responsible for executing tasks and providing resource isolation) architecture to support not only containers but also binaries and any custom task driver.</p></li><li><p>Is open source and written in Go. We have Go language experience within the team, and Nomad has a responsive community of maintainers on GitHub.</p></li></ul><h3>Deployment architecture</h3>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/7bD7wQ3kcMFYwDyGWzH1Jp/6328d4d458ac1bd99c74cc0121c57183/image3.png\" alt=\"\" class=\"kg-image\" width=\"622\" height=\"433\" loading=\"lazy\"/>\n            \n            </figure><p>Nomad is split in two different pieces:</p><ol><li><p><b>Nomad Server:</b> instances forming the cluster responsible for scheduling, five per data center to provide sufficient failure tolerance</p></li><li><p><b>Nomad Client:</b> instances executing the actual tasks, running on all machines in every data center</p></li></ol><p>To guarantee Nomad Server cluster reliability, we deployed instances on machines which are part of different failure domains:</p><ul><li><p>In different inter-connected physical data centers forming a single location</p></li><li><p>In different racks, connected to different switches</p></li><li><p>In different multi-node chassis (most of our edge hardware comes in the form of multi-node chassis, one chassis contains four individual servers)</p></li></ul><p>We also added logic to our configuration management tool to ensure we always keep a consistent number of Nomad Server instances regardless of the expansions and decommissions of servers happening on a close to daily basis.</p><p>The logic is rather simple, as server expansions and decommissions happen, the Nomad Server role gets redistributed to a new list of machines. Our configuration management tool then ensures that Nomad Server runs on the new machines before turning it off on the old ones.</p><p>Additionally, because server expansions and decommissions affect a subset of racks at a time and the Nomad Server role assignment logic provides rack-diversity guarantees, the cluster stays healthy as quorum is kept at all times.</p><h3>Job files</h3><p>Nomad job files are templated and checked into a git repository. Our configuration management tool then ensures the jobs are scheduled in every data center. From there, Nomad takes over and ensures the jobs are running at all times in each data center.</p><p>By exposing rack <a href=\"https://www.nomadproject.io/docs/job-specification/meta/\">metadata</a> to each Nomad Client, we are able to make sure each instance of a particular service runs in a different rack and is tied to a different failure domain. This way we make sure that the failure of one rack of servers won’t impact the service health as the service is also running in a different rack, unaffected by the failure.</p><p>We achieve this with the following job file <a href=\"https://www.nomadproject.io/docs/job-specification/constraint/\">constraint</a>:</p>\n            <pre class=\"language-javascript\"><code class=\"language-javascript\">constraint {\n  attribute = &quot;${meta.rack}&quot;\n  operator  = &quot;distinct_property&quot;\n}</pre></code>\n            <h3>Service discovery</h3><p>We leveraged Nomad integration with Consul to get Nomad jobs dynamically added to the Consul Service Catalog. This allows us to discover where a particular service is currently running in each data center by querying Consul. Additionally, with the Consul DNS Interface enabled, we can also use DNS-based lookups to target services running on Nomad.</p><h3>Observability</h3><p>To be able to properly operate as many Nomad clusters as we have data centers, good observability on Nomad clusters and services running on those clusters was essential.</p><p>We use Prometheus to scrape Nomad Server and Client instances running in each data center and Alertmanager to alert on key metrics. Using Prometheus metrics, we built a Grafana dashboard to provide visibility on each cluster.</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/2Y0I7dOMGQEs3TbRNyxbKF/ff758041090dfe472a5d4f24d9811813/image1-1.png\" alt=\"\" class=\"kg-image\" width=\"1909\" height=\"948\" loading=\"lazy\"/>\n            \n            </figure><p>We <a href=\"https://prometheus.io/docs/prometheus/latest/configuration/configuration/#consul_sd_config\">set up</a> our Prometheus instances to discover services running on Nomad by querying the Consul Service Directory and scraping their metrics periodically using the following Prometheus configuration:</p>\n            <pre class=\"language-javascript\"><code class=\"language-javascript\">- consul_sd_configs:\n  - server: localhost:8500\n  job_name: management_service_via_consul\n  relabel_configs:\n  - action: keep\n    regex: management-service\n    source_labels:\n    - __meta_consul_service</pre></code>\n            <p>We then use those metrics to create Grafana dashboards and set up alerts for services running on Nomad.</p><p>To restrict access to Nomad API endpoints, we enabled mutual TLS authentication and are generating client certificates for each entity interacting with Nomad. This way, only entities with a valid client certificate can interact with Nomad API endpoints in order to schedule jobs or perform any CLI operation.</p><h2>Challenges</h2><p>Deploying a new component always comes with its set of challenges; here is a list of a few hurdles we have had to overcome along the way.</p><h3>Initramfs rootfs and pivot_root</h3><p>When starting to use the <code>exec</code> driver to run binaries isolated in a <code>chroot</code> environment, we noticed our stateless root partition running on <a href=\"https://www.kernel.org/doc/Documentation/filesystems/ramfs-rootfs-initramfs.txt\">initramfs</a> was not supported as the task would not start and we got this error message in our logs:</p><p><code>Feb 12 19:49:03 machine nomad-client[258433]: 2020-02-12T19:49:03.332Z [ERROR] client.alloc_runner.task_runner: running driver failed: alloc_id=fa202-63b-33f-924-42cbd5 task=server error=&quot;failed to launch command with executor: rpc error: code = Unknown desc = container_linux.go:346: starting container process caused &quot;process_linux.go:449: container init caused \\&quot;rootfs_linux.go:109: jailing process inside rootfs caused \\\\\\&quot;pivot_root invalid argument\\\\\\&quot;\\&quot;&quot;&quot;</code></p><p>We filed a GitHub <a href=\"https://github.com/hashicorp/nomad/issues/7136\">issue</a> and submitted a workaround <a href=\"https://github.com/hashicorp/nomad/pull/7149\">pull request</a> which was promptly reviewed and merged upstream.</p><p>In parallel, for maximum isolation security, we worked on enabling <code>pivot_root</code> in our setup by modifying our boot process and other team members developed and proposed a patch to the <a href=\"https://lore.kernel.org/linux-fsdevel/20200305193511.28621-1-ignat@cloudflare.com/\">kernel mailing list</a> to make it easier in the future.</p><h3>Resource usage containment</h3><p>One very important aspect was to make sure the resource usage of tasks running on Nomad would not disrupt other services colocated on the same machine.</p><p>Disk space is a shared resource on every machine and being able to set a quota for Nomad was a must. We achieved this by isolating the Nomad data directory to a dedicated fixed-size mount point on each machine. Limiting disk bandwidth and IOPS, however, is not currently supported out of the box by Nomad.</p><p>Nomad job files have a <a href=\"https://www.nomadproject.io/docs/job-specification/resources/#memory\">resources section</a> where memory and CPU usage can be limited (memory is in MB, cpu is in MHz):</p>\n            <pre class=\"language-javascript\"><code class=\"language-javascript\">resources {\n  memory = 2000\n  cpu = 500\n}</pre></code>\n            <p>This uses cgroups under the hood and our testing showed that while memory limits are enforced as one would expect, the CPU limits are soft limits and not enforced as long as there is available CPU on the host machine.</p><h3>Workload (un)predictability</h3><p>As mentioned above, all machines currently run the same customer-facing workload. Scheduling individual jobs dynamically with Nomad to run on single machines challenges that assumption.</p><p>While our dynamic load balancing system, Unimog, balances requests based on resource usage to ensure it is close to identical on all machines, batch type jobs with spiky resource usage can pose a challenge.</p><p>We will be paying attention to this as we onboard more services and:</p><ul><li><p>attempt to limit resource usage spikiness of Nomad jobs with constraints aforementioned</p></li><li><p>ensure Unimog adjusts to this batch type workload and does not end up in a positive feedback loop</p></li></ul><h2>What we are running on Nomad</h2><p>Now Nomad has been deployed in every data center, we are able to improve the reliability of management services essential to operations by gradually onboarding them. We took a first step by onboarding our reboot and maintenance management service.</p><h3>Reboot and maintenance management service</h3><p>In each data center, we run a service which facilitates online unattended rolling reboots and maintenance of machines. This service used to run on a single well-known machine in each data center. This made it vulnerable to single machine failures and when down prevented machines from enabling automatically after a reboot. Therefore, it was a great first service to be onboarded to Nomad to improve its reliability.</p><p>We now have a guarantee this service is always running in each data center regardless of individual machine failures. Instead of other machines relying on a well-known address to target this service, they now query Consul DNS and dynamically figure out where the service is running to interact with it.</p><p>This is a big improvement in terms of reliability for this service, therefore many more management services are expected to follow in the upcoming months and we are very excited for this to happen.</p>",
		"id": "26zJkCEC3Mp3Il7GFxJuX3",
		"localeList": {
			"name": "How we use HashiCorp Nomad Config",
			"enUS": "English for Locale",
			"zhCN": "No Page for Locale",
			"zhHansCN": "No Page for Locale",
			"zhTW": "No Page for Locale",
			"frFR": "No Page for Locale",
			"deDE": "No Page for Locale",
			"itIT": "No Page for Locale",
			"jaJP": "No Page for Locale",
			"koKR": "No Page for Locale",
			"ptBR": "No Page for Locale",
			"esLA": "No Page for Locale",
			"esES": "No Page for Locale",
			"enAU": "No Page for Locale",
			"enCA": "No Page for Locale",
			"enIN": "No Page for Locale",
			"enGB": "No Page for Locale",
			"idID": "No Page for Locale",
			"ruRU": "No Page for Locale",
			"svSE": "No Page for Locale",
			"viVN": "No Page for Locale",
			"plPL": "No Page for Locale",
			"arAR": "No Page for Locale",
			"nlNL": "No Page for Locale",
			"thTH": "No Page for Locale",
			"trTR": "No Page for Locale",
			"heIL": "No Page for Locale",
			"lvLV": "No Page for Locale",
			"etEE": "No Page for Locale",
			"ltLT": "No Page for Locale"
		},
		"meta_description": "A walkthrough on how we are improving the reliability of our management services running in each data center by using Nomad for dynamic task scheduling.",
		"metadata": {
			"title": "How we use HashiCorp Nomad",
			"description": "A walkthrough on how we are improving the reliability of our management services running in each data center by using Nomad for dynamic task scheduling.",
			"imgPreview": "https://cf-assets.www.cloudflare.com/slt3lc6tev37/2OOtGzi0ie6RW2GzgQ1uQ2/1072974e3ec8570a63bd5deb6653e477/how-we-use-hashicorp-nomad-FB0Lz3.png"
		},
		"primary_author": {},
		"published_at": "2020-06-05T12:00:00.000+01:00",
		"slug": "how-we-use-hashicorp-nomad",
		"tags": [
			{
				"id": "3OPPjcK7cKutTdeAjpThfG",
				"name": "Edge",
				"slug": "edge"
			},
			{
				"id": "6agYbJnOhJ7B2ie9Ctf2cf",
				"name": "HashiCorp",
				"slug": "hashicorp"
			},
			{
				"id": "48r7QV00gLMWOIcM1CSDRy",
				"name": "Speed & Reliability",
				"slug": "speed-and-reliability"
			}
		],
		"title": "How we use HashiCorp Nomad",
		"updated_at": "2024-08-27T02:01:47.782Z",
		"url": "https://blog.cloudflare.com/how-we-use-hashicorp-nomad"
	},
	"translations": {
		"posts.by": "By",
		"footer.gdpr": "GDPR",
		"lang_blurb1": "This post is also available in {lang1}.",
		"lang_blurb2": "This post is also available in {lang1} and {lang2}.",
		"lang_blurb3": "This post is also available in {lang1}, {lang2} and {lang3}.",
		"footer.blurb": "Cloudflare's connectivity cloud protects <a target='_blank' href='https://www.cloudflare.com/network-services/' rel='noreferrer'>entire corporate networks</a>, helps customers build <a target='_blank' href='https://workers.cloudflare.com/' rel='noreferrer'>Internet-scale applications efficiently</a>, accelerates any <a target='_blank' href='https://www.cloudflare.com/performance/accelerate-internet-applications/' rel='noreferrer'>website or Internet application</a>, <a target='_blank' href='https://www.cloudflare.com/ddos/' rel='noreferrer'>wards off DDoS attacks</a>, keeps <a target='_blank' href='https://www.cloudflare.com/application-security/' rel='noreferrer'>hackers at bay</a>, and can help you on <a target='_blank' href='https://www.cloudflare.com/products/zero-trust/' rel='noreferrer'>your journey to Zero Trust</a>.<br/><br/>Visit <a target='_blank' href='https://one.one.one.one/' rel='noreferrer'>1.1.1.1</a> from any device to get started with our free app that makes your Internet faster and safer.<br/><br/>To learn more about our mission to help build a better Internet, <a target='_blank' href='https://www.cloudflare.com/learning/what-is-cloudflare/' rel='noreferrer'>start here</a>. If you&apos;re looking for a new career direction, check out <a target='_blank' href='http://www.cloudflare.com/careers' rel='noreferrer'>our open positions</a>.",
		"footer.press": "Press",
		"header.title": "The Cloudflare Blog",
		"footer.careers": "Careers",
		"footer.company": "Company",
		"footer.support": "Support",
		"footer.the_net": "theNet",
		"footer.our_team": "Our team",
		"footer.webinars": "Webinars",
		"page.more_posts": "More posts",
		"posts.time_read": "{time} min read",
		"footer.community": "Community",
		"footer.resources": "Resources",
		"footer.solutions": "Solutions",
		"footer.trademark": "Trademark",
		"header.subscribe": "Subscribe",
		"footer.compliance": "Compliance",
		"footer.free_plans": "Free plans",
		"footer.impact_ESG": "Impact/ESG",
		"posts.follow_on_X": "Follow on X",
		"footer.help_center": "Help center",
		"footer.network_map": "Network Map",
		"header.please_wait": "Please Wait",
		"page.related_posts": "Related posts",
		"footer.case_studies": "Case Studies",
		"footer.connect_2024": "Connect 2024",
		"footer.terms_of_use": "Terms of Use",
		"footer.white_papers": "White Papers",
		"footer.cloudflare_tv": "Cloudflare TV",
		"footer.community_hub": "Community Hub",
		"footer.compare_plans": "Compare plans",
		"footer.contact_sales": "Contact Sales",
		"header.contact_sales": "Contact Sales",
		"header.email_address": "Email Address",
		"page.error.not_found": "Page not found",
		"footer.developer_docs": "Developer docs",
		"footer.privacy_policy": "Privacy Policy",
		"footer.request_a_demo": "Request a demo",
		"page.continue_reading": "Continue reading",
		"footer.analysts_report": "Analyst reports",
		"footer.for_enterprises": "For enterprises",
		"footer.getting_started": "Getting Started",
		"footer.learning_center": "Learning Center",
		"footer.project_galileo": "Project Galileo",
		"pagination.newer_posts": "Newer Posts",
		"pagination.older_posts": "Older Posts",
		"posts.social_buttons.x": "Discuss on X",
		"footer.about_cloudflare": "About Cloudflare",
		"footer.athenian_project": "Athenian Project",
		"footer.become_a_partner": "Become a partner",
		"footer.cloudflare_radar": "Cloudflare Radar",
		"footer.network_services": "Network services",
		"footer.trust_and_safety": "Trust & Safety",
		"header.get_started_free": "Get Started Free",
		"page.search.placeholder": "Search Cloudflare",
		"footer.cloudflare_status": "Cloudflare Status",
		"footer.cookie_preference": "Cookie Preferences",
		"header.valid_email_error": "Must be valid email.",
		"footer.connectivity_cloud": "Connectivity cloud",
		"footer.developer_services": "Developer services",
		"footer.investor_relations": "Investor relations",
		"page.not_found.error_code": "Error Code: 404",
		"footer.logos_and_press_kit": "Logos & press kit",
		"footer.application_services": "Application services",
		"footer.get_a_recommendation": "Get a recommendation",
		"posts.social_buttons.reddit": "Discuss on Reddit",
		"footer.sse_and_sase_services": "SSE and SASE services",
		"page.not_found.outdated_link": "You may have used an outdated link, or you may have typed the address incorrectly.",
		"footer.report_security_issues": "Report Security Issues",
		"page.error.error_message_page": "Sorry, we can't find the page you are looking for.",
		"header.subscribe_notifications": "Subscribe to receive notifications of new posts:",
		"footer.cloudflare_for_campaigns": "Cloudflare for Campaigns",
		"header.subscription_confimation": "Subscription confirmed. Thank you for subscribing!",
		"posts.social_buttons.hackernews": "Discuss on Hacker News",
		"footer.diversity_equity_inclusion": "Diversity, equity & inclusion",
		"footer.critical_infrastructure_defense_project": "Critical Infrastructure Defense Project"
	}
}