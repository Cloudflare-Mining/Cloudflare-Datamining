<div class="mb2 gray5">5 min read</div><img class="mr2" src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/1ME74WmN0LL35y8Ic7DShc/09dc3ff329bb82197dd76eef7d2fab52/revenge-listening-sockets.png" alt="">
<div class="post-content lh-copy gray1">
	<p>Back in November we wrote <a href="https://blog.cloudflare.com/the-story-of-one-latency-spike">a blog post about one latency spike</a>. Today I'd like to share a continuation of that story. As it turns out, the misconfigured <code>rmem</code> setting wasn't the only source of added latency.</p>
	<p>It looked like Mr. Wolf hadn't finished his job.</p>
	<p>After adjusting the previously discussed <code>rmem</code> sysctl we continued monitoring our systems' latency. Among other things we measured <code>ping</code> times to our edge servers. While the worst case improved and we didn't see 1000ms+ pings anymore, the line still wasn't flat. Here's a graph of ping latency between an idling internal machine and a production server. The test was done within the datacenter, the packets never went to the public internet. The Y axis of the chart shows <code>ping</code> times in milliseconds, the X axis is the time of the measurement. Measurements were taken every second for over 6 hours:</p>
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/1XHIKdZc7gzLfYQHJOqlvn/75425f0074af36a9621cad2074873916/ping-48m4-csv-before.png" alt="" class="kg-image" width="1000" height="400" loading="lazy">

	</figure>
	<p>As you can see most pings finished below 1ms. But out of 21,600 measurements about 20 had high latency of up to 100ms. Not ideal, is it?</p>
	<div class="flex anchor relative">
		<h2 id="system-tap">System tap</h2>
		<a href="https://blog.cloudflare.com/#system-tap" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>The latency occurred within our datacenter and the packets weren't lost. This suggested a kernel issue again. Linux responds to ICMP pings from its soft interrupt handling code. A delay in handling <code>ping</code> indicates a delay in Soft IRQ handling which is really bad and can affect all packets delivered to a machine. Using the <a href="https://github.com/cloudflare/cloudflare-blog/blob/master/2016-04-bind-to-star/histogram-kernel.stp">system tap script</a> we were able to measure the time distribution of the main soft IRQ function <code>net_rx_action</code>:</p>
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/6A2YakORYW8kp1LE15b8nk/c5af58c7161329b1bae73ed3fa870156/a-net.png" alt="" class="kg-image" width="1069" height="494" loading="lazy">

	</figure>
	<p>This distribution was pretty awful. While most of the calls to <code>net_rx_action</code> were handled in under 81us (average), the slow outliers were really bad. Three calls took a whopping 32ms! No wonder the <code>ping</code> times were off.</p>
	<div class="flex anchor relative">
		<h2 id="the">The </h2>
		<a href="https://blog.cloudflare.com/#the" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>With some back and forth with flame graphs and the <a href="https://github.com/cloudflare/cloudflare-blog/blob/master/2016-04-bind-to-star/histogram-kernel.stp"><code>histogram-kernel.stp</code> script</a> we went deeper to look for the culprit. We found that <a href="http://lxr.free-electrons.com/source/net/ipv4/tcp_ipv4.c?v=3.18#L1585"><code>tcp_v4_rcv</code></a> had a similarly poor latency distribution. More specifically the problem lies between lines 1637 and 1642 in the <code>tcp_v4_rcv</code> function in the <code>tcp_ipv4.c</code> file. We wrote <a href="https://github.com/cloudflare/cloudflare-blog/blob/master/2016-04-bind-to-star/histogram-kernel2.stp">another script to show</a> just that:</p>
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/2YByqoyG6F8kapoqFISNcG/b157b7f5ccb58eea836f7833c175fd47/a-tcp.png" alt="" class="kg-image" width="1069" height="494" loading="lazy">

	</figure>
	<p>The latency is created at this specific line in <code>tcp_v4_rcv</code> function:</p>
	<pre class="language-.c"><code class="language-.c">sk = __inet_lookup_skb(&amp;tcp_hashinfo, skb, th-&gt;source, th-&gt;dest);</code></pre>
	<p>The numbers shown above indicate that the function usually terminated quickly, in under 2us, but sometimes it hit a slow path and took 1-2ms to finish.</p>
	<p>The <a href="http://lxr.free-electrons.com/source/include/net/inet_hashtables.h?v=3.18#L343"><code>__inet_lookup_skb</code></a> is inlined which makes it tricky to accurately measure. Fortunately the function is simple - all it does is to call <code>__inet_lookup_established</code> and <code>__inet_lookup_listener</code>. It's the latter function that was causing the trouble:</p>
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/4IcyrSCQ0rO6dGDDMJIYAw/6b90eca55fc434441a180a04be0ec071/a-tcp-copy.png" alt="" class="kg-image" width="1069" height="447" loading="lazy">

	</figure>
	<p>Let's discuss how <code>__inet_lookup</code> works. This function tries to find an appropriate connection <code>sock struct</code> structure for a packet. This is done in the <code>__inet_lookup_established</code> call. If that fails, the <code>__inet_lookup</code> will attempt to find a bound socket in listening state that could potentially handle the packet. For example, if the packet is SYN and the listening socket exists we should respond with SYN+ACK. If there is no bound listening socket we should send an RST instead. The <code>__inet_lookup_listener</code> function finds the bound socket in the <code>LHTABLE</code> hash table. It does so by using the destination port as a hash and picks an appropriate bucket in the hash table. Then it iterates over it linearly to find the matching listening socket.</p>
	<p>To understand the problem we traced the slow packets, with <a href="https://github.com/cloudflare/cloudflare-blog/blob/master/2016-04-bind-to-star/duration-inet-lookup2.stp">another crafted system tap script</a>. It hooks onto <code>__inet_lookup_listener</code> and prints out the details of only the slow packets:</p>
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/3OBYuGrWgdW7nOI1bp4ZIi/ecf8548bcd5e9e1e930080a93190d530/a-inet.png" alt="" class="kg-image" width="1069" height="313" loading="lazy">

	</figure>
	<p>With this data we went deeper and matched these log lines to specific packets captured with <code>tcpdump</code>. I'll spare you the details, but these are inbound SYN and RST packets which destination port modulo 32 is equal to 21. Check it out:</p>
	<ul>
		<li>
			<p>16725 % 32 = 21</p>
		</li>
		<li>
			<p>53 % 32 = 21</p>
		</li>
		<li>
			<p>63925 % 32 = 21</p>
		</li>
	</ul>
	<p>Now, where does this magic number come from?</p>
	<div class="flex anchor relative">
		<h2 id="the-listening-hash-table">The listening hash table</h2>
		<a href="https://blog.cloudflare.com/#the-listening-hash-table" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>As mentioned above, Linux maintains a listening hash table containing the listening TCP sockets - the <code>LHTABLE</code>. It has a fixed size of <a href="http://lxr.free-electrons.com/source/include/net/inet_hashtables.h?v=3.18#L117">32 buckets</a>:</p>
	<pre class="language-.txt"><code class="language-.txt">/* Yes, really, this is all you need. */
#define INET_LHTABLE_SIZE       32</code></pre>
	<p>To recap:</p>
	<ul>
		<li>
			<p>All the SYN and RST packets trigger a lookup in LHTABLE. Since the connection entry doesn't exist the <code>__inet_lookup_established</code> call fails and <code>__inet_lookup_listener</code> will be called.</p>
		</li>
		<li>
			<p>LHTABLE is small - it has only 32 buckets.</p>
		</li>
		<li>
			<p>LHTABLE is hashed by destination port only.</p>
		</li>
	</ul>
	<p>It's time for a quick diversion. Let's speak about CloudFlare's DNS server.</p>
	<div class="flex anchor relative">
		<h2 id="lets-speak-about-dns">Let's speak about DNS</h2>
		<a href="https://blog.cloudflare.com/#lets-speak-about-dns" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>At CloudFlare we are using a custom DNS server called <code>rrdns</code>. Among many other requirements, the server is designed to withstand DDoS attacks.</p>
	<p>Even though our server is pretty fast, when a large attack kicks in it might be unable to cope with the attack load. If that happens we must contain the damage - even if one IP address is under a heavy attack the server must still handle legitimate traffic on other IP addresses. In fact, our DNS architecture is designed to spread the load among 16k IP addresses.</p>
	<p>When <a href="https://www.cloudflare.com/ddos/under-attack">an IP address is under attack</a>, and the server is not keeping up with incoming packets, the kernel receive queue on a UDP socket will overflow. We monitor that by looking at the <code>netstat</code> counters:</p>
	<pre class="language-bash"><code class="language-bash">$ netstat -s --udp
Udp:
    43957575 packet receive errors</code></pre>
	<p>With that number increasing we can see the affected IP addresses by listing the UDP sockets with non-empty receive queues:</p>
	<pre class="language-bash"><code class="language-bash">$ netstat -ep4ln --udp|grep 53 |egrep -v "^udp *0"
udp   524293  0 173.245.1.1:53  0.0.0.0:*   0
udp   524288  0 173.245.2.3:53  0.0.0.0:*   0</code></pre>
	<p>In this case two IP addresses received heavy UDP traffic. It was more than the DNS server could handle, the receive queues built up and eventually overflowed. Fortunately, because we are binding to specific IP addresses, overflowing some UDP receive queues won't affect any other IP addresses.</p>
	<p>Binding to specific IP addresses is critical to keep our DNS infrastructure online. With this setup even if other mitigation techniques fail and the DNS server is left exposed to the packet flood, we are certain the attack will not affect handling DNS on other IP addresses.</p>
	<p>But what does that have to do with the <code>LHTABLE</code>? Well, in our setup we bound to specific IP addresses for both UDP <i>and</i> TCP. While having 16k listening <a href="http://lxr.free-electrons.com/source/net/ipv4/udp.c?v=3.18#L476">sockets in UDP is okay</a>, it turns out it is not fine for TCP.</p>
	<div class="flex anchor relative">
		<h2 id="what-happened">What happened</h2>
		<a href="https://blog.cloudflare.com/#what-happened" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>Due to our DNS setup we had 16k TCP sockets bound to different IP addresses on port 53. Since the port number is fixed, all these sockets ended in exactly one <code>LHTABLE</code> bucket. This particular bucket was number 21 (53 % 32 = 21). When an RST or SYN packet hit it, the <code>__inet_lookup_listener</code> call had to traverse all 16k socket entries. This wasn't fast, in fact it took 2ms to finish.</p>
	<p>To solve the problem we deployed two changes:</p>
	<ul>
		<li>
			<p>For TCP connections our DNS server now binds to ANY_IP address (aka: 0.0.0.0:53, *:53). We call this "bind to star". While binding to specific IP addresses is still necessary for UDP, there is little benefit in doing that for the TCP traffic. For TCP we can bind to star safely, without compromising our DDoS defenses.</p>
		</li>
		<li>
			<p>We increased the <code>LHTABLE</code> size in our kernels. We are not the first to do that: Bill Sommerfeld from Google <a href="http://patchwork.ozlabs.org/patch/79014">suggested that back in 2011</a>.</p>
		</li>
	</ul>
	<p>With these changes deployed the <code>ping</code> times within our datacenter are finally flat, as they should always have been:</p>
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/2SDuQY3mNlpBZoynVywAMr/77e69c736d0fc78c055583299ee41548/ping-48m3-csv-after.png" alt="" class="kg-image" width="1000" height="400" loading="lazy">

	</figure>
	<div class="flex anchor relative">
		<h2 id="final-words">Final words</h2>
		<a href="https://blog.cloudflare.com/#final-words" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>You can't have a very large number of bound TCP sockets and we learned that the hard way. We learned a bit about the Linux networking stack: the fact that <code>LHTABLE</code> is fixed size and is hashed by destination port only. Once again we showed a couple of <a href="https://github.com/cloudflare/cloudflare-blog/blob/master/2016-04-bind-to-star">powerful of System Tap scripts</a>.</p>
	<p>With the fixes deployed maximum latency numbers have dropped significantly. We are confident that soft interrupt handling in <code>net_rx_action</code> is behaving well.</p>
	<p>Mr Wolf has finally finished his assignment.</p>
	<p>If this sounds interesting, <a href="https://www.cloudflare.com/join-our-team">consider joining us</a>. We have teams in Singapore, San Francisco and London.</p>
</div>