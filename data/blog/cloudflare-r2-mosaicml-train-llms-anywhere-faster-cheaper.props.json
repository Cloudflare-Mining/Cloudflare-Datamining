{
	"footerBlurb": "Cloudflare's connectivity cloud protects <a target='_blank' href='https://www.cloudflare.com/network-services/' rel='noreferrer'>entire corporate networks</a>, helps customers build <a target='_blank' href='https://workers.cloudflare.com/' rel='noreferrer'>Internet-scale applications efficiently</a>, accelerates any <a target='_blank' href='https://www.cloudflare.com/performance/accelerate-internet-applications/' rel='noreferrer'>website or Internet application</a>, <a target='_blank' href='https://www.cloudflare.com/ddos/' rel='noreferrer'>wards off DDoS attacks</a>, keeps <a target='_blank' href='https://www.cloudflare.com/application-security/' rel='noreferrer'>hackers at bay</a>, and can help you on <a target='_blank' href='https://www.cloudflare.com/products/zero-trust/' rel='noreferrer'>your journey to Zero Trust</a>.<br/><br/>Visit <a target='_blank' href='https://one.one.one.one/' rel='noreferrer'>1.1.1.1</a> from any device to get started with our free app that makes your Internet faster and safer.<br/><br/>To learn more about our mission to help build a better Internet, <a target='_blank' href='https://www.cloudflare.com/learning/what-is-cloudflare/' rel='noreferrer'>start here</a>. If you&apos;re looking for a new career direction, check out <a target='_blank' href='http://www.cloudflare.com/careers' rel='noreferrer'>our open positions</a>.",
	"initialReadingTime": "4",
	"locale": "en-us",
	"localesAvailable": [
		"zh-cn",
		"fr-fr",
		"de-de",
		"ja-jp",
		"es-es"
	],
	"post": {
		"authors": [
			{
				"name": "Abhinav Venigalla (Guest Author)",
				"slug": "abhinav",
				"bio": "Abhi is the NLP Architect at MosaicML and works on reducing the time, complexity, and cost of training LLMs.",
				"profile_image": "https://cf-assets.www.cloudflare.com/slt3lc6tev37/2YVIhzp3dzHMd7KPDS1N0x/09cf990cf0661048f991d2e242d74f5e/abhinav.jpg",
				"location": null,
				"website": null,
				"twitter": "@abhi_venigalla",
				"facebook": null
			},
			{
				"name": "Phillip Jones",
				"slug": "phillip",
				"bio": null,
				"profile_image": "https://cf-assets.www.cloudflare.com/slt3lc6tev37/5KTNNpw9VHuwoZlwWsA7MN/8d9f90758b4674f9ed693f5d87439807/phillip.jpg",
				"location": null,
				"website": null,
				"twitter": "@akaphill",
				"facebook": null
			},
			{
				"name": "Abhi Das",
				"slug": "abhi",
				"bio": null,
				"profile_image": "https://cf-assets.www.cloudflare.com/slt3lc6tev37/1aSM2Cr0Slgy7gVytmzkgS/528b55e02da8bb82997ad1fb14ccf1c6/abhi.jpeg",
				"location": null,
				"website": null,
				"twitter": "@abhidasone",
				"facebook": null
			}
		],
		"excerpt": "Together, Cloudflare and MosaicML give customers the freedom to train LLMs on any compute, anywhere in the world, with zero switching costs. That means faster, cheaper training runs, and no vendor lock in.",
		"feature_image": "https://cf-assets.www.cloudflare.com/slt3lc6tev37/32hfH6xKjGVDSmSj0oLoEC/8d44f3b01963083aa9bfbf9d21b62025/cloudflare-r2-mosaicml-train-llms-anywhere-faster-cheaper.png",
		"featured": false,
		"html": "<p></p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/5BYHRApu97ZOlXvHf0cIMs/0a0788b5d2ed24de82c70c1497c02fa0/111.png\" alt=\"\" class=\"kg-image\" width=\"512\" height=\"288\" loading=\"lazy\"/>\n            \n            </figure><p>Building the large language models (LLMs) and diffusion models that power <a href=\"https://www.cloudflare.com/learning/ai/what-is-generative-ai/\">generative AI</a> requires massive infrastructure. The most obvious component is compute – hundreds to thousands of GPUs – but an equally critical (and often overlooked) component is the <b>data storage infrastructure.</b> Training datasets can be terabytes to petabytes in size, and this data needs to be read in parallel by thousands of processes. In addition, model checkpoints need to be saved frequently throughout a training run, and for LLMs these checkpoints can each be hundreds of gigabytes!</p><p>To manage storage costs and scalability, many machine learning teams have been moving to <a href=\"https://www.cloudflare.com/learning/cloud/what-is-object-storage/\">object storage</a> to host their datasets and checkpoints. Unfortunately, most object store providers use egress fees to “lock in” users to their platform. This makes it very difficult to leverage GPU capacity across multiple cloud providers, or take advantage of lower / dynamic pricing elsewhere, since the data and model checkpoints are too expensive to move. At a time when cloud GPUs are scarce, and new hardware options are entering the market, it’s more important than ever to stay flexible.</p><p>In addition to high egress fees, there is a technical barrier to object-store-centric machine learning training. Reading and writing data between object storage and compute clusters requires high throughput, efficient use of network bandwidth, determinism, and elasticity (the ability to train on different #s of GPUs). Building training software to handle all of this correctly and reliably is hard!</p><p>Today, we’re excited to show how MosaicML’s tools and Cloudflare R2 can be used together to address these challenges. First, with MosaicML’s open source <a href=\"https://github.com/mosaicml/streaming\">StreamingDataset</a> and <a href=\"https://github.com/mosaicml/composer\">Composer</a> libraries, you can easily stream in training data and read/write model checkpoints back to R2. All you need is an Internet connection. Second, thanks to R2’s zero-egress pricing, you can start/stop/move/resize jobs in response to GPU availability and prices across compute providers, without paying any data transfer fees. The MosaicML training platform makes it dead simple to orchestrate such training jobs across multiple clouds.</p><p>Together, Cloudflare and MosaicML give you the freedom to train LLMs on <i>any</i> compute, <i>anywhere</i> in the world, with zero switching costs. That means faster, cheaper training runs, and no vendor lock in :)</p><blockquote><p><i>“With the MosaicML training platform, customers can efficiently use R2 as the durable storage backend for training LLMs on any compute provider with zero egress fees. AI companies are facing outrageous cloud costs, and they are on the hunt for the tools that can provide them with the speed and flexibility to train their best model at the best price.”</i> – <b>Naveen Rao, CEO and co-founder, MosaicML</b></p></blockquote>\n          <div class=\"flex anchor relative\">\n            <h3 id=\"reading-data-from-r2-using-streamingdataset\">Reading data from R2 using StreamingDataset</h3>\n            <a href=\"#reading-data-from-r2-using-streamingdataset\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n              <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n            </a>\n          </div>\n        <p>To read data from R2 efficiently and deterministically, you can use the MosaicML <a href=\"https://github.com/mosaicml/streaming\">StreamingDataset</a> library. First, write your training data (images, text, video, anything!) into <code>.mds</code> shard files using the provided Python API:</p>\n            <pre class=\"language-python\"><code class=\"language-python\">import numpy as np\nfrom PIL import Image\nfrom streaming import MDSWriter\n\n# Local or remote directory in which to store the compressed output files\ndata_dir = &#039;path-to-dataset&#039;\n\n# A dictionary mapping input fields to their data types\ncolumns = {\n    &#039;image&#039;: &#039;jpeg&#039;,\n    &#039;class&#039;: &#039;int&#039;\n}\n\n# Shard compression, if any\ncompression = &#039;zstd&#039;\n\n# Save the samples as shards using MDSWriter\nwith MDSWriter(out=data_dir, columns=columns, compression=compression) as out:\n    for i in range(10000):\n        sample = {\n            &#039;image&#039;: Image.fromarray(np.random.randint(0, 256, (32, 32, 3), np.uint8)),\n            &#039;class&#039;: np.random.randint(10),\n        }\n        out.write(sample)</pre></code>\n            <p>After your dataset has been converted, you can upload it to R2. Below we demonstrate this with the <code>awscli</code> command line tool, but you can also use `wrangler `or any S3-compatible tool of your choice. StreamingDataset will also support direct cloud writing to R2 soon!</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">$ aws s3 cp --recursive path-to-dataset s3://my-bucket/folder --endpoint-url $S3_ENDPOINT_URL</pre></code>\n            <p>Finally, you can read the data into any device that has read access to your R2 bucket. You can fetch individual samples, loop over the dataset, and feed it into a standard PyTorch dataloader.</p>\n            <pre class=\"language-python\"><code class=\"language-python\">from torch.utils.data import DataLoader\nfrom streaming import StreamingDataset\n\n# Make sure that R2 credentials and $S3_ENDPOINT_URL are set in your environment    \n# e.g. export S3_ENDPOINT_URL=&quot;https://[uid].r2.cloudflarestorage.com&quot;\n\n# Remote path where full dataset is persistently stored\nremote = &#039;s3://my-bucket/folder&#039;\n\n# Local working dir where dataset is cached during operation\nlocal = &#039;/tmp/path-to-dataset&#039;\n\n# Create streaming dataset\ndataset = StreamingDataset(local=local, remote=remote, shuffle=True)\n\n# Let&#039;s see what is in sample #1337...\nsample = dataset[1337]\nimg = sample[&#039;image&#039;]\ncls = sample[&#039;class&#039;]\n\n# Create PyTorch DataLoader\ndataloader = DataLoader(dataset)</pre></code>\n            <p>StreamingDataset comes out of the box with high performance, elastic determinism, fast resumption, and multi-worker support. It also uses smart shuffling and distribution to ensure that download bandwidth is minimized. Across a variety of workloads such as LLMs and diffusion models, we find that there is no impact on training throughput (no dataloader bottleneck) when training from object stores like R2. For more information, check out the StreamingDataset <a href=\"https://www.mosaicml.com/blog/mosaicml-streamingdataset\">announcement blog</a>!</p>\n          <div class=\"flex anchor relative\">\n            <h3 id=\"reading-writing-model-checkpoints-to-r2-using-composer\">Reading/writing model checkpoints to R2 using Composer</h3>\n            <a href=\"#reading-writing-model-checkpoints-to-r2-using-composer\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n              <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n            </a>\n          </div>\n        <p>Streaming data into your training loop solves half of the problem, but how do you load/save your model checkpoints? Luckily, if you use a training library like <a href=\"https://github.com/mosaicml/composer\">Composer</a>, it’s as easy as pointing at an R2 path!</p>\n            <pre class=\"language-python\"><code class=\"language-python\">from composer import Trainer\n...\n\n# Make sure that R2 credentials and $S3_ENDPOINT_URL are set in your environment\n# e.g. export S3_ENDPOINT_URL=&quot;https://[uid].r2.cloudflarestorage.com&quot;\n\ntrainer = Trainer(\n        run_name=&#039;mpt-7b&#039;,\n        model=model,\n        train_dataloader=train_loader,\n        ...\n        save_folder=s3://my-bucket/mpt-7b/checkpoints,\n        save_interval=&#039;1000ba&#039;,\n        # load_path=s3://my-bucket/mpt-7b-prev/checkpoints/ep0-ba100-rank0.pt,\n    )</pre></code>\n            <p>Composer uses asynchronous uploads to minimize wait time as checkpoints are being saved during training. It also works out of the box with multi-GPU and multi-node training, and <b>does not require a shared file system.</b> This means you can skip setting up an expensive EFS/NFS system for your compute cluster, saving thousands of dollars or more per month on public clouds. All you need is an Internet connection and appropriate credentials – your checkpoints arrive safely in your R2 bucket giving you scalable and secure storage for your private models.</p>\n          <div class=\"flex anchor relative\">\n            <h3 id=\"using-mosaicml-and-r2-to-train-anywhere-efficiently\">Using MosaicML and R2 to train anywhere efficiently</h3>\n            <a href=\"#using-mosaicml-and-r2-to-train-anywhere-efficiently\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n              <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n            </a>\n          </div>\n        <p>Using the above tools together with Cloudflare R2 enables users to run training workloads on any compute provider, with total freedom and zero switching costs.</p><p>As a demonstration, in Figure X we use the MosaicML training platform to launch an LLM training job starting on Oracle Cloud Infrastructure, with data streaming in and checkpoints uploaded back to R2. Part way through, we pause the job and seamlessly resume on a different set of GPUs on Amazon Web Services. Composer loads the model weights from the last saved checkpoint in R2, and the streaming dataloader instantly resumes to the correct batch. Training continues deterministically. Finally, we move again to Google Cloud to finish the run.</p><p>As we train our LLM across three cloud providers, the only costs we pay are for GPU compute and data storage. No egress fees or lock in thanks to Cloudflare R2!</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/6d7rJLiQ1wI12thUIr792s/c79f31c62d5a4e3f2e55bea4d54273c2/image2-19.png\" alt=\"Using the MosaicML training platform with Cloudflare R2 to run an LLM training job across three different cloud providers, with zero egress fees.\" class=\"kg-image\" width=\"1999\" height=\"1304\" loading=\"lazy\"/>\n            \n            </figure><p><i>Using the MosaicML training platform with Cloudflare R2 to run an LLM training job across three different cloud providers, with zero egress fees.</i></p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">$ mcli get clusters\nNAME            PROVIDER      GPU_TYPE   GPUS             INSTANCE                   NODES\nmml-1            MosaicML   │  a100_80gb  8             │  mosaic.a100-80sxm.1        1    \n                            │  none       0             │  cpu                        1    \ngcp-1            GCP        │  t4         -             │  n1-standard-48-t4-4        -    \n                            │  a100_40gb  -             │  a2-highgpu-8g              -    \n                            │  none       0             │  cpu                        1    \naws-1            AWS        │  a100_40gb  ≤8,16,...,32  │  p4d.24xlarge               ≤4   \n                            │  none       0             │  cpu                        1    \noci-1            OCI        │  a100_40gb  8,16,...,64   │  oci.bm.gpu.b4.8            ≤8  \n                            │  none       0             │  cpu                        1    \n\n$ mcli create secret s3 --name r2-creds --config-file path/to/config --credentials-file path/to/credentials\n✔  Created s3 secret: r2-creds      \n\n$ mcli create secret env S3_ENDPOINT_URL=&quot;https://[uid].r2.cloudflarestorage.com&quot;\n✔  Created environment secret: s3-endpoint-url      \n               \n$ mcli run -f mpt-125m-r2.yaml --follow\n✔  Run mpt-125m-r2-X2k3Uq started                                                                                    \ni  Following run logs. Press Ctrl+C to quit.                                                                            \n                                                                                                                        \nCloning into &#039;llm-foundry&#039;...</pre></code>\n            <p><i>Using the MCLI command line tool to manage compute clusters, secrets, and submit runs.</i></p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">### mpt-125m-r2.yaml ###\n# set up secrets once with `mcli create secret ...`\n# and they will be present in the environment in any subsequent run\n\nintegrations:\n- integration_type: git_repo\n  git_repo: mosaicml/llm-foundry\n  git_branch: main\n  pip_install: -e .[gpu]\n\nimage: mosaicml/pytorch:1.13.1_cu117-python3.10-ubuntu20.04\n\ncommand: |\n  cd llm-foundry/scripts\n  composer train/train.py train/yamls/mpt/125m.yaml \\\n    data_remote=s3://bucket/path-to-data \\\n    max_duration=100ba \\\n    save_folder=s3://checkpoints/mpt-125m \\\n    save_interval=20ba\n\nrun_name: mpt-125m-r2\n\ngpu_num: 8\ngpu_type: a100_40gb\ncluster: oci-1  # can be any compute cluster!</pre></code>\n            <p><i>An MCLI job template. Specify a run name, a Docker image, a set of commands, and a compute cluster to run on.</i></p>\n          <div class=\"flex anchor relative\">\n            <h3 id=\"get-started-today\">Get started today!</h3>\n            <a href=\"#get-started-today\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n              <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n            </a>\n          </div>\n        <p>The MosaicML platform is an invaluable tool to take your training to the next level, and in this post, we explored how Cloudflare R2 empowers you to train models on your own data, with any compute provider – or all of them. By eliminating egress fees, R2’s storage is an exceptionally cost-effective complement to MosaicML training, providing maximum autonomy and control. With this combination, you can switch between cloud service providers to fit your organization’s needs over time.</p><p>To learn more about using MosaicML to train custom state-of-the-art AI on your own data visit <a href=\"https://www.mosaicml.com/\">here</a> or <a href=\"https://docs.google.com/forms/d/e/1FAIpQLSepW7QB3Xkv6T7GJRwrR9DmGAEjm5G2lBxJC7PUe3JXcBZYbw/viewform\">get in touch</a>.</p>\n          <div class=\"flex anchor relative\">\n            <h3 id=\"watch-on-cloudflare-tv\">Watch on Cloudflare TV</h3>\n            <a href=\"#watch-on-cloudflare-tv\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n              <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n            </a>\n          </div>\n        <!--kg-card-begin: html--><div style=\"position: relative; padding-top: 56.25%;\"><iframe src=\"https://customer-rhnwzxvb3mg4wz3v.cloudflarestream.com/e87f8536439db9b5eea7dfd33ad2f11e/iframe?preload=true&poster=https%3A%2F%2Fcustomer-rhnwzxvb3mg4wz3v.cloudflarestream.com%2Fe87f8536439db9b5eea7dfd33ad2f11e%2Fthumbnails%2Fthumbnail.jpg%3Ftime%3D1s%26height%3D600\" style=\"border: none; position: absolute; top: 0; left: 0; height: 100%; width: 100%;\" allow=\"accelerometer; gyroscope; autoplay; encrypted-media; picture-in-picture;\" allowfullscreen=\"true\"></iframe></div><!--kg-card-end: html--><p></p>",
		"id": "4ETryNsT8L8QFX8tPNzeye",
		"localeList": {
			"name": "Cloudflare R2 and MosaicML enable training LLMs on any compute, anywhere in the world, with zero switching costs Config",
			"enUS": "English for Locale",
			"zhCN": "Translated for Locale",
			"zhHansCN": "No Page for Locale",
			"zhTW": "No Page for Locale",
			"frFR": "Translated for Locale",
			"deDE": "Translated for Locale",
			"itIT": "No Page for Locale",
			"jaJP": "Translated for Locale",
			"koKR": "No Page for Locale",
			"ptBR": "No Page for Locale",
			"esLA": "No Page for Locale",
			"esES": "Translated for Locale",
			"enAU": "No Page for Locale",
			"enCA": "No Page for Locale",
			"enIN": "No Page for Locale",
			"enGB": "No Page for Locale",
			"idID": "No Page for Locale",
			"ruRU": "No Page for Locale",
			"svSE": "No Page for Locale",
			"viVN": "No Page for Locale",
			"plPL": "No Page for Locale",
			"arAR": "No Page for Locale",
			"nlNL": "No Page for Locale",
			"thTH": "No Page for Locale",
			"trTR": "No Page for Locale",
			"heIL": "No Page for Locale",
			"lvLV": "No Page for Locale",
			"etEE": "No Page for Locale",
			"ltLT": "No Page for Locale"
		},
		"meta_description": "Together, Cloudflare and MosaicML give customers the freedom to train LLMs on any compute, anywhere in the world, with zero switching costs. That means faster, cheaper training runs, and no vendor lock in.",
		"metadata": {
			"title": "Cloudflare R2 and MosaicML enable training LLMs on any compute, anywhere in the world, with zero switching costs",
			"description": "Together, Cloudflare and MosaicML give customers the freedom to train LLMs on any compute, anywhere in the world, with zero switching costs. That means faster, cheaper training runs, and no vendor lock in.",
			"imgPreview": "https://cf-assets.www.cloudflare.com/slt3lc6tev37/04InvHphos5l3oQktb3le/25ae2fc2157c52cbe13c1129dbdfbd2d/cloudflare-r2-mosaicml-train-llms-anywhere-faster-cheaper-bU6cJR.png"
		},
		"primary_author": {},
		"published_at": "2023-05-16T14:00:54.000+01:00",
		"slug": "cloudflare-r2-mosaicml-train-llms-anywhere-faster-cheaper",
		"tags": [
			{
				"id": "2xCnBweKwOI3VXdYsGVbMe",
				"name": "Developer Week",
				"slug": "developer-week"
			},
			{
				"id": "4HIPcb68qM0e26fIxyfzwQ",
				"name": "Developers",
				"slug": "developers"
			},
			{
				"id": "V86khSc459Yi1AhTlvtY7",
				"name": "Partners",
				"slug": "partners"
			},
			{
				"id": "7JpaihvGGjNhG2v4nTxeFV",
				"name": "R2 Storage",
				"slug": "cloudflare-r2"
			},
			{
				"id": "6U7dmCbmvXFOiNbJX41ArH",
				"name": "Egress",
				"slug": "egress"
			},
			{
				"id": "5OywGP63AdM9Umyvaku8OP",
				"name": "Connectivity Cloud",
				"slug": "connectivity-cloud"
			}
		],
		"title": "Cloudflare R2 and MosaicML enable training LLMs on any compute, anywhere in the world, with zero switching costs",
		"updated_at": "2024-08-27T01:15:43.638Z",
		"url": "https://blog.cloudflare.com/cloudflare-r2-mosaicml-train-llms-anywhere-faster-cheaper"
	},
	"translations": {
		"posts.by": "By",
		"footer.gdpr": "GDPR",
		"lang_blurb1": "This post is also available in {lang1}.",
		"lang_blurb2": "This post is also available in {lang1} and {lang2}.",
		"lang_blurb3": "This post is also available in {lang1}, {lang2} and {lang3}.",
		"footer.press": "Press",
		"header.title": "The Cloudflare Blog",
		"search.clear": "Clear",
		"search.filter": "Filter",
		"search.source": "Source",
		"footer.careers": "Careers",
		"footer.company": "Company",
		"footer.support": "Support",
		"footer.the_net": "theNet",
		"search.filters": "Filters",
		"footer.our_team": "Our team",
		"footer.webinars": "Webinars",
		"page.more_posts": "More posts",
		"posts.time_read": "{time} min read",
		"search.language": "Language",
		"footer.community": "Community",
		"footer.resources": "Resources",
		"footer.solutions": "Solutions",
		"footer.trademark": "Trademark",
		"header.subscribe": "Subscribe",
		"footer.compliance": "Compliance",
		"footer.free_plans": "Free plans",
		"footer.impact_ESG": "Impact/ESG",
		"posts.follow_on_X": "Follow on X",
		"footer.help_center": "Help center",
		"footer.network_map": "Network Map",
		"header.please_wait": "Please Wait",
		"page.related_posts": "Related posts",
		"search.result_stat": "Results <strong>{search_range}</strong> of <strong>{search_total}</strong> for <strong>{search_keyword}</strong>",
		"footer.case_studies": "Case Studies",
		"footer.connect_2024": "Connect 2024",
		"footer.terms_of_use": "Terms of Use",
		"footer.white_papers": "White Papers",
		"footer.cloudflare_tv": "Cloudflare TV",
		"footer.community_hub": "Community Hub",
		"footer.compare_plans": "Compare plans",
		"footer.contact_sales": "Contact Sales",
		"header.contact_sales": "Contact Sales",
		"header.email_address": "Email Address",
		"page.error.not_found": "Page not found",
		"footer.developer_docs": "Developer docs",
		"footer.privacy_policy": "Privacy Policy",
		"footer.request_a_demo": "Request a demo",
		"page.continue_reading": "Continue reading",
		"footer.analysts_report": "Analyst reports",
		"footer.for_enterprises": "For enterprises",
		"footer.getting_started": "Getting Started",
		"footer.learning_center": "Learning Center",
		"footer.project_galileo": "Project Galileo",
		"pagination.newer_posts": "Newer Posts",
		"pagination.older_posts": "Older Posts",
		"posts.social_buttons.x": "Discuss on X",
		"search.source_location": "Source/Location",
		"footer.about_cloudflare": "About Cloudflare",
		"footer.athenian_project": "Athenian Project",
		"footer.become_a_partner": "Become a partner",
		"footer.cloudflare_radar": "Cloudflare Radar",
		"footer.network_services": "Network services",
		"footer.trust_and_safety": "Trust & Safety",
		"header.get_started_free": "Get Started Free",
		"page.search.placeholder": "Search Cloudflare",
		"footer.cloudflare_status": "Cloudflare Status",
		"footer.cookie_preference": "Cookie Preferences",
		"header.valid_email_error": "Must be valid email.",
		"footer.connectivity_cloud": "Connectivity cloud",
		"footer.developer_services": "Developer services",
		"footer.investor_relations": "Investor relations",
		"page.not_found.error_code": "Error Code: 404",
		"footer.logos_and_press_kit": "Logos & press kit",
		"footer.application_services": "Application services",
		"footer.get_a_recommendation": "Get a recommendation",
		"posts.social_buttons.reddit": "Discuss on Reddit",
		"footer.sse_and_sase_services": "SSE and SASE services",
		"page.not_found.outdated_link": "You may have used an outdated link, or you may have typed the address incorrectly.",
		"footer.report_security_issues": "Report Security Issues",
		"page.error.error_message_page": "Sorry, we can't find the page you are looking for.",
		"header.subscribe_notifications": "Subscribe to receive notifications of new posts:",
		"footer.cloudflare_for_campaigns": "Cloudflare for Campaigns",
		"header.subscription_confimation": "Subscription confirmed. Thank you for subscribing!",
		"posts.social_buttons.hackernews": "Discuss on Hacker News",
		"footer.diversity_equity_inclusion": "Diversity, equity & inclusion",
		"footer.critical_infrastructure_defense_project": "Critical Infrastructure Defense Project"
	}
}