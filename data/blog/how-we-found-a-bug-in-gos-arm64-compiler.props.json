{
	"footerBlurb": "Cloudflare's connectivity cloud protects <a target='_blank' href='https://www.cloudflare.com/network-services/' rel='noreferrer'>entire corporate networks</a>, helps customers build <a target='_blank' href='https://workers.cloudflare.com/' rel='noreferrer'>Internet-scale applications efficiently</a>, accelerates any <a target='_blank' href='https://www.cloudflare.com/performance/accelerate-internet-applications/' rel='noreferrer'>website or Internet application</a>, <a target='_blank' href='https://www.cloudflare.com/ddos/' rel='noreferrer'>wards off DDoS attacks</a>, keeps <a target='_blank' href='https://www.cloudflare.com/application-security/' rel='noreferrer'>hackers at bay</a>, and can help you on <a target='_blank' href='https://www.cloudflare.com/products/zero-trust/' rel='noreferrer'>your journey to Zero Trust</a>.<br/><br/>Visit <a target='_blank' href='https://one.one.one.one/' rel='noreferrer'>1.1.1.1</a> from any device to get started with our free app that makes your Internet faster and safer.<br/><br/>To learn more about our mission to help build a better Internet, <a target='_blank' href='https://www.cloudflare.com/learning/what-is-cloudflare/' rel='noreferrer'>start here</a>. If you&apos;re looking for a new career direction, check out <a target='_blank' href='http://www.cloudflare.com/careers' rel='noreferrer'>our open positions</a>.",
	"initialReadingTime": "10",
	"locale": "en-us",
	"localesAvailable": [],
	"post": {
		"authors": [
			{
				"name": "Thea Heinen",
				"slug": "thea-heinen",
				"profile_image": "https://cf-assets.www.cloudflare.com/zkvhlag99gkb/1EE96CpoAMWY7aI0eTF6Ze/7f1ce70b27bd49ebf197bae9d22253d9/PXL_20240216_211439533.jpg",
				"publiclyIndex": true
			}
		],
		"excerpt": "84 million requests a second means even rare bugs appear often. We'll reveal how we discovered a race condition in the Go arm64 compiler and got it fixed.",
		"feature_image": "https://cf-assets.www.cloudflare.com/zkvhlag99gkb/1zaMttyutjMsqHUuEpCss3/8b7eb27d92e5fc53f6541d04fe0b778f/Bug_Bounty.png",
		"featured": false,
		"html": "<p>Every second, 84 million HTTP requests are hitting Cloudflare across our fleet of data centers in 330 cities. It means that even the rarest of bugs can show up frequently. In fact, it was our scale that recently led us to discover a bug in Go&#39;s arm64 compiler which causes a race condition in the generated code.</p><p>This post breaks down how we first encountered the bug, investigated it, and ultimately drove to the root cause.</p>\n    <div class=\"flex anchor relative\">\n      <h2 id=\"investigating-a-strange-panic\">Investigating a strange panic</h2>\n      <a href=\"#investigating-a-strange-panic\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>We run a service in our network which configures the kernel to handle traffic for some products like <a href=\"https://www.cloudflare.com/network-services/products/magic-transit/\"><u>Magic Transit</u></a> and <a href=\"https://www.cloudflare.com/network-services/products/magic-wan/\"><u>Magic WAN</u></a>. Our monitoring watches this closely, and it started to observe very sporadic panics on arm64 machines.</p><p>We first saw one with a fatal error stating that <a href=\"https://github.com/golang/go/blob/c0ee2fd4e309ef0b8f4ab6f4860e2626c8e00802/src/runtime/traceback.go#L566\"><u>traceback did not unwind completely</u></a>. That error suggests that invariants were violated when traversing the stack, likely because of stack corruption. After a brief investigation we decided that it was probably rare stack memory corruption. This was a largely idle control plane service where unplanned restarts have negligible impact, and so we felt that following up was not a priority unless it kept happening.</p><p>And then it kept happening. </p>\n    <div class=\"flex anchor relative\">\n      <h4 id=\"coredumps-per-hour\">Coredumps per hour</h4>\n      <a href=\"#coredumps-per-hour\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    \n          <figure class=\"kg-card kg-image-card\">\n          <Image src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/7ohMD2ph3I9l7lbZQWZ4xl/243e679d8293565fc6a55a670c9f3583/BLOG-2906_2.png\" alt=\"BLOG-2906 2\" class=\"kg-image\" width=\"1999\" height=\"526\" loading=\"lazy\"/>\n          </figure><p>When we first saw this bug we saw that the fatal errors correlated with recovered panics. These were caused by some old code which used panic/recover as error handling. </p><p>At this point, our theory was: </p><ol><li><p>All of the fatal panics happen within stack unwinding.</p></li><li><p>We correlated an increased volume of recovered panics with these fatal panics.</p></li><li><p>Recovering a panic unwinds goroutine stacks to call deferred functions.</p></li><li><p>A related <a href=\"https://github.com/golang/go/issues/73259\"><u>Go issue (#73259)</u></a> reported an arm64 stack unwinding crash.</p></li><li><p>Let’s stop using panic/recover for error handling and wait out the upstream fix?</p></li></ol><p>So we did that and watched as fatal panics stopped occurring as the release rolled out. Fatal panics gone, our theoretical mitigation seemed to work, and this was no longer our problem. We subscribed to the upstream issue so we could update when it was resolved and put it out of our minds.</p><p>But, this turned out to be a much stranger bug than expected. Putting it out of our minds was premature as the same class of fatal panics came back at a much higher rate. A month later, we were seeing up to 30 daily fatal panics with no real discernible cause; while that might account for only one machine a day in less than 10% of our data centers, we found it concerning that we didn’t understand the cause. The first thing we checked was the number of recovered panics, to match our previous pattern, but there were none. More interestingly, we could not correlate this increased rate of fatal panics with anything. A release? Infrastructure changes? The position of Mars? </p><p>At this point we felt like we needed to dive deeper to better understand the root cause. Pattern matching and hoping was clearly insufficient. </p><p>We saw two classes of this bug -- a crash while accessing invalid memory and an explicitly checked fatal error. </p>\n    <div class=\"flex anchor relative\">\n      <h4 id=\"fatal-error\">Fatal Error</h4>\n      <a href=\"#fatal-error\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    \n            <pre class=\"language-text\"><code class=\"language-text\">goroutine 153 gp=0x4000105340 m=324 mp=0x400639ea08 [GC worker (active)]:\n/usr/local/go/src/runtime/asm_arm64.s:244 +0x6c fp=0x7ff97fffe870 sp=0x7ff97fffe860 pc=0x55558d4098fc\nruntime.systemstack(0x0)\n       /usr/local/go/src/runtime/mgc.go:1508 +0x68 fp=0x7ff97fffe860 sp=0x7ff97fffe810 pc=0x55558d3a9408\nruntime.gcBgMarkWorker.func2()\n       /usr/local/go/src/runtime/mgcmark.go:1102\nruntime.gcDrainMarkWorkerIdle(...)\n       /usr/local/go/src/runtime/mgcmark.go:1188 +0x434 fp=0x7ff97fffe810 sp=0x7ff97fffe7a0 pc=0x55558d3ad514\nruntime.gcDrain(0x400005bc50, 0x7)\n       /usr/local/go/src/runtime/mgcmark.go:212 +0x1c8 fp=0x7ff97fffe7a0 sp=0x7ff97fffe6f0 pc=0x55558d3ab248\nruntime.markroot(0x400005bc50, 0x17e6, 0x1)\n       /usr/local/go/src/runtime/mgcmark.go:238 +0xa8 fp=0x7ff97fffe6f0 sp=0x7ff97fffe6a0 pc=0x55558d3ab578\nruntime.markroot.func1()\n       /usr/local/go/src/runtime/mgcmark.go:887 +0x290 fp=0x7ff97fffe6a0 sp=0x7ff97fffe560 pc=0x55558d3acaa0\nruntime.scanstack(0x4014494380, 0x400005bc50)\n       /usr/local/go/src/runtime/traceback.go:447 +0x2ac fp=0x7ff97fffe560 sp=0x7ff97fffe4d0 pc=0x55558d3eeb7c\nruntime.(*unwinder).next(0x7ff97fffe5b0?)\n       /usr/local/go/src/runtime/traceback.go:566 +0x110 fp=0x7ff97fffe4d0 sp=0x7ff97fffe490 pc=0x55558d3eed40\nruntime.(*unwinder).finishInternal(0x7ff97fffe4f8?)\n       /usr/local/go/src/runtime/panic.go:1073 +0x38 fp=0x7ff97fffe490 sp=0x7ff97fffe460 pc=0x55558d403388\nruntime.throw({0x55558de6aa27?, 0x7ff97fffe638?})\nruntime stack:\nfatal error: traceback did not unwind completely\n       stack=[0x4015d6a000-0x4015d8a000\nruntime: g8221077: frame.sp=0x4015d784c0 top=0x4015d89fd0</pre></code>\n            <p></p>\n    <div class=\"flex anchor relative\">\n      <h4 id=\"segmentation-fault\">Segmentation fault</h4>\n      <a href=\"#segmentation-fault\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    \n            <pre class=\"language-TEXT\"><code class=\"language-TEXT\">goroutine 187 gp=0x40003aea80 m=13 mp=0x40003ca008 [GC worker (active)]:\n       /usr/local/go/src/runtime/asm_arm64.s:244 +0x6c fp=0x7fff2afde870 sp=0x7fff2afde860 pc=0x55557e2d98fc\nruntime.systemstack(0x0)\n       /usr/local/go/src/runtime/mgc.go:1489 +0x94 fp=0x7fff2afde860 sp=0x7fff2afde810 pc=0x55557e279434\nruntime.gcBgMarkWorker.func2()\n       /usr/local/go/src/runtime/mgcmark.go:1112\nruntime.gcDrainMarkWorkerDedicated(...)\n       /usr/local/go/src/runtime/mgcmark.go:1188 +0x434 fp=0x7fff2afde810 sp=0x7fff2afde7a0 pc=0x55557e27d514\nruntime.gcDrain(0x4000059750, 0x3)\n       /usr/local/go/src/runtime/mgcmark.go:212 +0x1c8 fp=0x7fff2afde7a0 sp=0x7fff2afde6f0 pc=0x55557e27b248\nruntime.markroot(0x4000059750, 0xb8, 0x1)\n       /usr/local/go/src/runtime/mgcmark.go:238 +0xa8 fp=0x7fff2afde6f0 sp=0x7fff2afde6a0 pc=0x55557e27b578\nruntime.markroot.func1()\n       /usr/local/go/src/runtime/mgcmark.go:887 +0x290 fp=0x7fff2afde6a0 sp=0x7fff2afde560 pc=0x55557e27caa0\nruntime.scanstack(0x40042cc000, 0x4000059750)\n       /usr/local/go/src/runtime/traceback.go:458 +0x188 fp=0x7fff2afde560 sp=0x7fff2afde4d0 pc=0x55557e2bea58\nruntime.(*unwinder).next(0x7fff2afde5b0)\ngoroutine 0 gp=0x40003af880 m=13 mp=0x40003ca008 [idle]:\nPC=0x55557e2bea58 m=13 sigcode=1 addr=0x118\nSIGSEGV: segmentation violation</pre></code>\n            <p></p><p></p><p>Now we could observe some clear patterns. Both errors occur when unwinding the stack in <code>(*unwinder).next</code>. In one case we saw an intentional<a href=\"https://github.com/golang/go/blob/b3251514531123d7fd007682389bce7428d159a0/src/runtime/traceback.go#L566\"> <u>fatal error</u></a> as the runtime identified that unwinding could not complete and the stack was in a bad state. In the other case there was a direct memory access error that happened while trying to unwind the stack. The segfault was discussed in the <a href=\"https://github.com/golang/go/issues/73259#issuecomment-2786818812\"><u>GitHub issue</u></a> and a Go engineer identified it as dereference of a go scheduler struct, <a href=\"https://github.com/golang/go/blob/924fe98902cdebf20825ab5d1e4edfc0fed2966f/src/runtime/runtime2.go#L536\"><u>m</u></a>, when<a href=\"https://github.com/golang/go/blob/b3251514531123d7fd007682389bce7428d159a0/src/runtime/traceback.go#L458\"> <u>unwinding</u></a>.   </p>\n    <div class=\"flex anchor relative\">\n      <h3 id=\"a-review-of-go-scheduler-structs\">A review of Go scheduler structs</h3>\n      <a href=\"#a-review-of-go-scheduler-structs\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>Go uses a lightweight userspace scheduler to manage concurrency. Many goroutines are scheduled on a smaller number of kernel threads – this is often referred to as M:N scheduling. Any individual goroutine can be scheduled on any kernel thread. The scheduler has three core types – <a href=\"https://github.com/golang/go/blob/924fe98902cdebf20825ab5d1e4edfc0fed2966f/src/runtime/runtime2.go#L394\"><code><u>g</u></code></a>  (the goroutine), <a href=\"https://github.com/golang/go/blob/924fe98902cdebf20825ab5d1e4edfc0fed2966f/src/runtime/runtime2.go#L536\"><code><u>m</u></code></a> (the kernel thread, or “machine”), and <a href=\"https://github.com/golang/go/blob/924fe98902cdebf20825ab5d1e4edfc0fed2966f/src/runtime/runtime2.go#L644\"><code><u>p</u></code></a> (the physical execution context, or  “processor”). For a goroutine to be scheduled a free <code>m</code> must acquire a free <code>p</code>, which will execute a g. Each <code>g</code> contains a field for its m if it is currently running, otherwise it will be <b>nil</b>. This is all the context needed for this post but the <a href=\"https://github.com/golang/go/blob/master/src/runtime/HACKING.md#gs-ms-ps\"><u>go runtime docs</u></a> explore this more comprehensively. </p><p>At this point we can start to make inferences on what’s happening: the program crashes because we try to unwind a goroutine stack which is invalid. In the first backtrace, if a <a href=\"https://github.com/golang/go/blob/b3251514531123d7fd007682389bce7428d159a0/src/runtime/traceback.go#L446\"><u>return address is null, we call </u><code><u>finishInternal</u></code><u> and abort because the stack was not fully unwound</u></a>. The segmentation fault case in the second backtrace is a bit more interesting: if instead the return address is non-zero but not a function then the unwinder code assumes that the goroutine is currently running. It&#39;ll then dereference m and fault by accessing <code>m.incgo</code> (the offset of <code>incgo</code> into <code>struct m</code> is 0x118, the faulting memory access). </p><p>What, then, is causing this corruption? The traces were difficult to get anything useful from – our service has hundreds if not thousands of active goroutines. It was fairly clear from the beginning that the panic was remote from the actual bug. The crashes were all observed while unwinding the stack and if this were an issue any time the stack was unwound on arm64 we would be seeing it in many more services. We felt pretty confident that the stack unwinding was happening correctly but on an invalid stack. </p><p>Our investigation stalled for a while at this point – making guesses, testing guesses, trying to infer if the panic rate went up or down, or if nothing changed. There was <a href=\"https://github.com/golang/go/issues/73259\"><u>a known issue</u></a> on Go’s GitHub issue tracker which matched our symptoms almost exactly, but what they discussed was mostly what we already knew. At some point when looking through the linked stack traces we realized that their crash referenced an old version of a library that we were also using – Go Netlink.</p>\n            <pre class=\"language-text\"><code class=\"language-text\">goroutine 1267 gp=0x4002a8ea80 m=nil [runnable (scan)]:\nruntime.asyncPreempt2()\n        /usr/local/go/src/runtime/preempt.go:308 +0x3c fp=0x4004cec4c0 sp=0x4004cec4a0 pc=0x46353c\nruntime.asyncPreempt()\n        /usr/local/go/src/runtime/preempt_arm64.s:47 +0x9c fp=0x4004cec6b0 sp=0x4004cec4c0 pc=0x4a6a8c\ngithub.com/vishvananda/netlink/nl.(*NetlinkSocket).Receive(0x14360300000000?)\n        /go/pkg/mod/github.com/!data!dog/netlink@v1.0.1-0.20240223195320-c7a4f832a3d1/nl/nl_linux.go:803 +0x130 fp=0x4004cfc710 sp=0x4004cec6c0 pc=0xf95de0\n</pre></code>\n            <p></p><p>We spot-checked a few stack traces and confirmed the presence of this Netlink library. Querying our logs showed that not only did we share a library – every single segmentation fault we observed had happened while preempting<a href=\"https://github.com/vishvananda/netlink/blob/e1e260214862392fb28ff72c9b11adc84df73e2c/nl/nl_linux.go#L880\"> <code><u>NetlinkSocket.Receive</u></code></a>.</p>\n    <div class=\"flex anchor relative\">\n      <h3 id=\"whats-async-preemption\">What’s (async) preemption?</h3>\n      <a href=\"#whats-async-preemption\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>In the prehistoric era of Go (&lt;=1.13) the runtime was cooperatively scheduled. A goroutine would run until it decided it was ready to yield to the scheduler – usually due to explicit calls to <code>runtime.Gosched()</code> or injected yield points at function calls/IO operations. Since<a href=\"https://go.dev/doc/go1.14#runtime\"> <u>Go 1.14</u></a> the runtime instead does async preemption. The Go runtime has a thread <code>sysmon</code> which tracks the runtime of goroutines and will preempt any that run for longer than 10ms (at time of writing). It does this by sending <code>SIGURG</code> to the OS thread and in the signal handler will modify the program counter and stack to mimic a call to <code>asyncPreempt</code>.</p><p>At this point we had two broad theories:</p><ul><li><p>This is a Go Netlink bug – likely due to <code>unsafe.Pointer</code> usage which invoked undefined behavior but is only actually broken on arm64</p></li><li><p>This is a Go runtime bug and we&#39;re only triggering it in <code>NetlinkSocket.Receive</code> for some reason</p></li></ul><p>After finding the same bug publicly reported upstream, we were feeling confident this was caused by a Go runtime bug. However, upon seeing that both issues implicated the same function, we felt more skeptical – notably the Go Netlink library uses unsafe.Pointer so memory corruption was a plausible explanation even if we didn&#39;t understand why.</p><p>After an unsuccessful code audit we had hit a wall. The crashes were rare and remote from the root cause. Maybe these crashes were caused by a runtime bug, maybe they were caused by a Go Netlink bug. It seemed clear that there was something wrong with this area of the code, but code auditing wasn’t going anywhere. </p>\n    <div class=\"flex anchor relative\">\n      <h2 id=\"breakthrough\">Breakthrough</h2>\n      <a href=\"#breakthrough\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>At this point we had a fairly good understanding of what was crashing but very little understanding of <b>why</b> it was happening. It was clear that the root cause of the stack unwinder crashing was remote from the actual crash, and that it had to do with <code>(*NetlinkSocket).Receive</code>, but why? We were able to capture a <b>coredump</b> of a production crash and view it in a debugger. The backtrace confirmed what we already knew – that there was a segmentation fault when unwinding a stack. The crux of the issue revealed itself when we looked at the goroutine which had been preempted while calling <code>(*NetlinkSocket).Receive</code>.     </p>\n            <pre class=\"language-text\"><code class=\"language-text\">(dlv) bt\n0  0x0000555577579dec in runtime.asyncPreempt2\n   at /usr/local/go/src/runtime/preempt.go:306\n1  0x00005555775bc94c in runtime.asyncPreempt\n   at /usr/local/go/src/runtime/preempt_arm64.s:47\n2  0x0000555577cb2880 in github.com/vishvananda/netlink/nl.(*NetlinkSocket).Receive\n   at\n/vendor/github.com/vishvananda/netlink/nl/nl_linux.go:779\n3  0x0000555577cb19a8 in github.com/vishvananda/netlink/nl.(*NetlinkRequest).Execute\n   at \n/vendor/github.com/vishvananda/netlink/nl/nl_linux.go:532\n4  0x0000555577551124 in runtime.heapSetType\n   at /usr/local/go/src/runtime/mbitmap.go:714\n5  0x0000555577551124 in runtime.heapSetType\n   at /usr/local/go/src/runtime/mbitmap.go:714\n...\n(dlv) disass -a 0x555577cb2878 0x555577cb2888\nTEXT github.com/vishvananda/netlink/nl.(*NetlinkSocket).Receive(SB) /vendor/github.com/vishvananda/netlink/nl/nl_linux.go\n        nl_linux.go:779 0x555577cb2878  fdfb7fa9        LDP -8(RSP), (R29, R30)\n        nl_linux.go:779 0x555577cb287c  ff430191        ADD $80, RSP, RSP\n        nl_linux.go:779 0x555577cb2880  ff434091        ADD $(16&lt;&lt;12), RSP, RSP\n        nl_linux.go:779 0x555577cb2884  c0035fd6        RET\n</pre></code>\n            <p></p><p>The goroutine was paused between two opcodes in the function epilogue. Since the process of unwinding a stack relies on the stack frame being in a consistent state, it felt immediately suspicious that we preempted in the middle of adjusting the stack pointer. The goroutine had been paused at 0x555577cb2880, between<code> ADD $80, RSP, RSP and ADD $(16&lt;&lt;12), RSP, RSP</code>. </p><p>We queried the service logs to confirm our theory. This wasn’t isolated – the majority of stack traces showed that this same opcode was preempted. This was no longer a weird production crash we couldn’t reproduce. A crash happened when the Go runtime preempted between these two stack pointer adjustments. We had our smoking gun. </p>\n    <div class=\"flex anchor relative\">\n      <h2 id=\"building-a-minimal-reproducer\">Building a minimal reproducer</h2>\n      <a href=\"#building-a-minimal-reproducer\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>At this point we felt pretty confident that this was actually just a runtime bug and it should be reproducible in an isolated environment without any dependencies. The theory at this point was:</p><ol><li><p>Stack unwinding is triggered by garbage collection</p></li><li><p>Async preemption between a split stack pointer adjustment causes a crash</p></li><li><p>What if we make a function which splits the adjustment and then call it in a loop?</p></li></ol>\n            <pre class=\"language-Go\"><code class=\"language-Go\">package main\n\nimport (\n\t&quot;runtime&quot;\n)\n\n//go:noinline\nfunc big_stack(val int) int {\n\tvar big_buffer = make([]byte, 1 &lt;&lt; 16)\n\n\tsum := 0\n\t// prevent the compiler from optimizing out the stack\n\tfor i := 0; i &lt; (1&lt;&lt;16); i++ {\n\t\tbig_buffer[i] = byte(val)\n\t}\n\tfor i := 0; i &lt; (1&lt;&lt;16); i++ {\n\t\tsum ^= int(big_buffer[i])\n\t}\n\treturn sum\n}\n\nfunc main() {\n\tgo func() {\n\t\tfor {\n\t\t\truntime.GC()\n\t\t}\n\t}()\n\tfor {\n\t\t_ = big_stack(1000)\n\t}\n}\n</pre></code>\n            <p></p><p>This function ends up with a stack frame slightly larger than can be represented in 16 bits, and so on arm64 the Go compiler will split the stack pointer adjustment into two opcodes. If the runtime preempts between these opcodes then the stack unwinder will read an invalid stack pointer and crash. </p>\n            <pre class=\"language-text\"><code class=\"language-text\">; epilogue for main.big_stack\nADD $8, RSP, R29\nADD $(16&lt;&lt;12), R29, R29\nADD $16, RSP, RSP\n; preemption is problematic between these opcodes\nADD $(16&lt;&lt;12), RSP, RSP\nRET\n</pre></code>\n            <p></p><p>After running this for a few minutes the program panicked as expected!</p>\n            <pre class=\"language-text\"><code class=\"language-text\">SIGSEGV: segmentation violation\nPC=0x60598 m=8 sigcode=1 addr=0x118\n\ngoroutine 0 gp=0x400019c540 m=8 mp=0x4000198708 [idle]:\nruntime.(*unwinder).next(0x400030fd10)\n        /home/thea/sdk/go1.23.4/src/runtime/traceback.go:458 +0x188 fp=0x400030fcc0 sp=0x400030fc30 pc=0x60598\nruntime.scanstack(0x40000021c0, 0x400002f750)\n        /home/thea/sdk/go1.23.4/src/runtime/mgcmark.go:887 +0x290 \n\n[...]\n\ngoroutine 1 gp=0x40000021c0 m=nil [runnable (scan)]:\nruntime.asyncPreempt2()\n        /home/thea/sdk/go1.23.4/src/runtime/preempt.go:308 +0x3c fp=0x40003bfcf0 sp=0x40003bfcd0 pc=0x400cc\nruntime.asyncPreempt()\n        /home/thea/sdk/go1.23.4/src/runtime/preempt_arm64.s:47 +0x9c fp=0x40003bfee0 sp=0x40003bfcf0 pc=0x75aec\nmain.big_stack(0x40003cff38?)\n        /home/thea/dev/stack_corruption_reproducer/main.go:29 +0x94 fp=0x40003cff00 sp=0x40003bfef0 pc=0x77c04\nSegmentation fault (core dumped)\n\nreal    1m29.165s\nuser    4m4.987s\nsys     0m43.212s</pre></code>\n            <p></p><p>A reproducible crash with standard library only? This felt like conclusive evidence that our problem was a runtime bug.</p><p>This was an extremely particular reproducer! Even now with a good understanding of the bug and its fix, some of the behavior is still puzzling. It&#39;s a one-instruction race condition, so it’s unsurprising that small changes could have large impact. For example, this reproducer was originally written and tested on Go 1.23.4, but did not crash when compiled with 1.23.9 (the version in production), even though we could objdump the binary and see the split ADD still present! We don’t have a definite explanation for this behavior – even with the bug present there remain a few unknown variables which affect the likelihood of hitting the race condition. </p>\n    <div class=\"flex anchor relative\">\n      <h2 id=\"a-single-instruction-race-condition-window\">A single-instruction race condition window</h2>\n      <a href=\"#a-single-instruction-race-condition-window\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>arm64 is a fixed-length 4-byte instruction set architecture. This has a lot of implications on codegen but most relevant to this bug is the fact that immediate length is limited.<a href=\"https://developer.arm.com/documentation/ddi0596/2020-12/Base-Instructions/ADD--immediate---Add--immediate--\"> <code><u>add</u></code></a> gets a 12-bit immediate,<a href=\"https://developer.arm.com/documentation/dui0802/a/A64-General-Instructions/MOV--wide-immediate-\"> <code><u>mov</u></code></a> gets a 16-bit immediate, etc. How does the architecture handle this when the operands don&#39;t fit? It depends – <code>ADD</code> in particular reserves a bit for &quot;shift left by 12&quot; so any 24 bit addition can be decomposed into two opcodes. Other instructions are decomposed similarly, or just require loading an immediate into a register first. </p><p>The very last step of the Go compiler before emitting machine code involves transforming the program into <code>obj.Prog</code> structs. It&#39;s a very low level intermediate representation (IR) that mostly serves to be translated into machine code. </p>\n            <pre class=\"language-Go\"><code class=\"language-Go\">//https://github.com/golang/go/blob/fa2bb342d7b0024440d996c2d6d6778b7a5e0247/src/cmd/internal/obj/arm64/obj7.go#L856\n\n// Pop stack frame.\n// ADD $framesize, RSP, RSP\np = obj.Appendp(p, c.newprog)\np.As = AADD\np.From.Type = obj.TYPE_CONST\np.From.Offset = int64(c.autosize)\np.To.Type = obj.TYPE_REG\np.To.Reg = REGSP\np.Spadj = -c.autosize\n</pre></code>\n            <p></p><p>Notably, this IR is not aware of immediate length limitations. Instead, this happens in<a href=\"https://github.com/golang/go/blob/2f653a5a9e9112ff64f1392ff6e1d404aaf23e8c/src/cmd/internal/obj/arm64/asm7.go\"> <u>asm7.go</u></a> when Go&#39;s internal intermediate representation is translated into arm64 machine code. The assembler will classify an immediate in <a href=\"https://github.com/golang/go/blob/2f653a5a9e9112ff64f1392ff6e1d404aaf23e8c/src/cmd/internal/obj/arm64/asm7.go#L1905\"><u>conclass</u></a> based on bit size and then use that when emitting instructions – extra if needed.</p><p>The Go assembler uses a combination of (<code>mov, add</code>) opcodes for some adds that fit in 16-bit immediates, and prefers (<code>add, add + lsl 12</code>) opcodes for 16-bit+ immediates.   </p><p>Compare a stack of (slightly larger than) <code>1&lt;&lt;15</code>:</p>\n            <pre class=\"language-text\"><code class=\"language-text\">; //go:noinline\n; func big_stack() byte {\n; \tvar big_stack = make([]byte, 1&lt;&lt;15)\n; \treturn big_stack[0]\n; }\nMOVD $32776, R27\nADD R27, RSP, R29\nMOVD $32784, R27\nADD R27, RSP, RSP\nRET\n</pre></code>\n            <p></p><p>With a stack of <code>1&lt;&lt;16</code>:</p>\n            <pre class=\"language-text\"><code class=\"language-text\">; //go:noinline\n; func big_stack() byte {\n; \tvar big_stack = make([]byte, 1&lt;&lt;16)\n; \treturn big_stack[0]\n; } \nADD $8, RSP, R29\nADD $(16&lt;&lt;12), R29, R29\nADD $16, RSP, RSP\nADD $(16&lt;&lt;12), RSP, RSP\nRET\n</pre></code>\n            <p>In the larger stack case, there is a point between <code>ADD x, RSP, RSP</code> opcodes where the stack pointer is not pointing to the tip of a stack frame. We thought at first that this was a matter of memory corruption – that in handling async preemption the runtime would push a function call on the stack and corrupt the middle of the stack. However, this goroutine is already in the function epilogue – any data we corrupt is actively in the process of being thrown away. What&#39;s the issue then?  </p><p>The Go runtime often needs to <b>unwind</b> the stack, which means walking backwards through the chain of function calls. For example: garbage collection uses it to find live references on the stack, panicking relies on it to evaluate <code>defer</code> functions, and generating stack traces needs to print the call stack. For this to work the stack pointer <b>must be accurate during unwinding</b> because of how golang dereferences sp to determine the calling function. If the stack pointer is partially modified, the unwinder will look for the calling function in the middle of the stack. The underlying data is meaningless when interpreted as directions to a parent stack frame and then the runtime will likely crash. </p>\n            <pre class=\"language-Go\"><code class=\"language-Go\">//https://github.com/golang/go/blob/66536242fce34787230c42078a7bbd373ef8dcb0/src/runtime/traceback.go#L373\n\nif innermost &amp;&amp; frame.sp &lt; frame.fp || frame.lr == 0 {\n    lrPtr = frame.sp\n    frame.lr = *(*uintptr)(unsafe.Pointer(lrPtr))\n}\n</pre></code>\n            <p></p><p>When async preemption happens it will push a function call onto the stack but the parent stack frame is no longer correct because sp was only partially adjusted when the preemption happened. The crash flow looks something like this:  </p><ol><li><p>Async preemption happens between the two opcodes that <code>add x, rsp</code> expands to</p></li><li><p>Garbage collection triggers stack unwinding (to check for heap object liveness)</p></li><li><p>The unwinder starts traversing the stack of the problematic goroutine and correctly unwinds up to the problematic function</p></li><li><p>The unwinder dereferences <code>sp</code> to determine the parent function</p></li><li><p>Almost certainly the data behind <code>sp</code> is not a function</p></li><li><p>Crash</p></li></ol>\n          <figure class=\"kg-card kg-image-card\">\n          <Image src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/5LKOBC6lQzuKvwy0vyEfk2/054c9aaedc14d155294a682f7de3a610/BLOG-2906_3.png\" alt=\"BLOG-2906 3\" class=\"kg-image\" width=\"784\" height=\"491\" loading=\"lazy\"/>\n          </figure><p>We saw earlier a faulting stack trace which ended in <code>(*NetlinkSocket).Receive</code> – in this case stack unwinding faulted while it was trying to determine the parent frame.    </p>\n            <pre class=\"language-text\"><code class=\"language-text\">goroutine 90 gp=0x40042cc000 m=nil [preempted (scan)]:\nruntime.asyncPreempt2()\n/usr/local/go/src/runtime/preempt.go:306 +0x2c fp=0x40060a25d0 sp=0x40060a25b0 pc=0x55557e299dec\nruntime.asyncPreempt()\n/usr/local/go/src/runtime/preempt_arm64.s:47 +0x9c fp=0x40060a27c0 sp=0x40060a25d0 pc=0x55557e2dc94c\ngithub.com/vishvananda/netlink/nl.(*NetlinkSocket).Receive(0xff48ce6e060b2848?)\n/vendor/github.com/vishvananda/netlink/nl/nl_linux.go:779 +0x130 fp=0x40060b2820 sp=0x40060a27d0 pc=0x55557e9d2880\n</pre></code>\n            <p></p><p>Once we discovered the root cause we reported it with a reproducer and the bug was quickly fixed. This bug is fixed in <a href=\"https://github.com/golang/go/commit/e8794e650e05fad07a33fb6e3266a9e677d13fa8\"><u>go1.23.12</u></a>, <a href=\"https://github.com/golang/go/commit/6e1c4529e4e00ab58572deceab74cc4057e6f0b6\"><u>go1.24.6</u></a>, and <a href=\"https://github.com/golang/go/commit/f7cc61e7d7f77521e073137c6045ba73f66ef902\"><u>go1.25.0</u></a>. Previously, the go compiler emitted a single <code>add x, rsp</code> instruction and relied on the assembler to split immediates into multiple opcodes as necessary. After this change, stacks larger than 1&lt;&lt;12 will build the offset in a temporary register and then add that to <code>rsp</code> in a single, indivisible opcode. A goroutine can be preempted before or after the stack pointer modification, but never during. This means that the stack pointer is always valid and there is no race condition.</p>\n            <pre class=\"language-text\"><code class=\"language-text\">LDP -8(RSP), (R29, R30)\nMOVD $32, R27\nMOVK $(1&lt;&lt;16), R27\nADD R27, RSP, RSP\nRET</pre></code>\n            <p></p><p>This was a very fun problem to debug. We don’t often see bugs where you can accurately blame the compiler. Debugging it took weeks and we had to learn about areas of the Go runtime that people don’t usually need to think about. It’s a nice example of a rare race condition, the sort of bug that can only really be quantified at a large scale.</p><p>We’re always looking for people who enjoy this kind of detective work. <a href=\"https://www.cloudflare.com/careers/jobs/?department=Engineering\"><u>Our engineering teams are hiring</u></a>.   </p>",
		"id": "12E3V053vhNrZrU5I2AAV1",
		"localeList": {
			"name": "blog-english-only",
			"enUS": "English for Locale",
			"zhCN": "No Page for Locale",
			"zhHansCN": "No Page for Locale",
			"zhTW": "No Page for Locale",
			"frFR": "No Page for Locale",
			"deDE": "No Page for Locale",
			"itIT": "No Page for Locale",
			"jaJP": "No Page for Locale",
			"koKR": "No Page for Locale",
			"ptBR": "No Page for Locale",
			"esLA": "No Page for Locale",
			"esES": "No Page for Locale",
			"enAU": "No Page for Locale",
			"enCA": "No Page for Locale",
			"enIN": "No Page for Locale",
			"enGB": "No Page for Locale",
			"idID": "No Page for Locale",
			"ruRU": "No Page for Locale",
			"svSE": "No Page for Locale",
			"viVN": "No Page for Locale",
			"plPL": "No Page for Locale",
			"arAR": "No Page for Locale",
			"nlNL": "No Page for Locale",
			"thTH": "No Page for Locale",
			"trTR": "No Page for Locale",
			"heIL": "No Page for Locale",
			"lvLV": "No Page for Locale",
			"etEE": "No Page for Locale",
			"ltLT": "No Page for Locale"
		},
		"meta_description": "84 million requests a second means even rare bugs appear often. We'll reveal how we discovered a race condition in the Go arm64 compiler and got it fixed.",
		"metadata": {
			"title": "How we found a bug in Go's arm64 compiler",
			"description": "84 million requests a second means even rare bugs appear often. We'll reveal how we discovered a race condition in the Go arm64 compiler and got it fixed.",
			"imgPreview": "https://cf-assets.www.cloudflare.com/zkvhlag99gkb/45p8HVRfk35dfKi2bmrTXs/545379fb250a4c31bf69677951efe584/BLOG-2906_OG.png"
		},
		"primary_author": {},
		"publicly_index": true,
		"published_at": "2025-10-08T14:00+00:00",
		"slug": "how-we-found-a-bug-in-gos-arm64-compiler",
		"tags": [
			{
				"id": "2UVIYusJwlvsmPYl2AvSuR",
				"name": "Deep Dive",
				"slug": "deep-dive"
			},
			{
				"id": "KDI5hQcs301H8vxpGKXO0",
				"name": "Go",
				"slug": "go"
			},
			{
				"id": "6lhzEBz2B56RKa4nUEAGYJ",
				"name": "Programming",
				"slug": "programming"
			}
		],
		"title": "How we found a bug in Go's arm64 compiler",
		"updated_at": "2025-10-15T12:32:22.529Z",
		"url": "https://blog.cloudflare.com/how-we-found-a-bug-in-gos-arm64-compiler"
	},
	"translations": {
		"posts.by": "By",
		"footer.gdpr": "GDPR",
		"lang_blurb1": "This post is also available in {lang1}.",
		"lang_blurb2": "This post is also available in {lang1} and {lang2}.",
		"lang_blurb3": "This post is also available in {lang1}, {lang2} and {lang3}.",
		"footer.press": "Press",
		"header.title": "The Cloudflare Blog",
		"search.clear": "Clear",
		"search.filter": "Filter",
		"search.source": "Source",
		"footer.careers": "Careers",
		"footer.company": "Company",
		"footer.support": "Support",
		"footer.the_net": "theNet",
		"search.filters": "Filters",
		"footer.our_team": "Our team",
		"footer.webinars": "Webinars",
		"page.more_posts": "More posts",
		"posts.time_read": "{time} min read",
		"search.language": "Language",
		"footer.community": "Community",
		"footer.resources": "Resources",
		"footer.solutions": "Solutions",
		"footer.trademark": "Trademark",
		"header.subscribe": "Subscribe",
		"footer.compliance": "Compliance",
		"footer.free_plans": "Free plans",
		"footer.impact_ESG": "Impact/ESG",
		"posts.follow_on_X": "Follow on X",
		"footer.help_center": "Help center",
		"footer.network_map": "Network Map",
		"header.please_wait": "Please Wait",
		"page.related_posts": "Related posts",
		"search.result_stat": "Results <strong>{search_range}</strong> of <strong>{search_total}</strong> for <strong>{search_keyword}</strong>",
		"footer.case_studies": "Case Studies",
		"footer.connect_2024": "Connect 2024",
		"footer.terms_of_use": "Terms of Use",
		"footer.white_papers": "White Papers",
		"footer.cloudflare_tv": "Cloudflare TV",
		"footer.community_hub": "Community Hub",
		"footer.compare_plans": "Compare plans",
		"footer.contact_sales": "Contact Sales",
		"header.contact_sales": "Contact Sales",
		"header.email_address": "Email Address",
		"page.error.not_found": "Page not found",
		"footer.developer_docs": "Developer docs",
		"footer.privacy_policy": "Privacy Policy",
		"footer.request_a_demo": "Request a demo",
		"page.continue_reading": "Continue reading",
		"footer.analysts_report": "Analyst reports",
		"footer.for_enterprises": "For enterprises",
		"footer.getting_started": "Getting Started",
		"footer.learning_center": "Learning Center",
		"footer.project_galileo": "Project Galileo",
		"pagination.newer_posts": "Newer Posts",
		"pagination.older_posts": "Older Posts",
		"posts.social_buttons.x": "Discuss on X",
		"search.icon_aria_label": "Search",
		"search.source_location": "Source/Location",
		"footer.about_cloudflare": "About Cloudflare",
		"footer.athenian_project": "Athenian Project",
		"footer.become_a_partner": "Become a partner",
		"footer.cloudflare_radar": "Cloudflare Radar",
		"footer.network_services": "Network services",
		"footer.trust_and_safety": "Trust & Safety",
		"header.get_started_free": "Get Started Free",
		"page.search.placeholder": "Search Cloudflare",
		"footer.cloudflare_status": "Cloudflare Status",
		"footer.cookie_preference": "Cookie Preferences",
		"header.valid_email_error": "Must be valid email.",
		"search.result_stat_empty": "Results <strong>{search_range}</strong> of <strong>{search_total}</strong>",
		"footer.connectivity_cloud": "Connectivity cloud",
		"footer.developer_services": "Developer services",
		"footer.investor_relations": "Investor relations",
		"page.not_found.error_code": "Error Code: 404",
		"search.autocomplete_title": "Insert a query. Press enter to send",
		"footer.logos_and_press_kit": "Logos & press kit",
		"footer.application_services": "Application services",
		"footer.get_a_recommendation": "Get a recommendation",
		"posts.social_buttons.reddit": "Discuss on Reddit",
		"footer.sse_and_sase_services": "SSE and SASE services",
		"page.not_found.outdated_link": "You may have used an outdated link, or you may have typed the address incorrectly.",
		"footer.report_security_issues": "Report Security Issues",
		"page.error.error_message_page": "Sorry, we can't find the page you are looking for.",
		"header.subscribe_notifications": "Subscribe to receive notifications of new posts:",
		"footer.cloudflare_for_campaigns": "Cloudflare for Campaigns",
		"header.subscription_confimation": "Subscription confirmed. Thank you for subscribing!",
		"posts.social_buttons.hackernews": "Discuss on Hacker News",
		"footer.diversity_equity_inclusion": "Diversity, equity & inclusion",
		"footer.critical_infrastructure_defense_project": "Critical Infrastructure Defense Project"
	}
}