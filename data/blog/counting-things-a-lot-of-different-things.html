<div class="post-content lh-copy gray1">
	<!--kg-card-begin: markdown-->
	<p>Back in April we announced <a href="https://blog.cloudflare.com/rate-limiting/">Rate Limiting</a> of requests for every Cloudflare customer. Being able to rate limit at the edge of the network has many advantages: it’s easier for customers to set up and operate, their origin servers are not bothered by excessive traffic or layer 7 attacks, the performance and memory cost of rate limiting is offloaded to the edge, and more.</p>
	<p>In a nutshell, rate limiting works like this:</p>
	<ul>
		<li>
			<p>Customers can define one or more rate limit rules that match particular HTTP requests (failed login attempts, expensive API calls, etc.)</p>
		</li>
		<li>
			<p>Every request that matches the rule is counted per client IP address</p>
		</li>
		<li>
			<p>Once that counter exceeds a threshold, further requests are not allowed to reach the origin server and an error page is returned to the client instead</p>
		</li>
	</ul>
	<p>This is a simple yet effective protection against brute force attacks on login pages and other sorts of abusive traffic like L7 DoS attacks.</p>
	<p>Doing this with possibly millions of domains and even more millions of rules immediately becomes a bit more complicated. This article is a look at how we implemented a rate limiter able to run quickly and accurately at the edge of the network which is able to cope with the colossal volume of traffic we see at Cloudflare.</p>
	<h3 id="letsjustdothislocally">Let’s just do this locally!</h3>
	<p>As the Cloudflare edge servers are running NGINX, let’s first see how the stock <a href="https://nginx.org/en/docs/http/ngx_http_limit_req_module.html" target="_blank">rate limiting</a> module works:</p>
	<pre><code>http {
    limit_req_zone $binary_remote_addr zone=ratelimitzone:10m rate=15r/m;
    ...
    server {
        ...
        location /api/expensive_endpoint {
            limit_req zone=ratelimitzone;
        }
    }
}
</code></pre>
	<p>This module works great: it is reasonably simple to use (but requires a config reload for each change), and very efficient. The only problem is that if the incoming requests are spread across a large number of servers, this doesn’t work any more. The obvious alternative is to use some kind of centralized data store. Thanks to NGINX’s Lua scripting module, that we already use extensively, we could easily implement similar logic using any kind of central data backend.</p>
	<p>But then another problem arises: how to make this fast and efficient?</p>
	<h3 id="allroadsleadtoromenotwithanycast">All roads lead to Rome? Not with anycast!</h3>
	<p>Since Cloudflare has a vast and diverse network, reporting all counters to a single central point is not a realistic solution as the latency is far too high and guaranteeing the availability of the central service causes more challenges.</p>
	<p>First let’s take a look at how the traffic is routed in the Cloudflare network. All the traffic going to our edge servers is <a href="https://en.wikipedia.org/wiki/Anycast" target="_blank">anycast</a> traffic. This means that we announce the same IP address for a given web application, site or API worldwide, and traffic will be automatically and consistently routed to the closest live data center.</p>
	<p><img src="https://blog.cloudflare.com/content/images/2017/06/world.png" alt="Anycast routing example"></p>
	<p>This property is extremely valuable: we are sure that, under normal conditions<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>, the traffic from a single IP address will always reach the same PoP. Unfortunately each new TCP connection might hit a different server inside that PoP. But we can still narrow down our problem: we can actually create an isolated counting system inside each PoP. This mostly solves the latency problem and greatly improves the availability as well.</p>
	<h3 id="storingcounters">Storing counters</h3>
	<p>At Cloudflare, each server in our edge network is as independent as possible to make their administration simple. Unfortunately for rate limiting, we saw that we do need to share data across many different servers.</p>
	<p>We actually had a similar problem in the past with <a href="https://en.wikipedia.org/wiki/Transport_Layer_Security#Session_IDs" target="_blank">SSL session IDs</a>: each server needed to fetch TLS connection data about past connections. To solve that problem we created a <a href="https://github.com/twitter/twemproxy" target="_blank">Twemproxy</a> cluster inside each of our PoPs: this allows us to split a memcache<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup> database across many servers. A <a href="https://en.wikipedia.org/wiki/Consistent_hashing" target="_blank">consistent hashing</a> algorithm ensures that when the cluster is resized, only a few number of keys are hashed differently.</p>
	<p>In our architecture, each server hosts a shard of the database. As we already had experience with this system, we wanted to leverage it for the rate limit as well.</p>
	<h3 id="algorithms">Algorithms</h3>
	<p>Now let’s take a deeper look at how the different rate limit algorithms work. What we call the <em>sampling period</em> in the next paragraph is the reference unit of time for the counter (1 second for a 10 req/sec rule, 1 minute for a 600 req/min rule, ...).</p>
	<p>The most naive implementation is to simply increment a counter that we reset at the start of each sampling period. This works but is not terribly accurate as the counter will be arbitrarily reset at regular intervals, allowing regular traffic spikes to go through the rate limiter. This can be a problem for resource intensive endpoints.</p>
	<p>Another solution is to store the timestamp of every request and count how many were received during the last sampling period. This is more accurate, but has huge processing and memory requirements as checking the state of the counter require reading and processing a lot of data, especially if you want to rate limit over long period of time (for instance 5,000 req per hour).</p>
	<p>The <a href="https://en.wikipedia.org/wiki/Leaky_bucket" target="_blank">leaky bucket</a> algorithm allows a great level of accuracy while being nicer on resources (this is what the stock NGINX module is using). Conceptually, it works by incrementing a counter when each request comes in. That same counter is also decremented over time based on the allowed rate of requests until it reaches zero. The capacity of the bucket is what you are ready to accept as “burst” traffic (important given that legitimate traffic is not always perfectly regular). If the bucket is full despite its decay, further requests are mitigated.</p>
	<p><img src="https://blog.cloudflare.com/content/images/2017/06/leakybucket.svg.png" alt="Leaky bucket algorithm schema"></p>
	<p>However, in our case, this approach has two drawbacks:</p>
	<ul>
		<li>It has two parameters (average rate and burst) that are not always easy to tune properly</li>
		<li>We were constrained to use the memcached protocol and this algorithm requires multiple distinct operations that we cannot do atomically<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup></li>
	</ul>
	<p>So the situation was that the only operations available were <code>GET</code>, <code>SET</code> and <code>INCR</code> (atomic increment).</p>
	<h3 id="slidingwindowstotherescue">Sliding windows to the rescue</h3>
	<p><img src="https://blog.cloudflare.com/content/images/2017/06/14247536929_1a6315311d_z.jpg" alt=""><small><a href="https://creativecommons.org/licenses/by-sa/2.0/" target="_blank">CC BY-SA 2.0</a> <a href="https://www.flickr.com/photos/halfrain/14247536929/" target="_blank">image</a> by <a href="https://www.flickr.com/photos/halfrain/" target="_blank">halfrain</a></small></p>
	<p>The naive fixed window algorithm is actually not that bad: we just have to solve the problem of completely resetting the counter for each sampling period. But actually, can’t we just use the information from the previous counter in order to extrapolate an accurate approximation of the request rate?</p>
	<p>Let’s say I set a limit of 50 requests per minute on an API endpoint. The counter can be thought of like this:</p>
	<p><img src="https://blog.cloudflare.com/content/images/2017/06/sliding.svg.png" alt="Sliding window example"></p>
	<p>In this situation, I did 18 requests during the current minute, which started 15 seconds ago, and 42 requests during the entire previous minute. Based on this information, the rate approximation is calculated like this:</p>
	<pre><code>rate = 42 * ((60-15)/60) + 18
     = 42 * 0.75 + 18
     = 49.5 requests
</code></pre>
	<p>One more request during the next second and the rate limiter will start being very angry!</p>
	<p>This algorithm assumes a constant rate of requests during the previous sampling period (which can be any time span), this is why the result is only an approximation of the actual rate. This algorithm can be improved, but in practice it proved to be good enough:</p>
	<ul>
		<li>
			<p>It smoothes the traffic spike issue that the fixed window method has</p>
		</li>
		<li>
			<p>It very easy to understand and configure: no average vs. burst traffic, longer sampling periods can be used to achieve the same effect</p>
		</li>
		<li>
			<p>It is still very accurate, as an analysis on 400 million requests from 270,000 distinct sources shown:</p>
			<ul>
				<li>0.003% of requests have been wrongly allowed or rate limited</li>
				<li>An average difference of 6% between real rate and the approximate rate</li>
				<li>3 sources have been allowed despite generating traffic slightly above the threshold (false negatives), the actual rate was less than 15% above the threshold rate</li>
				<li>None of the mitigated sources was below the threshold (false positives)</li>
			</ul>
		</li>
	</ul>
	<p>Moreover, it offers interesting properties in our case:</p>
	<ul>
		<li>
			<p>Tiny memory usage: only two numbers per counter</p>
		</li>
		<li>
			<p>Incrementing a counter can be done by sending a single <code>INCR</code> command</p>
		</li>
		<li>
			<p>Calculating the rate is reasonably easy: one GET command<sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup> and some very simple, fast math</p>
		</li>
	</ul>
	<p>So here we are: we can finally implement a good counting system using only a few memcache primitives and without much contention. Still we were not happy with that: it requires a memcached query to get the rate. At Cloudflare we’ve seen a few of the largest L7 attacks ever. We knew that large scale attacks would have crushed the memcached cluster like this. More importantly, such operations would slow down legitimate requests a little, even under normal conditions. This is not acceptable.</p>
	<p>This is why the increment jobs are run asynchronously without slowing down the requests. If the request rate is above the threshold, another piece of data is stored asking all servers in the PoP to start applying the mitigation for that client. Only this bit of information is checked during request processing.</p>
	<p><img src="https://blog.cloudflare.com/content/images/2017/06/el-sequence.svg.png" alt="Request processing example"></p>
	<p>Even more interesting: once a mitigation has started, we know exactly when it will end. This means that we can cache that information in the server memory itself. Once a server starts to mitigate a client, it will not even run another query for the subsequent requests it might see from that source!</p>
	<p>This last tweak allowed us to efficiently mitigate large L7 attacks without noticeably penalizing legitimate requests.</p>
	<h3 id="conclusion">Conclusion</h3>
	<p>Despite being a young product, the rate limiter is already being used by many customers to control the rate of requests that their origin servers receive. The rate limiter already handles several billion requests per day and we recently mitigated attacks with as many as 400,000 requests per second to a single domain without degrading service for legitimate users.</p>
	<p>We just started to explore how we can efficiently protect our customers with this new tool. We are looking into more advanced optimizations and create new features on the top of the existing work.</p>
	<p>Interested in working on high-performance code running on thousands of servers at the edge of the network? Consider <a href="https://www.cloudflare.com/careers/" target="_blank">applying</a> to one of our open positions!</p>
	<hr>
	<hr class="footnotes-sep">
	<section class="footnotes">
		<ol class="footnotes-list">
			<li id="fn1" class="footnote-item">
				<p>The inner workings of anycast route changes are outside of the scope of this article, but we can assume that they are rare enough in this case. <a href="#fnref1" class="footnote-backref">↩︎</a></p>
			</li>
			<li id="fn2" class="footnote-item">
				<p>Twemproxy also supports Redis, but our existing infrastructure was backed by <a href="https://github.com/twitter/twemcache" target="_blank">Twemcache</a> (a Memcached fork) <a href="#fnref2" class="footnote-backref">↩︎</a></p>
			</li>
			<li id="fn3" class="footnote-item">
				<p>Memcache does support <a href="https://github.com/memcached/memcached/wiki/Commands#cas" target="_blank">CAS</a> (Compare-And-Set) operations and so optimistic transactions are possible, but it is hard to use in our case: during attacks, we will have a lot of requests, creating a lot of contention, in turn resulting in a lot of CAS transactions failing. <a href="#fnref3" class="footnote-backref">↩︎</a></p>
			</li>
			<li id="fn4" class="footnote-item">
				<p>The counters for the previous and current minute can be retrieved with a single GET command <a href="#fnref4" class="footnote-backref">↩︎</a></p>
			</li>
		</ol>
	</section>
	<!--kg-card-end: markdown-->
</div>