<div class="mb2 gray5">31 min read</div><img class="mr2" src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/1mRVILukccbpDXUHoFlnL9/a1d7b3ab969e4bdc685579280fdd1f25/image1.png" alt="">
<div class="post-content lh-copy gray1">
	<p>The cryptography that secures the Internet is evolving, and it's time to catch up. This post is a tutorial on lattice cryptography, the paradigm at the heart of the post-quantum (PQ) transition.</p>
	<p>Twelve years ago (in 2013), the <a href="https://en.wikipedia.org/wiki/Edward_Snowden"><u>revelation of mass surveillance in the US</u></a> kicked off the widespread adoption of <a href="https://www.cloudflare.com/learning/ssl/transport-layer-security-tls"><u>TLS</u></a> for encryption and authentication on the web. This transition was buoyed by the standardization and implementation of new, more efficient public-key cryptography based on <a href="https://blog.cloudflare.com/a-relatively-easy-to-understand-primer-on-elliptic-curve-cryptography"><u>elliptic curves</u></a>. Elliptic curve cryptography was both faster and required less communication than its predecessors, including <a href="https://en.wikipedia.org/wiki/RSA_(cryptosystem)"><u>RSA</u></a> and <a href="https://en.wikipedia.org/wiki/Diffie%E2%80%93Hellman_key_exchange"><u>Diffie-Hellman</u></a> over finite fields.</p>
	<p>Today's transition to <a href="https://www.cloudflare.com/learning/ssl/quantum/what-is-post-quantum-cryptography"><u>PQ cryptography</u></a> addresses a looming threat for TLS and beyond: once built, a sufficiently large quantum computer can be used to break all <a href="https://www.cloudflare.com/learning/ssl/how-does-public-key-encryption-work">public-key cryptography</a> in use today. And we continue to see <a href="https://blog.google/technology/research/google-willow-quantum-chip"><u>advancements</u></a> in quantum-computer engineering that bring us closer to this threat becoming a reality.</p>
	<p>Fortunately, this transition is well underway. The research and standards communities have spent the last several years developing alternatives that resist quantum cryptanalysis. For its part, Cloudflare has contributed to this process and is an early adopter of newly developed schemes. In fact, PQ encryption has been available at our edge <a href="https://blog.cloudflare.com/post-quantum-for-all"><u>since 2022</u></a> and is <a href="https://radar.cloudflare.com/adoption-and-usage#post-quantum-encryption-adoption"><u>used in over 35% of non-automated HTTPS traffic today (2025)</u></a>. And this year we're beginning a major push towards PQ authentication for the TLS ecosystem.</p>
	<p>Lattice-based cryptography is the first paradigm that will replace <a href="https://blog.cloudflare.com/a-relatively-easy-to-understand-primer-on-elliptic-curve-cryptography"><u>elliptic curves</u></a>. Apart from being PQ secure, lattices are often as fast, and sometimes faster, in terms of CPU time. However, this new paradigm for public key crypto has one major cost: <b>lattices require much more communication than elliptic curves.</b> For example, establishing an encryption key using lattices requires 2272 bytes of communication between the client and the server (<a href="https://csrc.nist.gov/pubs/fips/203/final"><u>ML-KEM-768</u></a>), compared to just 64 bytes for a key exchange using a modern elliptic-curve-based scheme (<a href="https://datatracker.ietf.org/doc/html/rfc7748"><u>X25519</u></a>). Accommodating such costs requires a significant amount of engineering, from dealing with <a href="https://blog.cloudflare.com/sizing-up-post-quantum-signatures"><u>TCP packet fragmentation</u></a>, to reworking <a href="https://blog.cloudflare.com/pq-2024/#two-migrations"><u>TLS and its public key infrastructure</u></a>. Thus, the PQ transition is going to require the participation of a large number of people with a variety of backgrounds, not just cryptographers.</p>
	<p>The primary audience for this blog post is those who find themselves involved in the PQ transition and want to better understand what's going on under the hood. However, more fundamentally, we think it's important for everyone to understand lattice cryptography on some level, especially if we're going to trust it for our security and privacy.</p>
	<p>We'll assume you have a software-engineering background and some familiarity with concepts like TLS, encryption, and authentication. We'll see that the math behind lattice cryptography is, at least at the highest level, not difficult to grasp. Readers with a crypto-engineering background who want to go deeper might want to start with the excellent <a href="https://eprint.iacr.org/2024/1287"><u>tutorial by Vadim Lyubashevsky</u></a> on which this blog post is based. We also recommend <a href="https://keymaterial.net/2023/09/01/learning-but-with-errors"><u>Sophie Schmieg's blog</u></a> on this subject.</p>
	<p>While the transition to lattice cryptography incurs costs, it also creates opportunities. Many things we can build with elliptic curves we can also build with lattices, though not always as efficiently; but there are also things we can do with lattices that we don't know how to do efficiently with anything else. We'll touch on some of these applications at the very end.</p>
	<p>We're going to cover a lot of ground in this post. If you stick with it, we hope you'll come away feeling empowered, not only to tackle the engineering challenges the PQ transition entails, but to solve problems you didn't know how to solve before.</p>
	<p>Strap in — let's have some fun!</p>
	<div class="flex anchor relative">
		<h3 id="encryption">Encryption</h3>
		<a href="https://blog.cloudflare.com/#encryption" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>The most pressing problem for the PQ transition is to ensure that tomorrow's quantum computers don't break today's encryption. An attacker today can store the packets exchanged between your laptop and a website you visit, and then, some time in the future, decrypt those packets with the help of a quantum computer. This means that much of the sensitive information transiting the Internet today — everything from <a href="https://blog.cloudflare.com/https-only-for-cloudflare-apis-shutting-the-door-on-cleartext-traffic"><u>API tokens</u></a> and passwords to database encryption keys — may one day be unlocked by a quantum computer.</p>
	<p>In fact, today's encryption in TLS is <i>mostly</i> PQ secure: <b>what's at risk is the process by which your browser and a server establish an encryption key</b>. Today this is usually done with elliptic-curve-based schemes, which are not PQ secure; our goal for this section is to understand how to do key exchange with lattices-based schemes, which are.</p>
	<p>We will work through and implement a simplified version of <a href="https://csrc.nist.gov/pubs/fips/203/final"><u>ML-KEM</u></a>, a.k.a. Kyber, the most widely deployed PQ key exchange in use today. Our code will be less efficient and secure than a <a href="https://pkg.go.dev/github.com/cloudflare/circl@v1.6.0/kem/mlkem"><u>spec-compliant, production-quality implementation</u></a>, but will be good enough to grasp the main ideas.</p>
	<p>Our starting point is a protocol that looks an awful lot like <a href="https://en.wikipedia.org/wiki/Diffie%E2%80%93Hellman_key_exchange"><u>Diffie-Hellman (DH)</u></a> key exchange. For those readers unacquainted with DH, the goal is for Alice and Bob to establish a shared secret over an insecure network. To do so, each picks a random secret number, computes the corresponding "key share", and sends the key share to the other:</p>
	<figure class="kg-card kg-image-card">
		<img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/5yWJO1Sem97PfhwqiKZRLm/bfacc44ca4f7187f4212b7b4616dd86d/image7.png" alt="" class="kg-image" width="1220" height="693" loading="lazy">
	</figure>
	<p>Alice's secret number is $s$ and her key share is $g^s$; Bob's secret number is $r$ and his key share is $g^r$. Then given their secret and their peer's key share, each can compute $g^{rs}$. The security of this protocol comes from how we choose $g$, $s$, and $r$ and how we do arithmetic. The most efficient instantiation of DH uses elliptic curves.</p>
	<p>In ML-KEM we replace operations on elliptic curves with <b>matrix </b>operations. It's not quite a drop-in replacement, so we'll need a little linear algebra to make sense of it. But don't worry: we're going to work with Python so we have running code to play with, and we'll use <a href="https://numpy.org"><u>NumPy</u></a> to keep things high level.</p>
	<div class="flex anchor relative">
		<h4 id="all-the-math-well-need">All the math we'll need</h4>
		<a href="https://blog.cloudflare.com/#all-the-math-well-need" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>A matrix is just a two-dimensional array of numbers. In NumPy, we can create a matrix as follows (importing <code>numpy</code> as <code>np</code>):</p>
	<pre class="language-Python"><code class="language-Python">A = np.matrix([[1, 2, 3],
               [4, 5, 6],
               [7, 8, 9]])</code></pre>
	<p>This defines <code>A</code> to be the <code>3</code>-by-<code>3</code> matrix with entries <code>A[0,0]==1, A[0,1]==2, A[0,2]==3, A[1,0]==4</code>, and so on.</p>
	<p>For the purposes of this post, the entries of our matrices will always be integers. Furthermore, whenever we add, subtract, or multiply two integers, we then <b>reduce</b> the result, just like we do with hours on a clock, so that we end up with a number in <code>range(Q)</code> for some positive number <code>Q</code>, called the modulus. The exact value doesn’t really matter now, but for ML-KEM it’s <code>Q=3329</code>, so let's go with that for now. (The modulus for a clock would be <code>Q=12</code>.)</p>
	<p>In Python, we write multiplication of integers <code>a</code> and <code>b</code> modulo <code>Q</code> as<code> c = a*b % Q</code>. Here we compute <code>a*b</code>, divide the result by <code>Q</code>, then set <code>c</code> to the remainder. For example, <code>42*1337</code> <code>% Q</code> is equal to <code>2890</code> rather than <code>56154</code>. Modular addition and subtraction are done analogously. For the rest of this blog, we will sometimes omit "<code>% Q</code>" when it's clear in context that we mean modular arithmetic.</p>
	<p>Next, we'll need three operations on matrices.</p>
	<p>The first is <b>matrix transpose</b>, written <code>A.T </code>in NumPy. This operation flips the matrix along its diagonal so that <code>A.T[j,i] == A[i,j]</code> for all rows <code>i</code> and columns <code>j</code>:</p>
	<pre class="language-Python"><code class="language-Python">print(A.T)
# [[1 4 7]
#  [2 5 8]
#  [3 6 9]]</code></pre>
	<p>To visualize this, imagine writing down a matrix on a translucent piece of paper. Draw a line from the top left corner to the bottom right corner of that paper, then rotate the paper 180° around that line:</p>
	<figure class="kg-card kg-image-card">
		<img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/2x3qNDptBldEKCgQBbmNPr/ad174561910e6c90753024625f16fd68/image3.png" alt="" class="kg-image" width="1220" height="332" loading="lazy">
	</figure>
	<p>The second operation we'll need is <b>matrix multiplication</b>. Normally, we will multiply a matrix by a <b>column vector</b>, which is just a matrix with one column. For example, the following <code>3</code>-by-<code>1</code> matrix is a column vector:</p>
	<pre class="language-Python"><code class="language-Python">s = np.matrix([[0],
               [1],
               [0]])</code></pre>
	<p>We can also write <code>s </code>more concisely as <code>np.matrix([[0,1,0]]).T</code>.&nbsp; To multiply a square matrix <code>A</code> by a column vector <code>s</code>, we compute the <b>dot product</b> of each row of <code>A</code> with <code>s</code>. That is, if <code>t = A*s % Q</code>, then <code>t[i] == (A[i,0]*s[0,0] + A[i,1]*s[1,0] + A[i,2]*s[2,0]) % Q </code>for each row <code>i</code>. The output will always be a column vector:</p>
	<pre class="language-Python"><code class="language-Python">print(A*s % Q)
# [[2]
#  [5]
#  [8]]</code></pre>
	<p>The number of rows of this column vector is equal to the number of rows of the matrix on the left hand side. In particular, if we take our column vector <code>s</code>, transpose it into a <code>1</code>-by-<code>3</code> matrix, and multiply it by a <code>3</code>-by-<code>1</code> matrix <code>r</code>, then we end up with a <code>1</code>-by-<code>1 </code>matrix:</p>
	<pre class="language-Python"><code class="language-Python">r = np.matrix([[1,2,3]]).T
print(s.T*r % Q)
# [[2]]</code></pre>
	<p>The final matrix operation we'll need is <b>matrix addition</b>. If <code>A</code> and <code>B</code> are both <code>N</code>-by-<code>M</code> matrices, then <code>C = (A+B) % Q</code> is the <code>N</code>-by-<code>M</code> matrix for which <code>C[i,j] == (A[i,j]+B[i,j]) % Q</code>. Of course, this only works if the matrices we're adding have the same dimensions.</p>
	<div class="flex anchor relative">
		<h4 id="warm-up">Warm up</h4>
		<a href="https://blog.cloudflare.com/#warm-up" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>Enough maths — let's get to exchanging some keys. We start with the DH diagram from before and swap out the computations with matrix operations. Note that this protocol is not secure, but will be the basis of a secure key exchange mechanism we'll develop in the next section:</p>
	<figure class="kg-card kg-image-card">
		<img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/3qRFXuCjStyX8IdRgHogJi/0ea04a675e6b8b52fc74a2375827890c/image6.png" alt="" class="kg-image" width="1220" height="693" loading="lazy">
	</figure>
	<ul>
		<li>
			<p>Alice and Bob agree on a public, <code>N</code>-by-<code>N</code> matrix <code>A</code>. This is analogous to the number $g$ that Alice and Bob agree on in the DH diagram.</p>
		</li>
		<li>
			<p>Alice chooses a random length<code>-N</code> vector <code>s</code> and sends <code>t = A*s % Q</code> to Bob.</p>
		</li>
		<li>
			<p>Bob chooses a random length<code>-N</code> vector <code>r</code> and sends <code>u = r.T*A % Q</code> to Alice. You can also compute this as <code>(A.T*r).T % Q.</code></p>
		</li>
	</ul>
	<p>The vectors <code>t</code> and <code>u</code> are analogous to DH key shares. After the exchange of these key shares, Alice and Bob can compute a shared secret. Alice computes the shared secret as <code>u*s % Q</code> and Bob computes the shared secret as <code>r.T*t % Q</code>. To see why they compute the same key, notice that <code>u*s == (r.T*A)*s == r.T*(A*s) == r.T*t.</code></p>
	<p>In fact, this key exchange is essentially what happens in ML-KEM. However, we don't use this directly, but rather as part of a <b>public key encryption scheme</b>. Public key encryption involves three algorithms:</p>
	<ul>
		<li>
			<p><code>key_gen():</code> The key generation algorithm that outputs a public encryption key <code>pk</code> and the corresponding secret decryption key <code>sk</code>.</p>
		</li>
		<li>
			<p><code>encrypt()</code>: The encryption algorithm that takes the public key and a plaintext and outputs a ciphertext.</p>
		</li>
		<li>
			<p><code>decrypt()</code>: The decryption algorithm that takes the secret key and a ciphertext and outputs the underlying plaintext. That is, <code>decrypt(sk, encrypt(pk, ptxt)) == ptxt</code> for any plaintext <code>ptxt</code>.</p>
		</li>
	</ul>
	<p>We'll say the scheme is secure if, given a ciphertext and the public key used to encrypt it, no attacker can discern any information about the underlying plaintext without knowledge of the secret key. Once we have this encryption scheme, we then transform it into a <b>key-encapsulation mechanism</b> (the "KEM" in "ML-KEM") in the last step. A KEM is very similar to encryption except that the plaintext is always a randomly generated key.</p>
	<p>Our encryption scheme is as follows:</p>
	<ul>
		<li>
			<p><code>key_gen()</code>: To generate a key pair, we choose a random, square matrix <code>A</code> and a random column vector <code>s</code>. We set our public key to <code>(A,t=A*s % Q)</code> and our secret key to <code>s</code>. Notice that <code>t </code>is Alice's&nbsp; key share from the key exchange protocol above.</p>
		</li>
		<li>
			<p><code>encrypt()</code>: Suppose our plaintext <code>ptxt </code>is an integer in <code>range(Q)</code>. To encrypt <code>ptxt</code>, Bob generates his key share <code>u</code>. He then derives the shared secret and adds it to <code>ptxt</code>. The ciphertext has two components:</p>
		</li>
	</ul>
	<blockquote>
		<p><code>u = r.T*A % Q</code></p>
		<p><code>v = (r.T*t + m) % Q</code></p>
	</blockquote>
	<p>Here <code>m </code>is a <code>1</code>-by-<code>1 </code>matrix containing the plaintext, i.e., <code>m = np.matrix([[ptxt]])</code>, and <code>r</code> is a random column vector.</p>
	<ul>
		<li>
			<p><code>decrypt()</code>: To decrypt, Alice computes the shared secret and subtracts it from <code>v</code>:</p>
		</li>
	</ul>
	<blockquote>
		<p><code>m = (v - u*s) % Q</code></p>
	</blockquote>
	<p>Some readers will notice that this looks an awful lot like <a href="https://en.wikipedia.org/wiki/ElGamal_encryption"><u>El Gamal</u></a> encryption. This isn't a coincidence. Good cryptographers roll their own crypto; great cryptographers steal from good cryptographers.</p>
	<p>Let's now put this together into code. The last thing we'll need is a method of generating random matrices and column vectors. We call this function <code>gen_mat() </code>below. Take a crack at implementing this yourself. Our scheme has two parameters: the modulus <code>Q</code>; and the dimension of <code>N</code> of the matrix and column vectors. The choice of <code>N</code> matters for security, but for now feel free to pick whatever value you want.</p>
	<pre class="language-Python"><code class="language-Python">def key_gen():
    # Here `gen_mat()` returns an N-by-N matrix with entries
    # randomly chosen from `range(0, Q)`.
    A = gen_mat(N, N, 0, Q)
    # Like above except the matrix is N-by-1.
    s = gen_mat(N, 1, 0, Q)
    t = A*s % Q
    return ((A, t), s)

def encrypt(pk, ptxt):
    (A, t) = pk
    m = np.matrix([[ptxt]])
    r = gen_mat(N, 1, 0, Q)
    u = r.T*A % Q
    v = (r.T*t + m) % Q
    return (u, v)

def decrypt(sk, ctxt):
    s = sk
    (u, v) = ctxt
    m = (v - u*s) % Q
    return m[0,0]

# Test
assert decrypt(sk, encrypt(pk, 1)) == 1</code></pre>

	<div class="flex anchor relative">
		<h4 id="making-the-scheme-secure-or-what-is-a-lattice">Making the scheme secure (or "What is a lattice?")</h4>
		<a href="https://blog.cloudflare.com/#making-the-scheme-secure-or-what-is-a-lattice" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>By now, you might be wondering what on Earth a lattice even is. We promise we'll define it, but before we do, it'll help to understand why our warm-up scheme is insecure and what it'll take to fix it.</p>
	<p>Readers familiar with linear algebra may already see the problem: in order for this scheme to be secure, it should be impossible for the attacker to recover the secret key <code>s</code>; but given the public <code>(A,t)</code>, we can immediately solve for <code>s</code> using <a href="https://en.wikipedia.org/wiki/Gaussian_elimination"><u>Gaussian elimination</u></a>.</p>
	<p>In more detail, if <code>A</code> is invertible, we can write the secret key as <code>A</code><code><sup>-1</sup></code><code>*t == A</code><code><sup>-1</sup></code><code>*(A*s) == (A</code><code><sup>-1</sup></code><code>*A)*s == s,</code> where <code>A</code><code><sup>-1</sup></code> is the inverse of <code>A</code>. (When you multiply a matrix by its inverse, you get the identity matrix <code>I</code>, which simply takes a column vector to itself, i.e., <code>I*s == s.</code>) We can use Gaussian elimination to compute this matrix. Intuitively, all we're doing is solving a set of linear equations, where the entries of <code>s</code> are the unknown variables. (Note that this is possible even if <code>A</code> is not invertible.)</p>
	<p>In order to make this encryption scheme secure, we need to make it a little... “messier”.</p>
	<div class="flex anchor relative">
		<h5 id="lets-get-messy">Let's get messy</h5>
		<a href="https://blog.cloudflare.com/#lets-get-messy" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>For starters, we need to make it hard to recover the secret key from the public key. Let's try the following: generate another random vector <code>e</code> and add it into <code>A*</code>s. Our key generation algorithm becomes:</p>
	<pre class="language-Python"><code class="language-Python">def key_gen():
    A = gen_mat(N, N, 0, Q)
    s = gen_mat(N, 1, 0, Q)
    e = gen_mat(N, 1, 0, Q)
    t = (A*s + e) % Q
    return ((A, t), s)</code></pre>
	<p>Our formula for the column vector component of the public key, <code>t</code>, now includes an additive term <code>e</code>, which we'll call the <b>error</b>. Like the secret key, the error is just a random vector.&nbsp;</p>
	<p>Notice that the previous attack no longer works: since <code>A</code><code><sup>-1</sup></code><code>*t == A</code><code><sup>-1</sup></code><code>*(A*s + e) == A</code><code><sup>-1</sup></code><code>*(A*s) + A</code><code><sup>-1</sup></code><code>*e == s + A</code><code><sup>-1</sup></code><code>*e</code>, we need to know <code>e</code> in order to compute <code>s</code>.</p>
	<p>Great, but this patch creates another problem. Take a second to plug in this new key generation algorithm into your implementation and test it out. What happens?</p>
	<p>You should see that <code>decrypt()</code> now outputs garbage. We can see why using a little algebra:</p>
	<blockquote>
		<p> <code>(v - u*s) == (r.T*t + m) - (r.T*A)*s</code></p>
		<p><code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;== r.T*(A*s + e) + m - (r.T*A)*s</code></p>
		<p><code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;== r.T*(A*s) + r.T*e + m - r.T*(A*s)</code></p>
		<p><code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;== r.T*e + m</code></p>
	</blockquote>
	<p>The entries of <code>r</code> and <code>e</code> are sampled randomly, so <code>r.T*e</code> is also uniformly random. It's as if we encrypted <code>m</code> with a <a href="https://en.wikipedia.org/wiki/One-time_pad"><u>one-time pad</u></a>, then threw away the one-time pad!</p>
	<h6>Handling decryption errors</h6>
	<p>What can we do about this? First, it would help if <code>r.T*e</code> were small so that decryption yields something that's close to the plaintext. Imagine we could generate <code>r</code> and<code> e</code> in such a way that <code>r.T*e</code> were in <code>range(-epsilon, epsilon+1)</code> for some small <code>epsilon</code>. Then <code>decrypt</code> would output a number in <code>range(ptxt-epsilon, ptxt+epsilon+1)</code>, which would be pretty close to the actual plaintext.</p>
	<p>However, we need to do better than get close. Imagine your browser failing to load your favorite website one-third of the time because of a decryption error. Nobody has time for that.</p>
	<p>ML-KEM reduces the probability of decryption errors by being clever about how we encode the plaintext. Suppose all we want to do is encrypt a single bit, i.e., <code>ptxt </code>is either <code>0</code> or <code>1</code>. Consider the numbers in <code>range(Q)</code>, and split the number line into four chunks of roughly equal length:</p>
	<figure class="kg-card kg-image-card">
		<img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/3xojaMtl881io3pH4BdXNe/d47aede96c751942b82fedf0ca396450/image2.png" alt="" class="kg-image" width="1220" height="333" loading="lazy">
	</figure>
	<p>Here we've labeled the region around zero (<code>-Q/4</code> to <code>Q/4</code> modulo <code>Q</code>) with <code>ptxt=0</code> and the region far away from zero with <code>ptxt=1</code>. To encode the bit, we set it to the integer corresponding to the middle of its range, i.e., <code>m = np.matrix([[ptxt * Q//2]])</code>. (Note the double "<code>//</code>" — this denotes integer division in Python.) To decode, we choose the <code>ptxt</code> corresponding to whatever range<code> m[0,0] </code>is in. That way if the decryption error is small, then we're highly likely to end up in the correct range.</p>
	<p>Now all that's left is to ensure the decryption error, <code>r.T*e</code>, is small. We do this by sampling <b>short vectors </b><code>r</code> and <code>e</code>. By "short" we mean the entries of these vectors are sampled from a range that is much smaller than <code>range(Q)</code>. In particular, we'll pick some small positive integer <code>beta</code> and sample entries <code>range(-beta,beta+1)</code>.</p>
	<p>How do we choose <code>beta</code>? Well, it should be small enough that decryption succeeds with overwhelming probability, but not so small that <code>r</code> and <code>e</code> are easy to guess and our scheme is broken. Take a minute or two to play with this. The parameters we can vary are:</p>
	<ul>
		<li>
			<p>the modulus <code>Q</code></p>
		</li>
		<li>
			<p>the dimension of the column vectors <code>N</code></p>
		</li>
		<li>
			<p>the shortness parameter <code>beta</code></p>
		</li>
	</ul>
	<p>For what ranges of these parameters is the decryption error low but the secret vectors are hard to guess? For what ranges is our scheme most efficient, in terms of runtime and communication cost (size of the public key plus the ciphertext)? We'll give a concrete answer at the end of this section, but in the meantime, we encourage you to play with this a bit.</p>
	<h6>Gauss strikes back</h6>
	<p>At this point, we have a working encryption scheme that mitigates at least one key-recovery attack. We've come pretty far, but we have at least one more problem.</p>
	<p>Take another look at our formula for the ciphertext&nbsp; <code>ctxt = (u,v)</code>. What would happen if we managed to recover the random vector <code>r</code>? That would be catastrophic, since <code>v == r.T*t + m</code>, and we already know <code>t</code> (part of the public key) and<code> v </code>(part of the ciphertext).</p>
	<p>Just as we were able to compute the secret key from the public key in our initial scheme, we can recover the encryption randomness <code>r</code> from the ciphertext component <code>u</code> using Gaussian elimination. Again, this is just because <code>r</code> is the solution to a system of linear equations.</p>
	<p>We can mitigate this plaintext-recovery attack just as before, by adding some noise. In particular, we'll generate a short vector according to <code>gen_mat(N,1,-beta,beta+1)</code> and add it into <code>u</code>. We also need to add noise to <code>v</code> in the same way, for reasons that we'll discuss in the next section.</p>
	<p>Once again, adding noise increases the probability of a decryption error, but this time the magnitude of the error also depends on the secret key <code>s</code>. To see this, recall that during decryption, we multiply <code>u</code> by <code>s</code> (to compute the shared secret), and the error vector is an additive term. We'll therefore need <code>s</code> to be a short vector as well.</p>
	<p>Let's now put together everything we've learned into an updated encryption scheme. Our scheme now has three parameters, <code>Q</code>, <code>N</code>, and <code>beta</code>, and can be used to encrypt a single bit:</p>
	<pre class="language-Python"><code class="language-Python">def key_gen():
    A = gen_mat(N, N, 0, Q)
    s = gen_mat(N, 1, -beta, beta+1)
    e1 = gen_mat(N, 1, -beta, beta+1)
    t = (A*s + e1) % Q
    return ((A, t), s)

def encrypt(pk, ptxt):
    (A, t) = pk
    m = np.matrix([[ptxt*(Q//2) % Q]])
    r = gen_mat(N, 1, -beta, beta+1)
    e2 = gen_mat(N, 1, -beta, beta+1)
    e3 = gen_mat(1, 1, -beta, beta+1)
    u = (r.T*A + e2) % Q
    v = (r.T*t + e3 + m) % Q
    return (u, v)

def decrypt(sk, ctxt):
    s = sk
    (u, v) = ctxt
    m = (v - u*s) % Q
    if m[0,0] in range(Q//4, 3*Q//4):
        return 1
    return 0

# Test
assert decrypt(sk, encrypt(pk, 0)) == 0
assert decrypt(sk, encrypt(pk, 1)) == 1</code></pre>
	<p>Before moving on, try to find parameters for which the scheme works and for which the secret and error vectors seem hard to guess.</p>
	<div class="flex anchor relative">
		<h5 id="learning-with-errors">Learning with errors</h5>
		<a href="https://blog.cloudflare.com/#learning-with-errors" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>So far we have a functioning encryption scheme for which we've mitigated two attacks, one a key-recovery attack and the other a plaintext-recovery attack. There seems to be no other obvious way of breaking our scheme, unless we choose parameters that are so weak that an attacker can easily guess the secret key s or ciphertext randomness r. Again, these vectors need to be short in order to prevent decryption errors, but not so short that they are easy to guess. (Likewise for the error terms.)</p>
	<p>Still, there may be other attacks that require a little more sophistication to pull off. For instance, there might be some mathematical analysis we can do to recover, or at least make a good guess of, a portion of the ciphertext randomness. This raises a more fundamental question: in general, how do we establish that cryptosystems like this are actually secure?</p>
	<p>As a first step, cryptographers like to try and reduce the attack surface. <b>Modern cryptosystems are designed so that the problem of attacking the scheme reduces to solving some other problem that is easier to reason about.</b></p>
	<p>Our public key encryption scheme is an excellent illustration of this idea. Think back to the key- and plaintext-recovery attacks from the previous section. What do these attacks have in common?</p>
	<p>In both instances, the attacker knows some public vector that allowed it to recover a secret vector:</p>
	<ul>
		<li>
			<p>In the key-recovery attack, the attacker knew <code>t</code> for which <code>A*s == t.</code></p>
		</li>
		<li>
			<p>In the plaintext-recovery attack, the attacker knew <code>u</code> for which <code>r.T*A == u </code>(or, equivalently,<code> A.T*r == u.T</code>).</p>
		</li>
	</ul>
	<p>The fix in both cases was to construct the public vector in such a manner that it is hard to solve for the secret, namely, by adding an error term. However, ideally the public vector would reveal no information about the secret whatsoever. This ideal is formalized by the <b>Learning With Errors (LWE)</b> problem.</p>
	<p>The LWE problem asks the attacker to distinguish between two distributions. Concretely, imagine we flip a coin, and if it comes up heads, we sample from the first distribution and give the sample to the attacker; and if the coin comes up tails, we sample from the second distribution and give the sample to the attacker. The distributions are as follows:</p>
	<ul>
		<li>
			<p><code>(A,t=A*s + e</code>) where <code>A</code> is a random matrix generated with <code>gen_mat(N,N,0,Q)</code> and <code>s</code> and <code>e</code> are short vectors generated with <code>gen_mat(N,1,-beta,beta+1)</code>.</p>
		</li>
		<li>
			<p><code>(A,t) </code>where <code>A</code> is a random matrix generated with <code>gen_mat(N,N,0,Q)</code> and <code>t</code> is a random vector generated with <code>gen_mat(N,1,0,Q)</code>.</p>
		</li>
	</ul>
	<p>The first distribution corresponds to what we actually do in the encryption scheme; in the second, <code>t</code> is just a random vector, and no longer a secret vector at all. We say that the LWE problem is "hard" if no attacker is able to guess the coin flip with probability significantly better than one-half.</p>
	<p>Our encryption is <i>passively </i>secure — meaning the ciphertext doesn't leak any information about the plaintext — if the LWE problem is hard for the parameters we chose. To see why, notice that both the public key and ciphertext look like LWE instances; if we can replace each instance with an instance of the random distribution, then the ciphertext would be completely independent of the plaintext and therefore leak no information about it at all. Note that, for this argument to go through, we also have to add the error term <code>e3</code> to the ciphertext component <code>v</code>.</p>
	<div class="flex anchor relative">
		<h5 id="choosing-the-parameters">Choosing the parameters</h5>
		<a href="https://blog.cloudflare.com/#choosing-the-parameters" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>We've established that if solving the LWE problem is hard for parameters <code>N</code>, <code>Q</code>, and <code>beta</code>, then so is breaking our public key encryption scheme. What's left for us to do is tune the parameters so that solving LWE is beyond the reach of any attacker we can think of. This is where lattices come in.</p>
	<h6>Lattices</h6>
	<p>A <b>lattice </b>is an infinite grid of points in high-dimensional space. A two-dimensional lattice might look something like this:</p>
	<figure class="kg-card kg-image-card">
		<img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/5Js1SkG7bWyCNwPxKaYf3i/277a665f0e44ec7594e7e11b35958bc8/image4.png" alt="" class="kg-image" width="640" height="480" loading="lazy">
	</figure>
	<p>The points always follow a clear pattern that resembles "lattice work" you might see in a garden:</p>
	<figure class="kg-card kg-image-card">
		<img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/5kOimwlM1guY2YMj9Gn9xu/62ee11ddde1e18ba0cf3151a0b87092e/image5.jpg" alt="" class="kg-image" width="640" height="480" loading="lazy">
	</figure>
	<p><sup><i>(Source: https://picryl.com/media/texture-wood-vintage-backgrounds-textures-8395bb)</i></sup></p>
	<p>For cryptography, we care about a special class of lattices, those defined by a matrix <code>P</code> that "recognizes" points in the lattice. That is, the lattice recognized by <code>P</code> is the set of vectors <code>v</code> for which <code>P*v == 0</code>, where "<code>0</code>" denotes the all-zero vector. The all-zero vector is <code>np.zeros((N,1), dtype=int)</code> in NumPy.</p>
	<p>Readers familiar with linear algebra may have a different definition of lattices in mind: in general, a lattice is the set of points obtained by taking linear combinations of some basis. Our lattices can also be formulated in this way, i.e., for a matrix <code>P</code> that recognizes a lattice, we can compute the basis vectors that generate the lattice. However, we don't much care about this representation here.</p>
	<p>The LWE problem boils down to distinguishing a set of points that are "close to" the lattice from a set of points that are "far away from" the lattice. We construct these points from an LWE instance and a random <code>(A,t)</code> respectively. Here we have an LWE sample (left) and a sample from the random distribution (right):</p>
	<figure class="kg-card kg-image-card">
		<img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/45bmYu3jdn8g7cI1i7WC3t/c82617991463169ecd83b1b944aa879a/image10.png" alt="" class="kg-image" width="640" height="480" loading="lazy">
	</figure>
	<p>What this shows is that the points of the LWE instance are much closer to the lattice than the random instance. This is indeed the case on average. However, while distinguishing LWE instances from random is easy in two dimensions, it gets harder in higher dimensions.</p>
	<p>Let's take a look at how we construct these points. First, let's take an LWE instance <code>(A,t=(A*s + e) % Q</code>) and consider the lattice recognized by the matrix <code>P</code> we get by concatenating <code>A</code> with the identity matrix <code>I</code>. This might look something like this (<code>N=3</code>):</p>
	<pre class="language-Python"><code class="language-Python">A = gen_mat(N, N, 0, Q)
P = np.concatenate((A, np.identity(N, dtype=int)), axis=1)
print(P)
# [[1570  634  161	1	0	0]
#  [1522 1215  861	0	1	0]
#  [ 344 2651 1889	0	0	1]]</code></pre>
	<p>Notice that we can compute <code>t</code> by multiplying <code>P</code> by the vector we get by concatenating <code>s</code> and <code>e</code> (<code>beta=2</code>):</p>
	<pre class="language-Python"><code class="language-Python">s = gen_mat(N, 1, -beta, beta+1)
e = gen_mat(N, 1, -beta, beta+1)
t = (A*s + e) % Q
z = np.concatenate((s, e))
print(z)
# [[-2]
#  [ 0]
#  [-2]
#  [ 0]
#  [-1]
#  [ 2]]
assert np.array_equal(t, P*z % Q)</code></pre>
	<p>Let <code>z</code> denote this vector and consider the set of points <code>v</code> for which <code>P*v == t</code>. By definition, we say this set of points is "close to" the lattice because <code>z</code> is a short vector. (Remember: by "short" we mean its entries are bounded around <code>0</code> by <code>beta</code>.)</p>
	<p>Now consider a random<code> (A,t)</code> and consider the set of points <code>v</code> for which <code>P*v == t</code>. We won't prove it, but it is a fact that this set of points is likely to be "far away from" the lattice in the sense that there is no short vector <code>z</code> for which<code> P*z == t.</code></p>
	<p>Intuitively, solving LWE gets harder as <code>z</code> gets longer. Indeed, increasing the average length of <code>z</code> (by making <code>beta</code> larger) increases the average distance to the lattice, making it look more like a random instance:&nbsp;</p>
	<figure class="kg-card kg-image-card">
		<img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/4oN8fydfP54b6EDLIOTYjI/85813f716b48164077b8f6d27586f768/unnamed__1_.png" alt="" class="kg-image" width="640" height="480" loading="lazy">
	</figure>
	<p>On the other hand, making <code>z</code> too long creates another problem.</p>
	<h6>Breaking lattice cryptography by finding short vectors</h6>
	<p>Given a random matrix <code>A</code>, the <b>Short Integer Solution (SIS)</b> problem is to find short vectors (i.e., whose entries are bounded by <code>beta</code>) <code>z1</code> and <code>z2</code> for which <code>(A*z1 + z2) % Q</code> is zero. Notice that this is equivalent to finding a short vector <code>z</code> in the lattice recognized by <code>P</code>:</p>
	<pre class="language-Python"><code class="language-Python">z = np.concatenate((z1, z2))
assert np.array_equal((A*z1 + z2) % Q, P*z % Q)</code></pre>
	<p>If we had a (quantum) computer program for solving SIS, then we could use this program to solve LWE as well: if <code>(A,t) </code>is an LWE instance, then <code>z1.T*t </code>will be small; otherwise, if <code>(A,t)</code> is random, then <code>z1.T*t </code>will be uniformly random. (You can convince yourself of this using a little algebra.) Therefore, in order for our encryption scheme to be secure, it must be hard to find short vectors in the lattice defined by those parameters.</p>
	<p>Intuitively, finding long vectors in the lattice is easier than finding short ones, which means that solving the SIS problem gets easier as <code>beta </code>gets closer to <code>Q</code>. On the other hand, as <code>beta </code> gets closer to <code>0</code>, it gets easier to distinguish LWE instances from random!</p>
	<p><b>This suggests a kind of Goldilocks zone for LWE-based encryption</b>: if the secret and noise vectors are too short, then LWE is easy; but if the secret and noise vectors are too long, then SIS is easy. The optimal choice is somewhere in the middle.</p>
	<h6>Enough math, just give me my parameters!</h6>
	<p>To tune our encryption scheme, we want to choose parameters for which the most efficient known algorithms (quantum or classical) for solving LWE are out of reach for any attacker with as many resources as we can imagine (and then some, in case new algorithms are discovered). But how do we know which attacks to look out for?</p>
	<p>Fortunately, the community of expert lattice cryptographers and cryptanalysts maintains a tool called <a href="https://github.com/malb/lattice-estimator"><u>lattice-estimator</u></a> that estimates the complexity of the best known (quantum) algorithms for lattice problems relevant to cryptography. Here's what we get when we run this tool for ML-KEM (this requires <a href="https://www.sagemath.org"><u>Sage</u></a> to run):</p>
	<pre class="language-Rust"><code class="language-Rust">sage: from estimator import *
sage: res = LWE.estimate.rough(schemes.Kyber768)
usvp        :: rop: ≈2^182.2, red: ≈2^182.2, δ: 1.002902, β: 624, d: 1427, tag: usvp
dual_hybrid :: rop: ≈2^174.3, red: ≈2^174.3, guess: ≈2^162.5, β: 597, p: 4, ζ: 10, t: 60, β': 597, N: ≈2^122.7, m: 768</code></pre>
	<p>The number that we're most interested in is "<code>rop</code>", which estimates the amount of computation the attack would consume. Playing with this tool a bit, we eventually find some parameters for our scheme for which the "<code>usvp</code>" and "<code>dual_hybrid</code>" attacks have comparable complexity. However, lattice-estimator identifies an attack it calls "<code>arora-gb</code>" that applies to our scheme, but not to ML-KEM, that has much lower complexity.&nbsp; (<code>N=600</code>, <code>Q=3329</code>, and <code>beta=4</code>):</p>
	<pre class="language-Rust"><code class="language-Rust">sage: res = LWE.estimate.rough(LWE.Parameters(n=600, q=3329, Xs=ND.Uniform(-4,4), Xe=ND.Uniform(-4,4)))
usvp        :: rop: ≈2^180.2, red: ≈2^180.2, δ: 1.002926, β: 617, d: 1246, tag: usvp
dual_hybrid :: rop: ≈2^226.2, red: ≈2^225.4, guess: ≈2^224.9, β: 599, p: 3, ζ: 10, t: 0, β': 599, N: ≈2^174.8, m: 600
arora-gb    :: rop: ≈2^129.4, dreg: 9, mem: ≈2^129.4, t: 4, m: ≈2^64.7</code></pre>
	<p>We'd have to bump the parameters even further to the scheme to a regime that has comparable security to ML-KEM.</p>
	<p>Finally, a word of warning: when designing lattice cryptography, determining whether our scheme is secure requires a lot more than estimating the cost of generic attacks on our LWE parameters. In the absence of a mathematical proof of security in a realistic adversarial model, we can't rule out other ways of breaking our scheme. Tread lightly, fair traveler, and bring a friend along for the journey.</p>
	<div class="flex anchor relative">
		<h4 id="making-the-scheme-efficient">Making the scheme efficient</h4>
		<a href="https://blog.cloudflare.com/#making-the-scheme-efficient" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>Now that we understand how to encrypt with LWE, let's take a quick look at how to make our scheme efficient.</p>
	<p>The main problem with our scheme is that we can only encrypt a bit at a time. This is because we had to split the <code>range(Q)</code> into two chunks, one that encodes <code>1</code> and another that encodes <code>0</code>. We could improve the bit rate by splitting the range into more chunks, but this would make decryption errors more likely.</p>
	<p>Another problem with our scheme is that the runtime depends heavily on our security parameters. Encryption requires <code>O(N</code><code><sup>2</sup></code><code>)</code> multiplications (multiplication is the most expensive part of a secure implementation of modular arithmetic), and in order for our scheme to be secure, we need to make <code>N</code> quite large.</p>
	<p>ML-KEM solves both of these problems by replacing modular arithmetic with arithmetic over a <a href="https://en.wikipedia.org/wiki/Polynomial_ring"><b><u>polynomial ring</u></b></a>. This means the entries of our matrices will be polynomials rather than integers. We need to define what it means to add, subtract, and multiply polynomials, but once we've done that, everything else about the encryption scheme is the same.</p>
	<p>In fact, you probably learned polynomial arithmetic in grade school. The only thing you might not be familiar with is polynomial modular reduction. To multiply two polynomials $f(X)$ and $g(X)$, we start by multiplying $f(X)\cdot g(X)$ as usual. Then we're going to <b>divide </b>$f(X)\cdot g(X)$ by some special polynomial — ML-KEM uses $X^{256}+1$ — and take the remainder. We won't try to explain this algorithm, but the takeaway is that the result is a polynomial with $256$ coefficients, each of which is an integer in <code>range(Q)</code>.</p>
	<p>The main advantage of using a polynomial ring for arithmetic is that we can pack more bits into the ciphertext. Our formula for the ciphertext is exactly the same (<code>u=r.T*A + e2, v=r.T*t + e3 + m</code>), but this time the plaintext <code>m </code>encodes a polynomial. Each coefficient of the polynomial encodes a bit, and we'll handle decryption errors just as we did before, by splitting <code>range(Q)</code> into two chunks, one that encodes <code>1</code> and another that encodes <code>0</code>. This allows us to reliably encrypt 256 bits (32 bytes) per ciphertext.</p>
	<p>Another advantage of using polynomials is that it significantly reduces the dimension of the matrix without impacting security. Concretely, the most widely used variant of ML-KEM, ML-KEM-768, uses a <code>3</code>-by-<code>3</code> matrix <code>A</code>, so just <code>9 </code>polynomials in total. (Note that $256 \cdot 3 = 768$, hence the name "ML-KEM-768".) However, note that we have to be careful in how we choose the modulus: $X^{256}+1$ is special in that it does not exhibit any algebraic structure that is known to permit attacks.</p>
	<p>The choices of <code>Q=3329</code> for the coefficient modulus and $X^{256}+1$ for the polynomial modulus have one more benefit. They allow polynomial multiplication to be carried out using the <a href="https://eprint.iacr.org/2024/585"><u>NTT algorithm</u></a>, which massively reduces the number of multiplications and additions we have to perform. In fact, this optimization is a major reason why ML-KEM is sometimes faster in terms of CPU time than key exchange with elliptic curves.</p>
	<p>We won't get into how NTT works here, except to say that the algorithm will look familiar to you if you've ever implemented RSA. In both cases we use the <a href="https://en.wikipedia.org/wiki/Chinese_remainder_theorem"><u>Chinese Remainder Theorem</u></a> to split multiplication up into multiple, cheaper multiplications with smaller moduli.</p>
	<div class="flex anchor relative">
		<h4 id="from-public-key-encryption-to-ml-kem">From public key encryption to ML-KEM</h4>
		<a href="https://blog.cloudflare.com/#from-public-key-encryption-to-ml-kem" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>The last step to build ML-KEM is to make the scheme secure against chosen ciphertext attacks (CCA). Currently, it's only secure against chosen plaintext attacks (CPA), which basically means that the ciphertext leaks no information about the plaintext, regardless of the distribution of plaintexts. CCA security is stronger in that it gives the attacker access to <a href="https://en.wikipedia.org/wiki/Adaptive_chosen-ciphertext_attack#Practical_attacks"><u>decryptions of ciphertexts of its choosing</u></a>. (Of course, it's not allowed to decrypt the target ciphertext itself.) The specific transform used in ML-KEM results in a CCA-secure KEM ("Key-Encapsulation Mechanism").</p>
	<p>Chosen ciphertext attacks might seem a bit abstract, but in fact they formalize a realistic threat model for many applications of KEMs (and public key encryption for that matter). For example, suppose we use the scheme in a protocol in which the server authenticates itself to a client by proving it was able to decrypt a ciphertext generated by the client. In this kind of protocol, the server acts as a sort of "decryption oracle" in which its responses to clients depend on the secret key. Unless the scheme is CCA secure, this oracle can be abused by an attacker to leak information about the secret key over time, allowing it to eventually impersonate the server.</p>
	<p>ML-KEM incorporates several more optimizations to make it as fast and as compact as possible. For example, instead of generating a random matrix <code>A</code>, we can derive it from a random, 32-byte string (called a "seed") using a hash-based primitive called a XOF ("eXtendable Output Function"), in the case of ML-KEM this XOF is <a href="https://pycryptodome.readthedocs.io/en/latest/src/hash/shake128.html"><u>SHAKE128</u></a>. This significantly reduces the size of the public key.</p>
	<p>Another interesting optimization is that the polynomial coefficients (integers in <code>range(Q)</code>) in the ciphertext are compressed by rounding off the least significant bits of each coefficient, thereby reducing the overall size of the ciphertext.</p>
	<p>All told, for the most widely deployed parameters (ML-KEM-768), the public key is 1184 bytes and the ciphertext is 1088 bytes. There's no obvious way to reduce this, except by reducing the size of the encapsulated key or the size of the public matrix <code>A</code>. The former would make ML-KEM useful for fewer applications, and the latter would reduce the security margin.</p>
	<p>Note that there are <a href="https://eprint.iacr.org/2022/031"><u>other lattice schemes</u></a> that are smaller, but they are based on different hardness assumptions and are still undergoing analysis.</p>
	<div class="flex anchor relative">
		<h3 id="authentication">Authentication</h3>
		<a href="https://blog.cloudflare.com/#authentication" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>In the previous section, we learned about ML-KEM, the algorithm already in use to make encryption PQ-secure. However, encryption is only one piece of the puzzle: establishing a secure connection also requires <b>authenticating</b> the server — and sometimes the client, depending on the application.</p>
	<p>Authentication is usually provided by a <b>digital signature scheme</b>, which uses a secret key to sign a message and a public key to verify the signature. The signature schemes used today aren't PQ-secure: a quantum computer can be used to compute the secret key corresponding to a server's public key, then use this key to impersonate the server.</p>
	<p>While this threat is less urgent than the threat to encryption, mitigating it is going to be more complicated. Over the years, we've bolted a number of signatures onto the <a href="https://www.cloudflare.com/learning/ssl/what-happens-in-a-tls-handshake">TLS handshake</a> in order to meet the evolving requirements of the web <a href="https://en.wikipedia.org/wiki/Public_key_infrastructure"><u>PKI</u></a>. We have PQ alternatives for these signatures, one of which we'll study in this section, but so far these signatures and their public keys are too large (i.e., take up too many bytes) to make comfortable replacements for today's schemes. Barring some breakthrough in <a href="https://blog.cloudflare.com/another-look-at-pq-signatures"><u>NIST's ongoing standardization effort</u></a>, we will have to re-engineer TLS and the web PKI to use fewer signatures.</p>
	<p>For now, let's dive into the PQ signature scheme we're likely to see deployed first: <a href="https://csrc.nist.gov/pubs/fips/204/final"><u>ML-DSA</u></a>, a.k.a. Dillithium. The design of ML-DSA follows a similar template as ML-KEM. We start by building some intermediate primitive, then we transform that primitive into the primitive we want, in this case a signature scheme.</p>
	<p>ML-DSA is quite a bit more involved than ML-KEM, so we're going to try to boil it down even further and just try to get across the main ideas.</p>
	<div class="flex anchor relative">
		<h4 id="warm-up">Warm up</h4>
		<a href="https://blog.cloudflare.com/#warm-up" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>Whereas ML-KEM is basically El Gamal encryption with elliptic curves replaced with lattices, ML-DSA is basically the <a href="https://www.zkdocs.com/docs/zkdocs/zero-knowledge-protocols/schnorr"><u>Schnorr identification protocol</u></a> with elliptic curves replaced with lattices. Schnorr's protocol is used by a <b>prover </b>to convince a <b>verifier</b> that it knows the secret key associated with its public key without revealing the secret key itself. The protocol has three moves and is executed with four algorithms:</p>
	<figure class="kg-card kg-image-card">
		<img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/7lV5fU3j9rRQ2j2Y4lpW8r/a12908c078ec46e0fa35ef674fcfdb07/image8.png" alt="" class="kg-image" width="1220" height="536" loading="lazy">
	</figure>
	<ol>
		<li>
			<p><code>initialize()</code>: The prover initializes the protocol and sends a <b>commitment</b> to the verifier</p>
		</li>
		<li>
			<p><code>challenge()</code>:&nbsp; The verifier receives the commitment and sends the prover a <b>challenge</b></p>
		</li>
		<li>
			<p><code>finish()</code>: The prover receives the challenge and sends the verifier the <b>proof</b>&nbsp;</p>
		</li>
		<li>
			<p><code>verify()</code>:&nbsp; Finally, the verifier uses the proof to decide whether the prover knows the secret key</p>
		</li>
	</ol>
	<p>We get the high-level structure of ML-DSA by making this protocol non-interactive. In particular, the prover derives the challenge itself by hashing the commitment together with the message to be signed. The signature consists of the commitment and proof: to verify the signature, the verifier recomputes the challenge from the commitment and message and runs <code>verify()</code>as usual.</p>
	<p>Let's jump right in to building Schnorr's identification protocol from lattices. If you've never seen this protocol before, then this will look a little like black magic at first. We'll go through it slowly enough to see how and why it works.</p>
	<p>Just like for ML-KEM, our public key is an LWE instance <code>(A,t=A*s1 + s2)</code>. However, this time our secret key is the <i>pair</i> of short vectors <code>(s1,s2)</code>, i.e., it includes the error term. Otherwise, key generation is exactly the same:</p>
	<pre class="language-Rust"><code class="language-Rust">def key_gen():
    A = gen_mat(N, N, 0, Q)
    s1 = gen_mat(N, 1, -beta, beta+1)
    s2 = gen_mat(N, 1, -beta, beta+1)
    t = (A*s1 + s2) % Q
    return ((A, t), (s1, s2))</code></pre>
	<p>To initialize the protocol, the prover generates another LWE instance <code>(A,w=A*y1 + y2)</code>. You'll see why in just a moment. The prover sends the <i>hash</i> of <code>w</code> as its commitment:</p>
	<pre class="language-Rust"><code class="language-Rust">def initialize(A):
    y1 = gen_mat(N, 1, -beta, beta+1)
    y2 = gen_mat(N, 1, -beta, beta+1)
    w = (A*y1 + y2) % Q
    return (H(w), (y1, y2))</code></pre>
	<p>Here <code>H </code>is some cryptographic hash function, like <a href="https://en.wikipedia.org/wiki/SHA-3"><u>SHA-3</u></a>. The prover stores the secret vectors<code> (y1,y2)</code> for use in its next move.</p>
	<p>Now it's time for the verifier's challenge. The challenge is just an integer, but we need to be careful about how we choose it. For now let's just pick it at random:</p>
	<pre class="language-Rust"><code class="language-Rust">def challenge():
    return random.randrange(0, Q)</code></pre>
	<p>Remember: when we turn this protocol into a digital signature, the challenge is derived from the commitment, <code>H(w)</code>, and the message. The range of this hash function must be the same as the set of outputs of <code>challenge()</code>.</p>
	<p>Now comes the fun part. The proof is a pair of vectors<code> (z1,z2)</code> satisfying <code>A*z1 + z2 == c*t + w</code>. We can easily produce this proof if we know the secret key:</p>
	<p> <code>z1 = (c*s1 + y1) % Q</code></p>
	<p> <code>z2 = (c*s2 + y2) % Q</code></p>
	<p>Then <code>A*z1 + z2 == A*(c*s1 + y1) + (c*s2 + y2) == c*(A*s1 + s2) + (A*y1 + y2) == c*t + w</code>. Our goal is to design the protocol such that it's hard to come up with <code>(z1,z2)</code> without knowing <code>(s1,s2)</code>, even after observing many executions of the protocol.</p>
	<p>Here are the <code>finish() </code>and <code>verify() </code>algorithms for completeness:</p>
	<pre class="language-Python"><code class="language-Python">def finish(s1, s2, y1, y2, c):
    z1 = (c*s1 + y1) % Q
    z2 = (c*s2 + y2) % Q
    return (z1, z2)

def verify(A, t, hw, c, z1, z2):
	return H((A*z1 + z2 - c*t) % Q) == hw

# Test
((A, t), (s1, s2)) = key_gen()
(hw, (y1, y2)) = initialize(A)        # hw: prover -&gt; verifier
c = challenge()                       # c: verifier -&gt; prover
(z1, z2) = finish(s1, s2, y1, y2, c)  # (z1, z2): prover -&gt; verifier
assert verify(A, t, hw, c, z1, z2)    # verifier</code></pre>
	<p>Notice that the verifier doesn't actually check <code>A*z1 + z2 == c*t + w</code> directly; we have to rearrange the equation so that we can set the commitment to <code>H(w)</code> rather than <code>w</code>. We'll explain the need for hashing in the next section.</p>
	<div class="flex anchor relative">
		<h4 id="making-this-scheme-secure">Making this scheme secure</h4>
		<a href="https://blog.cloudflare.com/#making-this-scheme-secure" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>The question of whether this protocol is secure boils down to whether it's possible to impersonate the prover without knowledge of the secret key. Let's put our attacker hat on and poke around.</p>
	<p>Perhaps there's a way to compute the secret key, either from the public key directly or by eavesdropping on executions of the protocol with the honest prover. If LWE is hard, then clearly there's no way we're going to extract the secret key from the public key <code>t</code>. Likewise, the commitment <code>H(w)</code>doesn't leak any information that would help us extract the secret key from the proof<code> (z1,z2)</code>.</p>
	<p>Let's take a closer look at the proof. Notice that the vectors <code>(y1,y2)</code> "mask" the secret key vectors, sort of how the shared secret masks the plaintext in ML-KEM. However, there's one big exception: we also scale the secret key vectors by the challenge <code>c</code>.</p>
	<p>What's the effect of scaling these vectors? If we squint at a few proofs, we start to see a pattern emerge. Let's look at <code>z1</code> first (<code>N=3, Q=3329, beta=4</code>):</p>
	<pre class="language-Python"><code class="language-Python">((A, t), (s1, s2)) = key_gen()
print('s1={}'.format(s1.T % Q))
for _ in range(10):
    (w, (y1, y2)) = initialize(A)
    c = challenge()
    (z1, z2) = finish(s1, s2, y1, y2, c)
    print('c={}, z1={}'.format(c, z1.T))
# s1=[[   1	0 3326]]
# c=1123, z1=[[1121 3327 3287]]
# c=1064, z1=[[1060	4  137]]
# c=1885, z1=[[1884 3327  999]]
# c=269, z1=[[ 270 3325 2524]]
# c=1506, z1=[[1510 3325 2141]]
# c=3147, z1=[[3149	4  547]]
# c=703, z1=[[ 700	4 1219]]
# c=1518, z1=[[1518 3327 2104]]
# c=1726, z1=[[1726	0 1478]]
# c=2591, z1=[[2589	4 2217]]</code></pre>
	<p>Indeed, with enough proof samples, we should be able to make a pretty good guess of the value of <code>s1</code>. In fact, for these parameters, there is a simple statistical analysis we can do to compute <code>s1</code> exactly. (Hint: <code>Q</code> is a prime number, which means <code>c*pow(c,-1,Q)==1 </code>whenever <code>c&gt;0</code>.) We can also apply this analysis to <code>s2</code>, or compute it directly from <code>t</code>, <code>s1</code>, and <code>A</code>.</p>
	<p>The main flaw in our protocol is that, although our secret vectors are short, scaling them makes them so long that they're not completely masked by<code> (y1,y2)</code>. Since <code>c</code> spans the entire <code>range(Q)</code>, so do the entries of <code>c*s1.</code> and <code>c*s2,</code> which means in order to mask these entries, we need the entries of <code>(y1,y2) </code>to span <code>range(Q)</code> as well. However, doing this would make solving LWE for <code>(A,w)</code> easy, by solving SIS. We somehow need to strike a balance between the length of the vectors of our LWE instances and the leakage induced by the challenge.</p>
	<p>Here's where things get tricky. Let's refer to the set of possible outputs of <code>challenge() </code>as the <b>challenge space. </b>We need the challenge space to be fairly large, large enough that the probability of outputting the same challenge twice is negligible.</p>
	<p>Why would such a collision be a problem? It's a little easier to see in the context of digital signatures. Let's say an attacker knows a valid signature for a message <code>m</code>. The signature includes the commitment <code>H(m)</code>, so the attacker also knows the challenge is <code>c == H(H(w),m)</code>. Suppose it manages to find a different message <code>m</code><code><sup>*</sup></code> for which <code>c == H(H(w),m</code><code><sup>*</sup></code><code>)</code>. Then the signature is also valid for m! And this attack is easy to pull off if the challenge space, that is, the set of possible outputs of <code>H</code>, is too small.</p>
	<p>Unfortunately, we can't make the challenge space larger simply by increasing the size of the modulus <code>Q</code>: the larger the challenge might be, the more information we'd leak about the secret key. We need a new idea.</p>
	<div class="flex anchor relative">
		<h5 id="the-best-of-both-worlds">The best of both worlds</h5>
		<a href="https://blog.cloudflare.com/#the-best-of-both-worlds" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>Remember that the hardness of LWE depends on the ratio between <code>beta</code> and <code>Q</code>. This means that <code>y1</code> and <code>y2</code> don't need to be short in absolute terms, but short relative to random vectors.</p>
	<p>With that in mind, consider the following idea. Let's take a larger modulus, say <code>Q=2**31 - 1</code>, and we'll continue to sample from the same challenge space, <code>range(2**16)</code>.</p>
	<p>First, notice that <code>z1</code> is now "relatively" short, since its entries are now in <code>range(-gamma, gamma+1)</code>, where <code>gamma = beta*(2**16-1),</code> rather than uniform over <code>range(Q)</code>. Let's also modify <code>initialize()</code> to sample the entries of <code>(y1,y2)</code> from the same range and see what happens:</p>
	<pre class="language-Python"><code class="language-Python">def initialize(A):
	y1 = gen_mat(N, 1, -gamma, gamma+1)
	y2 = gen_mat(N, 1, -gamma, gamma+1)
	w = (A*y1 + y2) % Q
	return (H(w), (y1, y2))

((A, t), (s1, s2)) = key_gen()
print('s1={}'.format(s1.T % Q))
for _ in range(10):
    (w, (y1, y2)) = initialize(A)
    c = challenge()
    (z1, z2) = finish(s1, s2, y1, y2, c)
    print('c={}, z1={}'.format(c, z1.T))
# s1=[[3 0 1]]
# c=31476, z1=[[175933 141954  93186]]
# c=27360, z1=[[    136404 2147438807     283758]]
# c=33536, z1=[[2147430945 2147377022     190671]]
# c=23283, z1=[[186516  73400   4955]]
# c=24756, z1=[[    328377 2147438906 2147388768]]
# c=12428, z1=[[2147340715     188675      90282]]
# c=24266, z1=[[    175498 2147261581 2147301553]]
# c=45331, z1=[[357595 185269 177155]]
# c=45641, z1=[[     21592 2147249191 2147446200]]
# c=57893, z1=[[297750 113335 144894]]</code></pre>
	<p>This is definitely going in the right direction, since there are no obvious correlations between <code>z1</code> and <code>s1</code>. (Likewise for <code>z2</code> and <code>s2</code>.) However, we're not quite there.</p>
	<p>One problem is that the challenge space is still quite small. With only <code>2**16 </code>challenges to choose from, we're likely to see a collision even after only a handful of protocol executions. We need the challenge space to be much, much larger, say around <code>2**256</code>. But then <code>Q</code> has to be an insanely large number in order for the <code>beta</code> to <code>Q</code> ratio to be secure.</p>
	<p>ML-DSA is able to side step this problem due to its use of arithmetic over polynomial rings. It uses the same modulus polynomial as ML-KEM, so the challenge is a polynomial with 256 coefficients. The coefficients are chosen carefully so that the challenge space is large, but multiplication by the challenge scales the secret vector by a small amount. Note that we still end up using a slightly larger modulus (<code>Q=8380417</code>) for ML-DSA than for ML-KEM, but only by about twelve bits.</p>
	<p>However, there is a more fundamental problem here, which is that we haven't completely ruled out that signatures may leak information about the secret key.</p>
	<div class="flex anchor relative">
		<h5 id="cause-and-effect">Cause and effect</h5>
		<a href="https://blog.cloudflare.com/#cause-and-effect" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>Suppose we run the protocol a number of times, and in each run, we happen to choose a relatively small value for some entry of <code>y1</code>. After enough runs, this would eventually allow us to reconstruct the corresponding entry of <code>s1</code>. To rule this out as a possibility, we need to make <code>y1 </code>even longer. (Likewise for <code>y2</code>.) But how long?</p>
	<p>Suppose we know that the entries of <code>z1</code> and <code>z2</code> are always in <code>range(-beta_loose,beta_loose+1) </code>for some <code>beta_loose &gt; beta</code>. Then we can <b>simulate </b>an honest run of the protocol as follows:</p>
	<pre class="language-Rust"><code class="language-Rust">def simulate(A, t):
    z1 = gen_mat(N, 1, -beta_loose, beta_loose+1)
    z2 = gen_mat(N, 1, -beta_loose, beta_loose+1)
    c = challenge()
    w = (A*z1 + z2 - c*t) % Q
    return (H(w), c, (z1, z2))

# Test
((A, t), (s1, s2)) = key_gen()
(hw, c, (z1, z2)) = simulate(A, t)
assert verify(A, t, hw, c, z1, z2)</code></pre>
	<p>This procedure perfectly simulates honest runs of the protocol, in the sense that the output of <code>simulate() </code>is indistinguishable from the transcript of a real run of the protocol with the honest prover. To see this, notice that the <code>w</code>,<code> c</code>, <code>z1</code>, and <code>z2 </code>all have the same mathematical relationship (the verification equation still holds) and have the same distribution.</p>
	<p>And here's the punch line: since this procedure doesn't use the secret key, it follows that the attacker learns nothing from eavesdropping on the honest prover that it can't compute from the public key itself. Pretty neat!</p>
	<p>What's left to do is arrange for <code>z1</code> and <code>z2</code> to fall in this range. First, we modify <code>initialize() </code>by increasing the range of <code>y1 </code>and<code> y2 </code>by<code> beta_loose</code>:</p>
	<pre class="language-Python"><code class="language-Python">def initialize(A):
    y1 = gen_mat(N, 1, -gamma+beta_loose, gamma+beta_loose+1)
    y2 = gen_mat(N, 1, -gamma+beta_loose, gamma+beta_loose+1)
    w = (A*y1 + y2) % Q
    return (H(w), (y1, y2))</code></pre>
	<p>This ensures the proof vectors <code>z1</code> and <code>z2 </code>are roughly uniform over r<code>ange(-beta_loose, beta_loose+1)</code>. However, they may fall slightly outside of this range, so need to modify <code>finalize() </code>to <b>abort </b>if not. Correspondingly, <code>verify() </code>should reject proof vectors that are out of range:</p>
	<pre class="language-Python"><code class="language-Python">def finish(s1, s2, y1, y2, c):
    z1 = (c*s1 + y1) % Q
    z2 = (c*s2 + y2) % Q
    if not in_range(z1, beta_loose) or not in_range(z2, beta_loose):
        return (None, None)
    return (z1, z2)

def verify(A, t, hw, c, z1, z2):
    if not in_range(z1, beta_loose) or not in_range(z2, beta_loose):
        return False
    return H((A*z1 + z2 - c*t) % Q) == hw</code></pre>
	<p>If <code>finish()</code> returns <code>(None,None)</code>, then the prover and verifier are meant to abort the protocol and retry until the protocol succeeds:</p>
	<pre class="language-Python"><code class="language-Python">((A, t), (s1, s2)) = key_gen()
while True:
    (hw, (y1, y2)) = initialize(A)        # hw: prover -&gt; verifier
    c = challenge()                       # c: verifier -&gt; prover
    (z1, z2) = finish(s1, s2, y1, y2, c)  # (z1, z2): prover -&gt; verifier
    if z1 is not None and z2 is not None:
        break
assert verify(A, t, hw, c, z1, z2)</code></pre>
	<p>Interestingly, we should expect aborts to be quite common. The parameters of ML-DSA are tuned so that the protocol runs five times on average before it succeeds.</p>
	<p>Another interesting point is that the security proof requires us to simulate not only successful protocol runs, but aborted protocol runs as well. More specifically, the protocol simulator must abort with the same probability as the real protocol, which implies that the rejection probability is <i>independent </i>of the secret key.</p>
	<p>The simulator also needs to be able to produce realistic looking commitments for aborted transcripts. This is exactly why the prover commits to the <i>hash</i> of <code>w </code>rather than <code>w </code> itself: in the security proof, we can easily simulate hashes of random inputs.</p>
	<div class="flex anchor relative">
		<h4 id="making-this-scheme-efficient">Making this scheme efficient</h4>
		<a href="https://blog.cloudflare.com/#making-this-scheme-efficient" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>ML-DSA benefits from many of the same optimizations as ML-KEM, including using polynomial rings, NTT for polynomial multiplication, and encoding polynomials with a fixed number of bits. However, ML-DSA has a few more tricks to make things smaller.</p>
	<p>First, in ML-DSA, instead of the pair of short vectors <code>z1 </code>and <code>z2</code>, the proof consists of a single vector <code>z=c*s1 + y,</code> where <code>y</code> was committed to in the previous step. In turn, we only end up with a single proof vector <code>z</code> rather than two as before. Getting this to work requires a special encoding of the commitment so that we can't compute <code>y</code> from it. ML-DSA uses a related trick to reduce the size of the <code>t</code> vector of the public key, but the details are more complicated.</p>
	<p>For the parameters we expect to deploy first (ML-DSA-44), the public key is 1312 bytes long and the signature is a whopping 2420 bytes. In contrast to ML-KEM, it is possible to shave off some more bytes. This does not come for free and requires complicating the scheme. An example is <a href="https://eprint.iacr.org/2023/624.pdf"><u>HAETAE</u></a>, which changes the distributions used. <a href="https://github.com/pornin/rust-fn-dsa"><u>Falcon</u></a> takes it a step further with even smaller signatures, using a completely different approach, which although elegant is also more complex to implement.</p>
	<div class="flex anchor relative">
		<h3 id="wrap-up">Wrap up</h3>
		<a href="https://blog.cloudflare.com/#wrap-up" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>Lattice cryptography underpins the first generation of PQ algorithms to get widely deployed on the Internet. ML-KEM is already widely used today to protect encryption from quantum computers, and in the coming years we expect to see ML-DSA deployed to get ahead of the threat of quantum computers to authentication.</p>
	<p>Lattices are also the basis of a new frontier for cryptography: computing on encrypted data.</p>
	<p>Suppose you wanted to aggregate some metrics submitted by clients without learning the metrics themselves. <a href="https://eprint.iacr.org/2024/936"><u>With LWE-based encryption</u></a>, you can arrange for each client to encrypt their metrics before submission, aggregate the ciphertexts, then decrypt to get the aggregate.</p>
	<p>Suppose instead that a server has a database that it wants to provide clients access to without revealing to the server which rows of the database the client wants to query. <a href="https://eprint.iacr.org/2022/949"><u>LWE-based encryption</u></a> allows the database to be encoded in a manner that permits encrypted queries.</p>
	<p>These applications are special cases of a paradigm known as <a href="https://en.wikipedia.org/wiki/Homomorphic_encryption"><u>FHE</u></a> ("Fully Homomorphic Encryption"), which allows for arbitrary computations on encrypted data. FHE is an extremely powerful primitive, and the only way we know how to build it today is with lattices. However, for most applications, FHE is far less practical than a special-purpose protocol would be (lattice-based or not). Still, over the years we've seen FHE get better and better, and for many applications it is already a decent option. Perhaps we'll dig into this and other lattice schemes in a future blog post.</p>
	<p>We hope you enjoyed this whirlwind tour of lattices. Thanks for reading!</p>
</div>