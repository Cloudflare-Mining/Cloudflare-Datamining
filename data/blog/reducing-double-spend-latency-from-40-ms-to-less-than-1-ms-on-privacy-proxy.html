<div class="mb2 gray5">8 min read</div><img class="mr2" src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/YfNgPROBiBht4vDzxfXiF/1cd01ce1a6a9d5dd5600073ae07e9cce/hero.png" alt="">
<div class="post-content lh-copy gray1">
	<p>One of Cloudflare’s big focus areas is making the Internet faster for end users. Part of the way we do that is by looking at the "big rocks" or bottlenecks that might be slowing things down — particularly processes on the critical path. When we recently turned our attention to our privacy proxy product, we found a big opportunity for improvement.</p>
	<p>What is our privacy proxy product? These proxies let users browse the web without exposing their personal information to the websites they’re visiting. Cloudflare runs infrastructure for privacy proxies like <a href="https://blog.cloudflare.com/icloud-private-relay"><u>Apple’s Private Relay</u></a> and <a href="https://blog.cloudflare.com/cloudflare-now-powering-microsoft-edge-secure-network"><u>Microsoft’s Edge Secure Network</u></a>.</p>
	<p>Like any secure infrastructure, we make sure that users authenticate to these privacy proxies before we open up a connection to the website they’re visiting. In order to do this in a privacy-preserving way (so that Cloudflare collects the least possible information about end-users) we use an open Internet standard – <a href="https://www.rfc-editor.org/rfc/rfc9578.html">Privacy Pass </a>–&nbsp;to issue tokens that authenticate to our proxy service.</p>
	<p>Every time a user visits a website via our Privacy Proxy, we check the validity of the Privacy Pass token which is included in the Proxy-Authorization header in their request. Before we cryptographically validate a user's token, we check if this token has already been spent. If the token is unspent, we let the user request through. Otherwise, it’s a "double-spend". From an access control perspective, double-spends are indicative of a problem. From a privacy perspective, double-spends can reduce the anonymity set and privacy characteristics. From a performance perspective, our privacy proxies see millions of requests per second – and any time spent authenticating delays people from accessing sites – so the check needs to be fast. Let’s see how we reduced the latency of these double-spend checks from ~40 ms to &lt;1 ms.</p>
	<div class="flex anchor relative">
		<h2 id="how-did-we-discover-the-issue">How did we discover the issue?</h2>
		<a href="https://blog.cloudflare.com/#how-did-we-discover-the-issue" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>We use a tracing platform, <a href="https://www.jaegertracing.io"><u>Jaeger</u></a>. It lets us see which paths our code took and how long functions took to run. When we looked into these traces, we saw latencies of ~ 40 ms. It was a good lead, but it alone was not enough to conclude it was an issue. The reason was we only sample a small percentage of our traces, so what we saw was not the whole picture. We needed to look at more data. We could’ve increased how many traces we sampled, but traces are large and heavy for our systems to process. Metrics are a lighter weight solution. We added metrics to get data on all double-spend checks.</p>
	<figure class="kg-card kg-image-card">
		<img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/67v4incoE8gXu22EBSLnN0/3c5fbd6b44ccc25398c905889b61c05e/image4.png" alt="" class="kg-image" width="1999" height="549" loading="lazy">
	</figure>
	<p>The lines in this graph are median latencies we saw for the slowest privacy proxies around the world. The metrics data gave us confidence that it was a problem affecting a large portion of requests… assuming that ~ 45 ms was longer than expected. But, was it expected? What numbers did we expect?</p>
	<div class="flex anchor relative">
		<h2 id="the-expected-latency">The expected latency</h2>
		<a href="https://blog.cloudflare.com/#the-expected-latency" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>To understand what times are reasonable to expect, let’s go into detail on what makes up a “double-spend check”. When we do a double-spend check, we ask a backing data store if a Privacy Pass token exists. The data store we use is <a href="https://memcached.org"><code><u>memcached</u></code></a>. We have many <code>memcached</code> instances running on servers around the world, so which server do we ask? For this, we use <a href="https://github.com/facebook/mcrouter"><code><u>mcrouter</u></code></a>. Instead of figuring out which <code>memcached</code> server to ask, we give our request to <code>mcrouter</code>, and it will handle choosing a good <code>memcached</code> server to use. We looked at the median time it took for <code>mcrouter</code> to process our request. This graph shows the average latencies per server over time. There are spikes, but most of the time the latency is &lt; 1 ms.&nbsp;</p>
	<figure class="kg-card kg-image-card">
		<img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/7LHxvtd813oeu1DyFh7MOA/0126ceb6212b50e8deeeffabba57e3e5/image1.png" alt="" class="kg-image" width="1032" height="468" loading="lazy">
	</figure>
	<p>By this point, we were confident that double-spend check latencies were longer than expected everywhere, and we started looking for the root cause.</p>
	<div class="flex anchor relative">
		<h2 id="how-did-we-investigate-the-issue">How did we investigate the issue?</h2>
		<a href="https://blog.cloudflare.com/#how-did-we-investigate-the-issue" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>We took inspiration from the scientific method. We analyzed our code, created theories for why sections of code caused latency, and used data to reject those theories. For any remaining theories, we implemented fixes and tested if they worked.</p>
	<p>Let’s look at the code. At a high level, the double-spend checking logic is:</p>
	<ol>
		<li>
			<p>Get a connection, which can be broken down into:</p>
			<ol>
				<li>
					<p>Send a <code>memcached version</code> command. This serves as a health check for whether the connection is still good to send data on.</p>
				</li>
				<li>
					<p>If the connection is still good, acquire it. Otherwise, establish a new connection.</p>
				</li>
			</ol>
		</li>
		<li>
			<p>Send a <code>memcached get</code> command on the connection.</p>
		</li>
	</ol>
	<p>Let’s go through the theories we had for each step listed above.</p>
	<div class="flex anchor relative">
		<h2 id="theory-1-health-check-takes-long">Theory 1: health check takes long</h2>
		<a href="https://blog.cloudflare.com/#theory-1-health-check-takes-long" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>We measured the health check primarily as a sanity check. The version command is simple and fast to process, so it should not take long. And we remained sane. The median latency was &lt; 1 ms.</p>
	<figure class="kg-card kg-image-card">
		<img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/6amdAWUKl3IvmGlvgwJhMP/57b6895aacf960b08ffc7d36d4569d25/image5.png" alt="" class="kg-image" width="1999" height="290" loading="lazy">
	</figure>
	<div class="flex anchor relative">
		<h2 id="theory-2-waiting-to-get-a-connection">Theory 2: waiting to get a connection</h2>
		<a href="https://blog.cloudflare.com/#theory-2-waiting-to-get-a-connection" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>To understand why we may need to wait to get a connection, let’s go into more detail on how we get a connection. In our code, we use a connection pool. The pool is a set of ready-to-go connections to <code>mcrouter</code>. The benefit of having a pool is that we do not have to pay the overhead of establishing a connection every time we want to make a request. Pools have a size limit, though. Our limit was 20 per server, and this is where a potential problem lies. Imagine we have a server that processes 5,000 requests every second, and requests stay for 45 ms. We can use something called <a href="https://en.wikipedia.org/wiki/Little%27s_law"><u>Little’s Law</u></a> to estimate the average number of requests in our system: <code>5000 x 0.045 = 225</code>. Due to our pool size limits, we can only have 20 connections at a time, so we can only process 20 requests at any point in time. That means 205 requests are just waiting! When we do a double-spend check, maybe we’re waiting ~ 40 ms to get a connection?</p>
	<p>We looked at the metrics of many different servers. No matter what the requests per second was, the latency was consistently ~ 40 ms, disproving the theory. For example, this graph shows data from a server that saw a maximum of 20 requests per second. It shows a histogram over time, and the large majority of requests fall in the 40 - 50 ms bucket.</p>
	<figure class="kg-card kg-image-card">
		<img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/1EJ7SlTzqMVLTIOTvqH1HL/7d64c441e606ecbe1823585f4ff19086/image7.png" alt="" class="kg-image" width="1999" height="330" loading="lazy">
	</figure>
	<div class="flex anchor relative">
		<h2 id="theory-3-delays-in-nagles-algorithm-and-delayed-acks">Theory 3: delays in Nagle’s algorithm and delayed acks</h2>
		<a href="https://blog.cloudflare.com/#theory-3-delays-in-nagles-algorithm-and-delayed-acks" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>We decided to chat with Gemini, giving it the observations we had so far. It suggested many things, but the most interesting was to check if <code>TCP_NODELAY</code> was set. If we had set this option in our code, it would’ve disabled something called <a href="https://en.wikipedia.org/wiki/Nagle%27s_algorithm"><u>Nagle’s algorithm</u></a>. Nagle’s algorithm itself was not a problem, but when enabled alongside another feature, <a href="https://en.wikipedia.org/wiki/TCP_delayed_acknowledgment"><u>delayed ACKs</u></a>, latencies could creep in. To explain why, let’s go through an analogy.</p>
	<p>Suppose we run a group chat app. Normally, people type a full thought and send it in one message. But, we have a friend who sends one word at a time: "Hi". Send. "how". Send. "are". Send. “you”. Send. That’s a lot of notifications. Nagle’s algorithm aims to prevent this. Nagle says that if the friend wants to send one short message, that’s fine, but it only lets them do it once per turn. When they try to send more single words right after, Nagle will save the words in a draft message. Once the draft message hits a certain length, Nagle sends. But what if the draft message never hits that length? To manage this, delayed ACKs initiates a 40 ms timer whenever the friend sends a message. If the app gets no further input before the timer ends, the message is sent to the group.</p>
	<p>I took a closer look at the code, both Cloudflare authored code and code from dependencies we rely on. We depended on the <a href="https://crates.io/crates/memcache-async"><code>memcache-async</code></a> crate for implementing the code that lets us send <code>memcache</code> commands. Here is the code for sending a <code>memcached version</code> command:</p>
	<pre class="language-Rust"><code class="language-Rust">self.io.write_all(b"version\r\n").await?;
self.io.flush().await?;</code></pre>
	<p>Nothing out of the ordinary. Then, we looked inside the get function.</p>
	<pre class="language-Rust"><code class="language-Rust">let writer = self.io.get_mut();
writer.write_all(b"get ").await?;
writer.write_all(key.as_ref()).await?;
writer.write_all(b"\r\n").await?;
writer.flush().await?;</code></pre>
	<p>In our code, we set <code>io</code> as a <code>TcpStream</code>, meaning that each <code>write_all</code> call resulted in sending a message. With Nagle’s algorithm enabled, the data flow looked like this:</p>
	<figure class="kg-card kg-image-card">
		<img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/6Vj6xfkbnIg2gmPeLy9g9I/2b003d8a4d81782148697fc83e793c6f/Screenshot_2025-07-24_at_13.16.05.png" alt="" class="kg-image" width="1474" height="724" loading="lazy">
	</figure>
	<p>Oof. We tried to send all three small messages, but after we sent the “get “, the kernel put the token and <code>\r\n</code> in a buffer and started waiting. When <code>mcrouter</code> got the “get “, it could not do anything because it did not have the full command. So, it waited 40 ms. Then, it sent an ACK in response. We got the ACK, and sent the rest of the command in the buffer. <code>mcrouter</code> got the rest of the command, processed it, and returned a response telling us if the token exists. What would the data flow look like with Nagle’s algorithm disabled?</p>
	<figure class="kg-card kg-image-card">
		<img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/4O0s8hb64olT2PDDc5wTFL/3cfe500a1f235276502db9e608cef966/Screenshot_2025-07-24_at_13.17.11.png" alt="" class="kg-image" width="1424" height="604" loading="lazy">
	</figure>
	<p>We would send all three small messages. <code>mcrouter</code> would have the full command, and return a response immediately. No waiting, whatsoever.</p>
	<div class="flex anchor relative">
		<h2 id="why-40-ms">Why 40 ms?</h2>
		<a href="https://blog.cloudflare.com/#why-40-ms" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>Our Linux servers have minimum bounds for the delay. Here is a snippet of Linux source code that defines those bounds.</p>
	<pre class="language-C++"><code class="language-C++">#if HZ &gt;= 100
#define TCP_DELACK_MIN	((unsigned)(HZ/25))	/* minimal time to delay before sending an ACK */
#define TCP_ATO_MIN	((unsigned)(HZ/25))
#else
#define TCP_DELACK_MIN	4U
#define TCP_ATO_MIN	4U
#endif</code></pre>
	<p>The comment tells us that <code>TCP_DELACK_MIN</code> is the minimum time delayed ACKs will wait before sending an ACK. We spent some time digging through Cloudflare’s custom kernel settings and found this:</p>
	<pre class="language-Rust"><code class="language-Rust">CONFIG_HZ=1000</code></pre>
	<p><code>CONFIG_HZ</code> eventually propagates to <code>HZ</code> and results in a 40 ms delay. That's where the number comes from!</p>
	<div class="flex anchor relative">
		<h2 id="the-fix">The fix</h2>
		<a href="https://blog.cloudflare.com/#the-fix" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>We were sending three separate messages for a single command when we only needed to send one. We captured what a <code>get</code> command looked like in Wireshark to verify we were sending three separate messages. (We captured this locally on MacOS. Interestingly, we got an ACK for every message.)</p>
	<figure class="kg-card kg-image-card">
		<img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/4B2qC70Dpeu25dTOP4V2hj/3720d8012f7d452696ca6cbe265d366e/image9.png" alt="" class="kg-image" width="1999" height="267" loading="lazy">
	</figure>
	<p>The fix was to use <code>BufWriter&lt;TcpStream&gt;</code> so that <code>write_all</code> would buffer the small messages in a user-space memory buffer, and <code>flush</code> would send the entire <code>memcached</code> command in one message. The Wireshark capture looked much cleaner.</p>
	<figure class="kg-card kg-image-card">
		<img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/5S6R7qAIad9pjKIfQYTWbA/c7bfe663b707ba4653977319a02e5e07/image3.png" alt="" class="kg-image" width="1999" height="174" loading="lazy">
	</figure>
	<div class="flex anchor relative">
		<h2 id="conclusion">Conclusion</h2>
		<a href="https://blog.cloudflare.com/#conclusion" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>After deploying the fix to production, we saw the median double-spend check latency drop to expected values everywhere.</p>
	<figure class="kg-card kg-image-card">
		<img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/4kKCQFTw5wp0jEwdPcALb4/8425bcfe526c2eeb9570c7a98fc62c62/image8.png" alt="" class="kg-image" width="1999" height="411" loading="lazy">
	</figure>
	<p>Our investigation followed a systematic, data-driven approach. We began by using observability tools to confirm the problem's scale. From there, we formed testable hypotheses and used data to systematically disprove them. This process ultimately led us to a subtle interaction between Nagle’s algorithm and delayed ACKs, caused by how we made use of a third-party dependency.</p>
	<p>Ultimately, our mission is to help build a better Internet. Every millisecond saved contributes to a faster and more seamless, private browsing experience for end users. We're excited to have this rolled out and excited to continue to chase further performance improvements!</p>
</div>