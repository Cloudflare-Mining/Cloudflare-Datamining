{
	"initialReadingTime": "19",
	"locale": "en-us",
	"localesAvailable": [
		"zh-cn",
		"zh-tw",
		"fr-fr",
		"de-de",
		"ja-jp"
	],
	"post": {
		"authors": [
			{
				"name": "George Thomas",
				"slug": "george",
				"bio": "I am a systems Engineer who works at the San Francisco office. ",
				"profile_image": "https://cf-assets.www.cloudflare.com/slt3lc6tev37/7JkNNinN2jouiqfkQCe8np/506bd468d7b39ebee5a87d360e9ce742/george.png",
				"location": "San Francisco",
				"website": null,
				"twitter": "@georgeagain",
				"facebook": null
			}
		],
		"excerpt": "We want to give you a behind the scenes look at how we have evolved the core mechanism of our product–namely, exactly how it kicks in to queue traffic in response to spikes",
		"feature_image": "https://cf-assets.www.cloudflare.com/slt3lc6tev37/3bCUD8fhDDdX6B9Kg6FnFL/c33029d0eb939d6ba888a3fec2de733f/how-waiting-room-queues.png",
		"featured": false,
		"html": "<p></p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/308ogSo9qOQp2ng3ve7spq/0d5964a4355efc10476e7c953f1298e9/image3-7.png\" alt=\"How Waiting Room makes queueing decisions on Cloudflare's highly distributed network\" class=\"kg-image\" width=\"1999\" height=\"1125\" loading=\"lazy\"/>\n            \n            </figure><p>Almost three years ago, we <a href=\"/cloudflare-waiting-room/\">launched Cloudflare Waiting Room</a> to protect our customers’ sites from overwhelming spikes in legitimate traffic that could bring down their sites. Waiting Room gives customers control over user experience even in times of high traffic by placing excess traffic in a customizable, on-brand waiting room, dynamically admitting users as spots become available on their sites. Since the launch of Waiting Room, we’ve continued to expand its functionality based on customer feedback with features like <a href=\"/waiting-room-random-queueing-and-custom-web-mobile-apps/\">mobile app support</a>, <a href=\"/understand-the-impact-of-your-waiting-rooms-settings-with-waiting-room-analytics/\">analytics</a>, <a href=\"/waiting-room-bypass-rules/\">Waiting Room bypass rules</a>, and <a href=\"/tag/waiting-room/\">more</a>.</p><p>We love announcing new features and solving problems for our customers by expanding the capabilities of Waiting Room. But, today, we want to give you a behind the scenes look at how we have evolved the core mechanism of our product–namely, exactly how it kicks in to queue traffic in response to spikes.</p>\n          <div class=\"flex anchor relative\">\n            <h2 id=\"how-was-the-waiting-room-built-and-what-are-the-challenges\">How was the Waiting Room built, and what are the challenges?</h2>\n            <a href=\"#how-was-the-waiting-room-built-and-what-are-the-challenges\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n              <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n            </a>\n          </div>\n          <p>The diagram below shows a quick overview of where the Waiting room sits when a customer enables it for their website.</p>\n            <figure class=\"kg-card kg-image-card kg-width-wide\">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/e3jZQvH7ed5YAFbSYk5km/a32668534f89f772748d8a662433c2b2/Waiting-Room-overview.png\" alt=\"Waiting Room overview\" class=\"kg-image\" width=\"1600\" height=\"1127\" loading=\"lazy\"/>\n            \n            </figure><p>Waiting Room is built on <a href=\"https://workers.cloudflare.com/\">Workers</a> that runs across a global network of Cloudflare data centers. The requests to a customer’s website can go to many different Cloudflare data centers. To optimize for minimal <a href=\"https://www.cloudflare.com/learning/performance/glossary/what-is-latency/\">latency</a> and enhanced performance, these requests are routed to the data center with the most geographical proximity. When a new user makes a request to the host/path covered by the Waiting room, the waiting room worker decides whether to send the user to the origin or the waiting room. This decision is made by making use of the waiting room state which gives an idea of how many users are on the origin.</p><p>The waiting room state changes continuously based on the traffic around the world. This information can be stored in a central location or changes can get propagated around the world eventually. Storing this information in a central location can add significant latency to each request as the central location can be really far from where the request is originating from. So every data center works with its own waiting room state which is a snapshot of the traffic pattern for the website around the world available at that point in time. Before letting a user into the website, we do not want to wait for information from everywhere else in the world as that adds significant latency to the request. This is the reason why we chose not to have a central location but have a pipeline where changes in traffic get propagated eventually around the world.</p><p>This pipeline which aggregates the waiting room state in the background is built on Cloudflare <a href=\"/introducing-workers-durable-objects/\">Durable Objects</a>. In 2021, we wrote a blog talking about <a href=\"/building-waiting-room-on-workers-and-durable-objects/\">how the aggregation pipeline works</a> and the different design decisions we took there if you are interested. This pipeline ensures that every data center gets updated information about changes in traffic within a few seconds.</p><p>The Waiting room has to make a decision whether to send users to the website or queue them based on the state that it currently sees. This has to be done while making sure we queue at the right time so that the customer&#39;s website does not get overloaded. We also have to make sure we do not queue too early as we might be queueing for a falsely suspected spike in traffic. Being in a queue could cause some users to abandon going to the website. Waiting Room runs on every server in <a href=\"https://www.cloudflare.com/network/\">Cloudflare’s network</a>, which spans over 300 cities in more than 100 countries. We want to make sure, for every new user, the decision whether to go to the website or the queue is made with minimal latency. This is what makes the decision of when to queue a hard question for the waiting room. In this blog, we will cover how we approached that tradeoff. Our algorithm has evolved to decrease the false positives while continuing to respect the customer’s set limits.</p>\n          <div class=\"flex anchor relative\">\n            <h2 id=\"how-a-waiting-room-decides-when-to-queue-users\">How a waiting room decides when to queue users</h2>\n            <a href=\"#how-a-waiting-room-decides-when-to-queue-users\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n              <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n            </a>\n          </div>\n          <p>The most important factor that determines when your waiting room will start queuing is how you configured the traffic settings. There are two traffic limits that you will set when configuring a waiting room–<i>total active users</i> and <i>new users per minute</i>.The <i>total active users</i> is a target threshold for how many simultaneous users you want to allow on the pages covered by your waiting room. <i>New users per minute</i> defines the target threshold for the maximum rate of user influx to your website per minute. A sharp spike in either of these values might result in queuing. Another configuration that affects how we calculate the <i>total active users</i> is <i>session duration.</i> A user is considered active for <i>session duration</i> minutes since the request is made to any page covered by a waiting room.</p><p>The graph below is from one of our internal monitoring tools for a customer and shows a customer&#39;s traffic pattern for 2 days. This customer has set their limits, <i>new users per minute</i> and <i>total active users</i> to 200 and 200 respectively.</p>\n            <figure class=\"kg-card kg-image-card kg-width-wide\">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/1ldSHbmG07jQ9qRikiy9Yh/4a4e21e822003c509cb31be364519279/Screen-Shot-2023-09-11-at-10.30.21-AM.png\" alt=\"Customer traffic for 2 days between September 9th to 11th with 2 spikes in traffic\" class=\"kg-image\" width=\"1600\" height=\"927\" loading=\"lazy\"/>\n            \n            </figure><p>If you look at their traffic you can see that users were queued on September 11th around 11:45. At that point in time, the <i>total active users</i> was around 200. As the <i>total active users</i> ramped down (around 12:30)<i>,</i> the queued users progressed to 0. The queueing started again on September 11th around 15:00 when total active users got to 200. The users that were queued around this time ensured that the traffic going to the website is around the limits set by the customer.</p><p>Once a user gets access to the website, we give them an encrypted <a href=\"https://www.cloudflare.com/learning/privacy/what-are-cookies/\">cookie</a> which indicates they have already gained access. The contents of the cookie can look like this.</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">{  \n  &quot;bucketId&quot;: &quot;Mon, 11 Sep 2023 11:45:00 GMT&quot;,\n  &quot;lastCheckInTime&quot;: &quot;Mon, 11 Sep 2023 11:45:54 GMT&quot;,\n  &quot;acceptedAt&quot;: &quot;Mon, 11 Sep 2023 11:45:54 GMT&quot;\n}</pre></code>\n            <p>The cookie is like a ticket which indicates entry to the waiting room.The <i>bucketId</i> indicates which cluster of users this user is part of. The <i>acceptedAt</i> time and <i>lastCheckInTime</i> indicate when the last interaction with the workers was. This information can ensure if the ticket is valid for entry or not when we compare it with the <i>session duration</i> value that the customer sets while configuring the waiting room. If the cookie is valid, we let the user through which ensures users who are on the website continue to be able to browse the website. If the cookie is invalid, we create a new cookie treating the user as a new user and if there is queueing happening on the website they get to the back of the queue. In the next section let us see how we decide when to queue those users.</p><p>To understand this further, let&#39;s see what the contents of the waiting room state are. For the customer we discussed above, at the time &quot;Mon, 11 Sep 2023 11:45:54 GMT&quot;, the state could look like this.</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">{  \n  &quot;activeUsers&quot;: 50,\n}</pre></code>\n            <p>As mentioned above the customer’s configuration has <i>new users per minute</i> and t_otal active users_ equal to 200 and 200 respectively.</p><p>So the state indicates that there is space for the new users as there are only 50 active users when it&#39;s possible to have 200. So there is space for another 150 users to go in. Let&#39;s assume those 50 users could have come from two data centers San Jose (20 users) and London (30 users). We also keep track of the number of workers that are active across the globe as well as the number of workers active at the data center in which the state is calculated. The state key below could be the one calculated at San Jose.</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">{  \n  &quot;activeUsers&quot;: 50,\n  &quot;globalWorkersActive&quot;: 10,\n  &quot;dataCenterWorkersActive&quot;: 3,\n  &quot;trafficHistory&quot;: {\n    &quot;Mon, 11 Sep 2023 11:44:00 GMT&quot;: {\n       San Jose: 20/200, // 10%\n       London: 30/200, // 15%\n       Anywhere: 150/200 // 75%\n    }\n  }\n}</pre></code>\n            <p>Imagine at the time &quot;<code>Mon, 11 Sep 2023 11:45:54 GMT</code>&quot;, we get a request to that waiting room at a datacenter in San Jose.</p><p>To see if the user that reached San Jose can go to the origin we first check the traffic history in the past minute to see the distribution of traffic at that time. This is because a lot of websites are popular in certain parts of the world. For a lot of these websites the traffic tends to come from the same data centers.</p><p>Looking at the traffic history for the minute &quot;<code>Mon, 11 Sep 2023 11:44:00 GMT</code>&quot; we see San Jose has 20 users out of 200 users going there (10%) at that time. For the current time &quot;<code>Mon, 11 Sep 2023 11:45:54 GMT</code>&quot; we divide the slots available at the website at the same ratio as the traffic history in the past minute. So we can send 10% of 150 slots available from San Jose which is 15 users. We also know that there are three active workers as &quot;<code>dataCenterWorkersActive</code>&quot; is <code>3</code>.</p><p>The number of slots available for the data center is divided evenly among the workers in the data center. So every worker in San Jose can send 15/3 users to the website. If the worker that received the traffic has not sent any users to the origin for the current minute they can send up to <i>five</i> users (15/3).</p><p>At the same time (&quot;<code>Mon, 11 Sep 2023 11:45:54 GMT</code>&quot;), imagine a request goes to a data center in Delhi. The worker at the data center in Delhi checks the trafficHistory and sees that there are no slots allotted for it. For traffic like this we have reserved the Anywhere slots as we are really far away from the limit.</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">{  \n  &quot;activeUsers&quot;:50,\n  &quot;globalWorkersActive&quot;: 10,\n  &quot;dataCenterWorkersActive&quot;: 1,\n  &quot;trafficHistory&quot;: {\n    &quot;Mon, 11 Sep 2023 11:44:00 GMT&quot;: {\n       San Jose: 20/200, // 10%\n       London: 30/200, // 15%\n       Anywhere: 150/200 // 75%\n    }\n  }\n}</pre></code>\n            <p>The <code>Anywhere</code> slots are divided among all the active workers in the globe as any worker around the world can take a part of this pie. 75% of the remaining 150 slots which is 113.</p><p>The state key also keeps track of the number of workers (<code>globalWorkersActive</code>) that have spawned around the world. The Anywhere slots allotted are divided among all the active workers in the world if available. <code>globalWorkersActive</code> is 10 when we look at the waiting room state. So every active worker can send as many as 113/10 which is approximately 11 users. So the first 11 users that come to a worker in the minute <code>Mon, 11 Sep 2023 11:45:00 GMT</code> gets admitted to the origin. The extra users get queued. The extra reserved slots (5) in San Jose for minute  <code>Mon, 11 Sep 2023 11:45:00 GMT</code> discussed before ensures that we can admit up to 16(5 + 11) users from a worker from San Jose to the website.</p>\n          <div class=\"flex anchor relative\">\n            <h2 id=\"queuing-at-the-worker-level-can-cause-users-to-get-queued-before-the-slots-available-for-the-data-center\">Queuing at the worker level can cause users to get queued before the slots available for the data center</h2>\n            <a href=\"#queuing-at-the-worker-level-can-cause-users-to-get-queued-before-the-slots-available-for-the-data-center\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n              <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n            </a>\n          </div>\n          <p>As we can see from the example above, we decide whether to queue or not at the worker level. The number of new users that go to workers around the world can be non-uniform. To understand what can happen when there is non-uniform distribution of traffic to two workers, let us look at the diagram below.</p>\n            <figure class=\"kg-card kg-image-card kg-width-wide\">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/2I3CYq6IbnjE7T1A4yEOC1/02cf9dcb4acaeca3bc68e2bca8343f3f/Side-effect-of-dividing-slots-at-worker-level.png\" alt=\"Side effect of dividing slots at worker level\" class=\"kg-image\" width=\"1600\" height=\"1013\" loading=\"lazy\"/>\n            \n            </figure><p>Imagine the slots available for a data center in San Jose are <i>ten</i>. There are two workers running in San Jose. <i>Seven</i> users go to worker1 and <i>one</i> user goes to worker2. In this situation worker1 will let in <i>five</i> out of the <i>seven</i> workers to the website and <i>two</i> of them get queued as worker1 only has <i>five</i> slots available. The <i>one</i> user that shows up at worker2 also gets to go to the origin. So we queue <i>two</i> users, when in reality <i>ten</i> users can get sent from the datacenter San Jose when only <i>eight</i> users show up.</p><p>This issue while dividing slots evenly among workers results in queueing before a waiting room’s configured traffic limits, typically within 20-30% of the limits set. This approach has advantages which we will discuss next. We have made changes to the approach to decrease the frequency with which queuing occurs outside that 20-30% range, queuing as close to limits as possible, while still ensuring Waiting Room is prepared to catch spikes. Later in this blog, we will cover how we achieved this by updating how we allocate and count slots.</p>\n          <div class=\"flex anchor relative\">\n            <h3 id=\"what-is-the-advantage-of-workers-making-these-decisions\">What is the advantage of workers making these decisions?</h3>\n            <a href=\"#what-is-the-advantage-of-workers-making-these-decisions\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n              <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n            </a>\n          </div>\n        <p>The example above talked about how a worker in San Jose and Delhi makes decisions to let users through to the origin. The advantage of making decisions at the worker level is that we can make decisions without any significant latency added to the request. This is because to make the decision, there is no need to leave the data center to get information about the waiting room as we are always working with the state that is currently available in the data center. The queueing starts when the slots run out within the worker. The lack of additional latency added enables the customers to turn on the waiting room all the time without worrying about extra latency to their users.</p><p>Waiting Room’s number one priority is to ensure that customer’s sites remain up and running at all times, even in the face of unexpected and overwhelming traffic surges. To that end, it is critical that a waiting room prioritizes staying near or below traffic limits set by the customer for that room. When a spike happens at one data center around the world, say at San Jose, the local state at the data center will take a few seconds to get to Delhi.</p><p>Splitting the slots among workers ensures that working with slightly outdated data does not cause the overall limit to be exceeded by an impactful amount. For example, the <code>activeUsers</code> value can be 26 in the San Jose data center and 100 in the other data center where the spike is happening. At that point in time, sending extra users from Delhi may not overshoot the overall limit by much as they only have a part of the pie to start with in Delhi. Therefore, queueing before overall limits are reached is part of the design to make sure your overall limits are respected. In the next section we will cover the approaches we implemented to queue as close to limits as possible without increasing the risk of exceeding traffic limits.</p>\n          <div class=\"flex anchor relative\">\n            <h2 id=\"allocating-more-slots-when-traffic-is-low-relative-to-waiting-room-limits\">Allocating more slots when traffic is low relative to waiting room limits</h2>\n            <a href=\"#allocating-more-slots-when-traffic-is-low-relative-to-waiting-room-limits\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n              <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n            </a>\n          </div>\n          <p>The first case we wanted to address was queuing that occurs when traffic is far from limits. While rare and typically lasting for one refresh interval (20s) for the end users who are queued, this was our first priority when updating our queuing algorithm. To solve this, while allocating slots we looked at the utilization (how far you are from traffic limits) and allotted more slots when traffic is really far away from the limits. The idea behind this was to prevent the queueing that happens at lower limits while still being able to readjust slots available per worker when there are more users on the origin.</p><p>To understand this let&#39;s revisit the example where there is non-uniform distribution of traffic to two workers. So two workers similar to the one we discussed before are shown below. In this case the utilization is low (10%). This means we are far from the limits. So the slots allocated(8) are closer to the <code>slotsAvailable</code> for the datacenter San Jose which is 10. As you can see in the diagram below, all the eight users that go to either worker get to reach the website with this modified slot allocation as we are providing more slots per worker at lower utilization levels.</p>\n            <figure class=\"kg-card kg-image-card kg-width-wide\">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/7zNp1o6DEAbGL4GSYjUkeJ/fa76b41beea6d244b474302c4c8d9364/Division-of-slots-among-workers-at-lower-utilization.png\" alt=\"Division of slots among workers at lower utilization\" class=\"kg-image\" width=\"1600\" height=\"1013\" loading=\"lazy\"/>\n            \n            </figure><p>The diagram below shows how the slots allocated per worker changes with utilization (how far you are away from limits). As you can see here, we are allocating more slots per worker at lower utilization. As the utilization increases, the slots allocated per worker decrease as it’s getting closer to the limits, and we are better prepared for spikes in traffic. At 10% utilization every worker gets close to the slots available for the data center. As the utilization is close to 100% it becomes close to the slots available divided by worker count in the data center.</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/2qy2IXaKqoER8zkDRL9mAZ/7a734f79cd402ef7105b6195dd13cf69/Alloting-more-slots-at-lower-limits.png\" alt=\"Allotting more slots at lower limits\" class=\"kg-image\" width=\"1600\" height=\"1480\" loading=\"lazy\"/>\n            \n            </figure>\n          <div class=\"flex anchor relative\">\n            <h3 id=\"how-do-we-achieve-more-slots-at-lower-utilization\">How do we achieve more slots at lower utilization?</h3>\n            <a href=\"#how-do-we-achieve-more-slots-at-lower-utilization\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n              <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n            </a>\n          </div>\n        <p>This section delves into the mathematics which helps us get there. If you are not interested in these details, meet us at the “Risk of over provisioning” section.</p><p>To understand this further, let&#39;s revisit the previous example where requests come to the Delhi data center. The <code>activeUsers</code> value is 50, so utilization is 50/200 which is around 25%.</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">{\n  &quot;activeUsers&quot;: 50,\n  &quot;globalWorkersActive&quot;: 10,\n  &quot;dataCenterWorkersActive&quot;: 1,\n  &quot;trafficHistory&quot;: {\n    &quot;Mon, 11 Sep 2023 11:44:00 GMT&quot;: {\n       San Jose: 20/200, // 10%\n       London: 30/200, // 15%\n       Anywhere: 150/200 // 75%\n    }\n  }\n}</pre></code>\n            <p>The idea is to allocate more slots at lower utilization levels. This ensures that customers do not see unexpected queueing behaviors when traffic is far away from limits. At time <code>Mon, 11 Sep 2023 11:45:54 GMT</code> requests to Delhi are at 25% utilization based on the local state key.</p><p>To allocate more slots to be available at lower utilization we added a <code>workerMultiplier</code> which moves proportionally to the utilization. At lower utilization the multiplier is lower and at higher utilization it is close to one.</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">workerMultiplier = (utilization)^curveFactor\nadaptedWorkerCount = actualWorkerCount * workerMultiplier</pre></code>\n            <p><code>utilization</code> - how far away from the limits you are.</p><p><code>curveFactor</code> - is the exponent which can be adjusted which decides how aggressive we are with the distribution of extra budgets at lower worker counts. To understand this let&#39;s look at the graph of how y = x and y = x^2 looks between values 0 and 1.</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/4wgTahQXjs2Heu9fW2jOgG/93f53a5d280e9ab9f194a87c906a343e/Graph-for-y-x-curveFactor.png\" alt=\"Graph for y=x^curveFactor\" class=\"kg-image\" width=\"1600\" height=\"1480\" loading=\"lazy\"/>\n            \n            </figure><p>The graph for y=x is a straight line passing through (0, 0) and (1, 1).</p><p>The graph for <code>y=x^2</code> is a curved line where y increases slower than <code>x</code> when <code>x &lt; 1</code> and passes through (0, 0) and (1, 1)</p><p>Using the concept of how the curves work, we derived the formula for <code>workerCountMultiplier</code> where <code><i>y=workerCountMultiplier</i></code><i>,</i> <code><i>x=utilization</i></code> and <code><i>curveFactor</i></code> is the power which can be adjusted which decides how aggressive we are with the distribution of extra budgets at lower worker counts. When <code>curveFactor</code> is 1, the <code>workerMultiplier</code> is equal to the utilization.</p><p>Let&#39;s come back to the example we discussed before and see what the value of the curve factor will be. At time <code>Mon, 11 Sep 2023 11:45:54 GMT</code> requests to Delhi are at 25% utilization based on the local state key. The Anywhere slots are divided among all the active workers in the globe as any worker around the world can take a part of this pie. i.e. 75% of the remaining 150 slots (113).</p><p><code>globalWorkersActive</code> is 10 when we look at the waiting room state. In this case we do not divide the 113 slots by 10 but instead divide by the adapted worker count which is <code>globalWorkersActive ***** workerMultiplier</code>. If <code>curveFactor</code> is <code>1</code>, the <code>workerMultiplier</code> is equal to the utilization which is at 25% or 0.25.</p><p>So effective <code>workerCount</code> = 10 * 0.25 = 2.5</p><p>So, every active worker can send as many as 113/2.5 which is approximately 45 users. The first 45 users that come to a worker in the minute <code>Mon, 11 Sep 2023 11:45:00 GMT</code> gets admitted to the origin. The extra users get queued.</p><p>Therefore, at lower utilization (when traffic is farther from the limits) each worker gets more slots. But, if the sum of slots are added up, there is a higher chance of exceeding the overall limit.</p>\n          <div class=\"flex anchor relative\">\n            <h3 id=\"risk-of-over-provisioning\">Risk of over provisioning</h3>\n            <a href=\"#risk-of-over-provisioning\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n              <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n            </a>\n          </div>\n        <p>The method of giving more slots at lower limits decreases the chances of queuing when traffic is low relative to traffic limits. However, at lower utilization levels a uniform spike happening around the world could cause more users to go into the origin than expected. The diagram below shows the case where this can be an issue. As you can see the slots available are <i>ten</i> for the data center. At 10% utilization we discussed before, each worker can have <i>eight</i> slots each. If <i>eight</i> users show up at one worker and <i>seven</i> show up at another, we will be sending <i>fifteen</i> users to the website when only <i>ten</i> are the maximum available slots for the data center.</p>\n            <figure class=\"kg-card kg-image-card kg-width-wide\">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/3aKrVwAvh40LjZGBcGl43R/8387284e8d9ce08d930e1d414a15e773/Risk-of-over-provisioning-at-lower-utilization.png\" alt=\"Risk of over provisioning at lower utilization\" class=\"kg-image\" width=\"1600\" height=\"1013\" loading=\"lazy\"/>\n            \n            </figure><p>With the range of customers and types of traffic we have, we were able to see cases where this became a problem. A traffic spike from low utilization levels could cause overshooting of the global limits. This is because we are over provisioned at lower limits and this increases the risk of significantly exceeding traffic limits. We needed to implement a safer approach which would not cause limits to be exceeded while also decreasing the chance of queueing when traffic is low relative to traffic limits.</p><p>Taking a step back and thinking about our approach, one of the assumptions we had was that the traffic in a data center directly correlates to the worker count that is found in a data center. In practice what we found is that this was not true for all customers. Even if the traffic correlates to the worker count, the new users going to the workers in the data centers may not correlate. This is because the slots we allocate are for new users but the traffic that a data center sees consists of both users who are already on the website and new users trying to go to the website.</p><p>In the next section we are talking about an approach where worker counts do not get used and instead workers communicate with other workers in the data center. For that we introduced a new service which is a durable object counter.</p>\n          <div class=\"flex anchor relative\">\n            <h2 id=\"decrease-the-number-of-times-we-divide-the-slots-by-introducing-data-center-counters\">Decrease the number of times we divide the slots by introducing Data Center Counters</h2>\n            <a href=\"#decrease-the-number-of-times-we-divide-the-slots-by-introducing-data-center-counters\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n              <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n            </a>\n          </div>\n          <p>From the example above, we can see that overprovisioning at the worker level has the risk of using up more slots than what is allotted for a data center. If we do not over provision at low levels we have the risk of queuing users way before their configured limits are reached which we discussed first. So there has to be a solution which can achieve both these things.</p><p>The overprovisioning was done so that the workers do not run out of slots quickly when an uneven number of new users reach a bunch of workers. If there is a way to communicate between two workers in a data center, we do not need to divide slots among workers in the data center based on worker count. For that communication to take place, we introduced counters. Counters are a bunch of small durable object instances that do counting for a set of workers in the data center.</p><p>To understand how it helps with avoiding usage of worker counts, let&#39;s check the diagram below. There are two workers talking to a <i>Data Center Counter</i> below. Just as we discussed before, the workers let users through to the website based on the waiting room state. The count of the number of users let through was stored in the memory of the worker before. By introducing counters, it is done in the <i>Data Center Counter</i>. Whenever a new user makes a request to the worker, the worker talks to the counter to know the current value of the counter. In the example below for the first new request to the worker the counter value received is 9. When a data center has 10 slots available, that will mean the user can go to the website. If the next worker receives a new user and makes a request just after that, it will get a value 10 and based on the slots available for the worker, the user will get queued.</p>\n            <figure class=\"kg-card kg-image-card kg-width-wide\">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/ttIdi7gwzmK4ENeqkIGO0/351fb10609f5818c43fa472cddba89eb/Counters-helping-workers-communicate-with-each-other.png\" alt=\"Counters helping workers communicate with each other\" class=\"kg-image\" width=\"1600\" height=\"1170\" loading=\"lazy\"/>\n            \n            </figure><p>The <i>Data Center Counter</i> acts as a point of synchronization for the workers in the waiting room. Essentially, this enables the workers to talk to each other without really talking to each other directly. This is similar to how a ticketing counter works. Whenever one worker lets someone in, they request tickets from the counter, so another worker requesting the tickets from the counter will not get the same ticket number. If the ticket value is valid, the new user gets to go to the website. So when different numbers of new users show up at workers, we will not over allocate or under allocate slots for the worker as the number of slots used is calculated by the counter which is for the data center.</p><p>The diagram below shows the behavior when an uneven number of new users reach the workers, one gets <i>seven</i> new users and the other worker gets <i>one</i> new user. All <i>eight</i> users that show up at the workers in the diagram below get to the website as the slots available for the data center is <i>ten</i> which is below <i>ten</i>.</p>\n            <figure class=\"kg-card kg-image-card kg-width-wide\">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/bais5OnxQpewoeNM2T81H/7f766d3d58a380f16af8ad93e5e815d3/Uneven-number-of-requests-to-Workers-does-not-cause-queueing.png\" alt=\"Uneven number of requests to workers does not cause queueing\" class=\"kg-image\" width=\"1600\" height=\"1013\" loading=\"lazy\"/>\n            \n            </figure><p>This also does not cause excess users to get sent to the website as we do not send extra users when the counter value equals the <code>slotsAvailable</code> for the data center. Out of the <i>fifteen</i> users that show up at the workers in the diagram below <i>ten</i> will get to the website and <i>five</i> will get queued which is what we would expect.</p>\n            <figure class=\"kg-card kg-image-card kg-width-wide\">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/7IF1Ze4p1TvZM9YUfCLyla/146ffc7044ed34ecdfebf08ef7b58a0b/Risk-of-over-provisioning-at-lower-utilization-also-does-not-exist-as-counters-help-Workers-communicate-with-each-other.png\" alt=\"\" class=\"kg-image\" width=\"1600\" height=\"1013\" loading=\"lazy\"/>\n            \n            </figure><p>Risk of over provisioning at lower utilization also does not exist as counters help workers to communicate with each other.</p><p>To understand this further, let&#39;s look at the previous example we talked about and see how it works with the actual waiting room state.</p><p>The waiting room state for the customer is as follows.</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">{  \n  &quot;activeUsers&quot;: 50,\n  &quot;globalWorkersActive&quot;: 10,\n  &quot;dataCenterWorkersActive&quot;: 3,\n  &quot;trafficHistory&quot;: {\n    &quot;Mon, 11 Sep 2023 11:44:00 GMT&quot;: {\n       San Jose: 20/200, // 10%\n       London: 30/200, // 15%\n       Anywhere: 150/200 // 75%\n    }\n  }\n}</pre></code>\n            <p>The objective is to not divide the slots among workers so that we don’t need to use that information from the state. At time <code>Mon, 11 Sep 2023 11:45:54 GMT</code> requests come to San Jose. So, we can send 10% of 150 slots available from San Jose which is 15.</p><p>The durable object counter at San Jose keeps returning the counter value it is at right now for every new user that reaches the data center. It will increment the value by 1 after it returns to a worker. So the first 15 new users that come to the worker get a unique counter value. If the value received for a user is less than 15 they get to use the slots at the data center.</p><p>Once the slots available for the data center runs out, the users can make use of the slots allocated for Anywhere data-centers as these are not reserved for any particular data center. Once a worker in San Jose gets a ticket value that says 15, it realizes that it&#39;s not possible to go to the website using the slots from San Jose.</p><p>The Anywhere slots are available for all the active workers in the globe i.e. 75% of the remaining 150 slots (113). The Anywhere slots are handled by a durable object that workers from different data centers can talk to when they want to use Anywhere slots. Even if 128 (113 + 15) users end up going to the same worker for this customer we will not queue them. This increases the ability of Waiting Room to handle an uneven number of new users going to workers around the world which in turn helps the customers to queue close to the configured limits.</p>\n          <div class=\"flex anchor relative\">\n            <h3 id=\"why-do-counters-work-well-for-us\">Why do counters work well for us?</h3>\n            <a href=\"#why-do-counters-work-well-for-us\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n              <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n            </a>\n          </div>\n        <p>When we built the Waiting Room, we wanted the decisions for entry into the website to be made at the worker level itself without talking to other services when the request is in flight to the website. We made that choice to avoid adding latency to user requests. By introducing a synchronization point at a durable object counter, we are deviating from that by introducing a call to a durable object counter.</p><p>However, the durable object for the data center stays within the same data center. This leads to minimal additional latency which is usually less than 10 ms. For the calls to the durable object that handles Anywhere data centers, the worker may have to cross oceans and long distances. This could cause the latency to be around 60 or 70 ms in those cases. The 95th percentile values shown below are higher because of calls that go to farther data centers.</p>\n            <figure class=\"kg-card kg-image-card kg-width-wide\">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/5LC6pdXV2rZxZAntvU36Bs/8e3772682ffd39eec8b7b1c93b87211b/1Dr9LuESmHPXU4nQlr_9WIncDSE9uXcbHA6qevspl.png\" alt=\"Graph showing percentile distribution of counter latencies from our production dashboard\" class=\"kg-image\" width=\"1600\" height=\"688\" loading=\"lazy\"/>\n            \n            </figure><p>The design decision to add counters adds a slight extra latency for new users going to the website. We deemed the trade-off acceptable because this reduces the number of users that get queued before limits are reached. In addition, the counters are only required when new users try to go into the website. Once new users get to the origin, they get entry directly from workers as the proof of entry is available in the cookies that the customers come with, and we can let them in based on that.</p><p>Counters are really simple services which do simple counting and do nothing else. This keeps the memory and CPU footprint of the counters minimal. Moreover, we have a lot of counters around the world handling the coordination between a subset of workers.This helps counters to successfully handle the load for the synchronization requirements from the workers. These factors add up to make counters a viable solution for our use case.</p>\n          <div class=\"flex anchor relative\">\n            <h2 id=\"summary\">Summary</h2>\n            <a href=\"#summary\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n              <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n            </a>\n          </div>\n          <p>Waiting Room was designed with our number one priority in mind–to ensure that our customers’ sites remain up and running, no matter the volume or ramp up of legitimate traffic. Waiting Room runs on every server in Cloudflare’s network, which spans over 300 cities in more than 100 countries. We want to make sure, for every new user, the decision whether to go to the website or the queue is made with minimal latency and is done at the right time. This decision is a hard one as queuing too early at a data center can cause us to queue earlier than the customer set limits. Queuing too late can cause us to overshoot the customer set limits.</p><p>With our initial approach where we divide slots among our workers evenly we were sometimes queuing too early but were pretty good at respecting customer set limits. Our next approach of giving more slots at low utilization (low traffic levels compared to customer limits) ensured that we did better at the cases where we queued earlier than the customer set limits as every worker has more slots to work with at each worker. But as we have seen, this made us more likely to overshoot when a sudden spike in traffic occurred after a period of low utilization.</p><p>With counters we are able to get the best of both worlds as we avoid the division of slots by worker counts. Using counters we are able to ensure that we do not queue too early or too late based on the customer set limits. This comes at the cost of a little bit of latency to every request from a new user which we have found to be negligible and creates a better user experience than getting queued early.</p><p>We keep iterating on our approach to make sure we are always queuing people at the right time and above all protecting your website. As more and more customers are using the waiting room, we are learning more about different types of traffic and that is helping the product be better for everyone.</p>",
		"id": "378OAooWIn9X080mpldET2",
		"localeList": {
			"name": "How Waiting Room makes queueing decisions on Cloudflare's highly distributed network Config",
			"enUS": "English for Locale",
			"zhCN": "Translated for Locale",
			"zhHansCN": "No Page for Locale",
			"zhTW": "Translated for Locale",
			"frFR": "Translated for Locale",
			"deDE": "Translated for Locale",
			"itIT": "No Page for Locale",
			"jaJP": "Translated for Locale",
			"koKR": "No Page for Locale",
			"ptBR": "No Page for Locale",
			"esLA": "No Page for Locale",
			"esES": "No Page for Locale",
			"enAU": "No Page for Locale",
			"enCA": "No Page for Locale",
			"enIN": "No Page for Locale",
			"enGB": "No Page for Locale",
			"idID": "No Page for Locale",
			"ruRU": "No Page for Locale",
			"svSE": "No Page for Locale",
			"viVN": "No Page for Locale",
			"plPL": "No Page for Locale",
			"arAR": "No Page for Locale",
			"nlNL": "No Page for Locale",
			"thTH": "No Page for Locale",
			"trTR": "No Page for Locale",
			"heIL": "No Page for Locale",
			"lvLV": "No Page for Locale",
			"etEE": "No Page for Locale",
			"ltLT": "No Page for Locale"
		},
		"meta_description": "We want to give you a behind the scenes look at how we have evolved the core mechanism of our product–namely, exactly how it kicks in to queue traffic in response to spikes.",
		"metadata": {
			"title": "How Waiting Room makes queueing decisions on Cloudflare's highly distributed network",
			"description": "We want to give you a behind the scenes look at how we have evolved the core mechanism of our product–namely, exactly how it kicks in to queue traffic in response to spikes.",
			"imgPreview": "https://cf-assets.www.cloudflare.com/slt3lc6tev37/1ZmA2hsrNUhCEcpT4Kw5m4/fd8a71637ffd75061609d7b955550ecd/how-waiting-room-queues-CUVlza.png"
		},
		"primary_author": {},
		"published_at": "2023-09-20T14:00:58.000+01:00",
		"slug": "how-waiting-room-queues",
		"tags": [
			{
				"id": "79jLdGrSe87cix3F6A5Snu",
				"name": "Waiting Room",
				"slug": "waiting-room"
			},
			{
				"id": "6hbkItfupogJP3aRDAq6v8",
				"name": "Cloudflare Workers",
				"slug": "workers"
			},
			{
				"id": "5v2UZdTRX1Rw9akmhexnxs",
				"name": "Durable Objects",
				"slug": "durable-objects"
			},
			{
				"id": "1U6ifhBwTuaJ2w4pjNOzNT",
				"name": "Network",
				"slug": "network"
			},
			{
				"id": "4HIPcb68qM0e26fIxyfzwQ",
				"name": "Developers",
				"slug": "developers"
			},
			{
				"id": "3JAY3z7p7An94s6ScuSQPf",
				"name": "Developer Platform",
				"slug": "developer-platform"
			}
		],
		"title": "How Waiting Room makes queueing decisions on Cloudflare's highly distributed network",
		"updated_at": "2024-08-27T01:06:37.409Z",
		"url": "https://blog.cloudflare.com/how-waiting-room-queues"
	},
	"translations": {
		"posts.by": "By",
		"footer.gdpr": "GDPR",
		"lang_blurb1": "This post is also available in {lang1}.",
		"lang_blurb2": "This post is also available in {lang1} and {lang2}.",
		"lang_blurb3": "This post is also available in {lang1}, {lang2} and {lang3}.",
		"footer.blurb": "Cloudflare's connectivity cloud protects <a target='_blank' href='https://www.cloudflare.com/network-services/' rel='noreferrer'>entire corporate networks</a>, helps customers build <a target='_blank' href='https://workers.cloudflare.com/' rel='noreferrer'>Internet-scale applications efficiently</a>, accelerates any <a target='_blank' href='https://www.cloudflare.com/performance/accelerate-internet-applications/' rel='noreferrer'>website or Internet application</a>, <a target='_blank' href='https://www.cloudflare.com/ddos/' rel='noreferrer'>wards off DDoS attacks</a>, keeps <a target='_blank' href='https://www.cloudflare.com/application-security/' rel='noreferrer'>hackers at bay</a>, and can help you on <a target='_blank' href='https://www.cloudflare.com/products/zero-trust/' rel='noreferrer'>your journey to Zero Trust</a>.<br/><br/>Visit <a target='_blank' href='https://one.one.one.one/' rel='noreferrer'>1.1.1.1</a> from any device to get started with our free app that makes your Internet faster and safer.<br/><br/>To learn more about our mission to help build a better Internet, <a target='_blank' href='https://www.cloudflare.com/learning/what-is-cloudflare/' rel='noreferrer'>start here</a>. If you&apos;re looking for a new career direction, check out <a target='_blank' href='http://www.cloudflare.com/careers' rel='noreferrer'>our open positions</a>.",
		"footer.press": "Press",
		"header.title": "The Cloudflare Blog",
		"search.clear": "Clear",
		"search.filter": "Filter",
		"search.source": "Source",
		"footer.careers": "Careers",
		"footer.company": "Company",
		"footer.support": "Support",
		"footer.the_net": "theNet",
		"search.filters": "Filters",
		"footer.our_team": "Our team",
		"footer.webinars": "Webinars",
		"page.more_posts": "More posts",
		"posts.time_read": "{time} min read",
		"search.language": "Language",
		"footer.community": "Community",
		"footer.resources": "Resources",
		"footer.solutions": "Solutions",
		"footer.trademark": "Trademark",
		"header.subscribe": "Subscribe",
		"footer.compliance": "Compliance",
		"footer.free_plans": "Free plans",
		"footer.impact_ESG": "Impact/ESG",
		"posts.follow_on_X": "Follow on X",
		"footer.help_center": "Help center",
		"footer.network_map": "Network Map",
		"header.please_wait": "Please Wait",
		"page.related_posts": "Related posts",
		"search.result_stat": "Results <strong>{search_range}</strong> of <strong>{search_total}</strong> for <strong>{search_keyword}</strong>",
		"footer.case_studies": "Case Studies",
		"footer.connect_2024": "Connect 2024",
		"footer.terms_of_use": "Terms of Use",
		"footer.white_papers": "White Papers",
		"footer.cloudflare_tv": "Cloudflare TV",
		"footer.community_hub": "Community Hub",
		"footer.compare_plans": "Compare plans",
		"footer.contact_sales": "Contact Sales",
		"header.contact_sales": "Contact Sales",
		"header.email_address": "Email Address",
		"page.error.not_found": "Page not found",
		"footer.developer_docs": "Developer docs",
		"footer.privacy_policy": "Privacy Policy",
		"footer.request_a_demo": "Request a demo",
		"page.continue_reading": "Continue reading",
		"footer.analysts_report": "Analyst reports",
		"footer.for_enterprises": "For enterprises",
		"footer.getting_started": "Getting Started",
		"footer.learning_center": "Learning Center",
		"footer.project_galileo": "Project Galileo",
		"pagination.newer_posts": "Newer Posts",
		"pagination.older_posts": "Older Posts",
		"posts.social_buttons.x": "Discuss on X",
		"search.source_location": "Source/Location",
		"footer.about_cloudflare": "About Cloudflare",
		"footer.athenian_project": "Athenian Project",
		"footer.become_a_partner": "Become a partner",
		"footer.cloudflare_radar": "Cloudflare Radar",
		"footer.network_services": "Network services",
		"footer.trust_and_safety": "Trust & Safety",
		"header.get_started_free": "Get Started Free",
		"page.search.placeholder": "Search Cloudflare",
		"footer.cloudflare_status": "Cloudflare Status",
		"footer.cookie_preference": "Cookie Preferences",
		"header.valid_email_error": "Must be valid email.",
		"footer.connectivity_cloud": "Connectivity cloud",
		"footer.developer_services": "Developer services",
		"footer.investor_relations": "Investor relations",
		"page.not_found.error_code": "Error Code: 404",
		"footer.logos_and_press_kit": "Logos & press kit",
		"footer.application_services": "Application services",
		"footer.get_a_recommendation": "Get a recommendation",
		"posts.social_buttons.reddit": "Discuss on Reddit",
		"footer.sse_and_sase_services": "SSE and SASE services",
		"page.not_found.outdated_link": "You may have used an outdated link, or you may have typed the address incorrectly.",
		"footer.report_security_issues": "Report Security Issues",
		"page.error.error_message_page": "Sorry, we can't find the page you are looking for.",
		"header.subscribe_notifications": "Subscribe to receive notifications of new posts:",
		"footer.cloudflare_for_campaigns": "Cloudflare for Campaigns",
		"header.subscription_confimation": "Subscription confirmed. Thank you for subscribing!",
		"posts.social_buttons.hackernews": "Discuss on Hacker News",
		"footer.diversity_equity_inclusion": "Diversity, equity & inclusion",
		"footer.critical_infrastructure_defense_project": "Critical Infrastructure Defense Project"
	}
}