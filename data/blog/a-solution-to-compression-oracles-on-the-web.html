<div class="mb2 gray5">4 min read</div>
<div class="post-content lh-copy gray1">
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/1CF9rdyyaBeVuDQkAC1WG2/eac67cb97ba7ce6f93b2ed60d10b822e/Ressort_de_compression.jpg" alt="Spring" class="kg-image" width="638" height="365" loading="lazy">

	</figure>
	<p><a href="https://commons.wikimedia.org/wiki/File:Ressort_de_compression.jpg">CC 3.0 by Jean-Jacques MILAN</a></p>
	<p><i>This is a guest post by Blake Loring, a PhD student at Royal Holloway, University of London. Blake worked at Cloudflare as an intern in the summer of 2017.</i></p>
	<p>Compression is often considered an essential tool when reducing the bandwidth usage of internet services. The impact that the use of such compression schemes can have on security, however, has often been overlooked. The recently detailed <a href="https://en.wikipedia.org/wiki/CRIME">CRIME</a>, <a href="http://breachattack.com">BREACH</a>, <a href="https://www.blackhat.com/eu-13/briefings.html#Beery">TIME</a> and <a href="https://www.blackhat.com/docs/us-16/materials/us-16-VanGoethem-HEIST-HTTP-Encrypted-Information-Can-Be-Stolen-Through-TCP-Windows-wp.pdf">HEIST</a> attacks on TLS have shown that if an attacker can make requests on behalf of a user then secret information can be extracted from encrypted messages using only the length of the response. Deciding whether an element of a web-page should be secret often depends on the content of the page, however there are some common elements of web-pages which should always remain secret such as <a href="https://en.wikipedia.org/wiki/Cross-site_request_forgery">Cross-Site Request Forgery (CSRF)</a> tokens. Such tokens are used to ensure that malicious webpages cannot forge requests from a user by enforcing that any request must contain a secret token included in a previous response.</p>
	<p>I worked at Cloudflare last summer to investigate possible solutions to this problem. The result is a project called <a href="https://github.com/cloudflare/cf-nocompress">cf-nocompress</a>. The aim of this project was to develop a tool which automatically mitigates instances of the attack, in particular CSRF extraction, on Cloudflare hosted services transparently without significantly impacting the effectiveness of compression. We have published a <a href="https://github.com/cloudflare/cf-nocompress/tree/master/cf-nocompress">proof-of-concept implementation</a> on GitHub, and provide a <a href="https://compression.website">challenge site</a> and <a href="https://github.com/cloudflare/cf-nocompress/tree/master/example_attack/src/main">tool</a> which demonstrates the attack in action).</p>
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/OWY4iAZGfQjKwmBgRkway/3e8c1a109936ad8001d5da45572f5226/compression.website.jpg" alt="compression.website" class="kg-image" width="1920" height="1276" loading="lazy">

	</figure>
	<h3>The Problem</h3>
	<p>Most web compression schemes reduce the size of data by replacing common sequences with references to a dictionary of terms created during the compression. When using such compression schemes the size of the encrypted response will be reduced if there are repeated strings within the plaintext. This can be exploited through the use of a canary, an element in a request which we know will be added to the response, to test whether a string exists within the original response using the compressed response length. From this we can extract the contents of portions of a webpage incrementally by guessing each subsequent character. This attack creates an opportunity for malicious JavaScript to extract CSRF tokens and other confidential information from a webpage through malicious code served to a browser using either a packet sniffer (a methodology created by Duong and Rizzo as part of the <a href="https://blog.cryptographyengineering.com/2011/09/21/brief-diversion-beast-attack-on-tlsssl">BEAST attack</a>) or JavaScript APIs which reveal network statistics (described by Vanhoef and Van Goethem in <a href="https://www.blackhat.com/docs/us-16/materials/us-16-VanGoethem-HEIST-HTTP-Encrypted-Information-Can-Be-Stolen-Through-TCP-Windows.pdf">HEIST</a>).</p>
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/23iahybYJxsleUyYAbpWoQ/8c8d70096bc225735f83c395279f3d02/Artboard---4.png" alt="Artboard---4" class="kg-image" width="1810" height="3308" loading="lazy">

	</figure>
	<p>There are two common mitigation schemes for this attack. The first is to send a unique CSRF every time a page is loaded. By removing the consistent element from the page the threat of attack is removed. This approach requires the server to keep state of valid CSRFs and whether they have been used, additionally it can only be used to protect page tokens and not user-readable data. Another approach is to XOR all secrets in a response with a per-request random number and then transmit the number with the response. Once received a piece of JavaScript can then be used to recover the original secret by XORing the data again. Alternatively, the server can be modified to expect the XOR variant and the random number rather than the original secret. This approach allows for all secrets to be protected, however it requires client side post-processing. Additionally, both approaches require extensive, per page, modification which make mitigation incredibly cumbersome in practice. At present the only way to fully mitigate such an attack is to disable compression entirely on vulnerable websites, an impractical solution for most websites and content delivery networks.</p>
	<h3>Our Solution</h3>
	<p>We decided to use selective compression, compressing only non-secret parts of a page, in order to stop the extraction of secret information from a page. We found that in most cases a secret within a webpage can be described in terms of a classical regular expression. These descriptions allow us to identify secrets online as a response is streamed. Once the secrets are identified they can be flagged so that a modified compression library can ensure that they are not added to the dictionary. The primary advantage of this approach is that protection can be offered transparently by the web-server and the application does not need to be modified as long as a regular expression can be used to clearly express which portions of a response are secret. In addition, we do not need to maintain state for each user or require client-side JavaScript to appropriately render the page.</p>
	<p>The proof-of-concept is implemented as a plugin for NGINX and requires a small patch to the gzip module. The plugin uses <a href="https://github.com/openresty/sregex">sregex</a> to identify secrets within a page. The modified gzip functions as normal, however when a secret is processed compression is disabled. This ensures secrets do not get added to the compression dictionary, removing any on response size.</p>
	<h3>Additional security considerations</h3>
	<p>The regular expression matching engine we use in this proof-of-concept is not guaranteed to run in constant time. As such, matching a string against some regular expressions could introduce a timing based side-channel attack. This issue is compounded by the complexity of modern regular expressions as matching time can often be non-intuitive. Whilst in many cases the risk such an attack would pose is minimal, a limited matcher with constant runtime and restrictions on unbounded loops should be developed if our mitigation is adopted.</p>
	<h3>The Challenge Site</h3>
	<p>We have set up the challenge website <a href="https://compression.website">compression.website</a> with protection, and a clone of the site <a href="https://compression.website/unsafe">compression.website/unsafe</a> without it. The page is a simple form with a per-client CSRF designed to emulate common CSRF protection. Using the example attack presented with the library we have shown that we are able to extract the CSRF from the size of request responses in the unprotected variant but we have not been able to extract it on the protected site. We welcome attempts to extract the CSRF without access to the unencrypted response.</p>
</div>