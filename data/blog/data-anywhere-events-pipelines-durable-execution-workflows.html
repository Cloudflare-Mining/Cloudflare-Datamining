<div class="mb2 gray5">8 min read</div>
<div class="post-content lh-copy gray1">
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/4a3NcRwpfQK1mgW7n37Yao/3d3f101e973320edd2dd9b873b0b2e36/image3-4.png" alt="Data Anywhere with Pipelines, Event Notifications, and Workflows" class="kg-image" width="1999" height="1125" loading="lazy">

	</figure>
	<p>Data is fundamental to any real-world application: the database storing your user data and inventory, the analytics tracking sales events and/or error rates, the object storage with your web assets and/or the Parquet files driving your data science team, and the vector database enabling semantic search or AI-powered recommendations for your users.</p>
	<p>When we first announced Workers <a href="https://blog.cloudflare.com/introducing-cloudflare-workers">back in 2017</a>, and then <a href="https://blog.cloudflare.com/introducing-workers-kv">Workers KV</a>, <a href="https://www.cloudflare.com/developer-platform/r2">Cloudflare R2</a>, and <a href="https://blog.cloudflare.com/introducing-d1">D1</a>, it was obvious that the next big challenge to solve for developers would be in making it easier to ingest, store, and query the data needed to build scalable, full-stack applications.</p>
	<p>To that end, as part of our quest to make building stateful, distributed-by-default applications even easier, we‚Äôre launching our new Event Notifications service; a preview of our upcoming streaming ingestion product, Pipelines; and a sneak peek into our take on durable execution, Workflows.</p>
	<h3>Event-based architectures</h3>
	<p>When you‚Äôre writing data ‚Äî whether that‚Äôs new data, changing existing data, or deleting old data ‚Äî you often want to trigger other, asynchronous work to run in response. That could be processing user-driven uploads, updating search indexes as the underlying data changes, or removing associated rows in your SQL database when content is removed.</p>
	<p>In order to make these event-driven workflows far easier to build across Cloudflare, we‚Äôre launching the first step towards a wider Event Notifications platform across Cloudflare, starting with notifications support in R2.</p>
	<p>You can read more in the deep-dive on <a href="https://blog.cloudflare.com/r2-events-gcs-migration-infrequent-access">Event Notifications for R2</a>, but in a nutshell: you can configure changes to content in any R2 bucket to write directly to a <a href="https://developers.cloudflare.com/queues">Queue</a>, allowing you to reliably consume those events in a Worker or to <a href="https://developers.cloudflare.com/queues/reference/pull-consumers">pull from compute</a> in a legacy cloud.</p>
	<figure class="kg-card kg-image-card kg-width-wide">

		<img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/63wR0NuQIFaZUo5TvDvS29/92999ffd89864a36a832e0341852fc06/image2-3.png" alt="" class="kg-image" width="1999" height="1020" loading="lazy">

	</figure>
	<p>Event Notifications for R2 are just the beginning, though. There are many kinds of events you might want to trigger as a developer ‚Äî these are just some of the event types we‚Äôre planning to support:</p>
	<ul>
		<li>
			<p>Changes (writes) to key-value pairs in your <a href="https://developers.cloudflare.com/kv">Workers KV</a> namespaces.</p>
		</li>
		<li>
			<p>Updates to your <a href="https://developers.cloudflare.com/d1">D1 databases</a>, including changed rows or triggers.</p>
		</li>
		<li>
			<p><a href="https://developers.cloudflare.com/workers/configuration/versions-and-deployments/gradual-deployments">Deployments</a> to your Cloudflare Workers</p>
		</li>
	</ul>
	<p>Consuming event notifications from a single Worker is just <i>one</i> approach, though. As you start to consume events, you may want to trigger multi-step <i>workflows</i> that execute reliably, resume from errors or exceptions, and ensure that previous steps aren‚Äôt duplicated or repeated unnecessarily. An event notification framework turns out to be just the thing needed to drive <a href="https://blog.cloudflare.com/#durable-exectution">a workflow engine that <i>executes durably</i>‚Ä¶</a></p>
	<h3>Making it even easier to ingest data</h3>
	<p>When we launched <a href="https://developers.cloudflare.com/r2">Cloudflare R2</a>, our object storage service, we knew that supporting the de facto-standard <a href="https://developers.cloudflare.com/r2/api/s3/api">S3 API</a> was critical in order to allow developers to bring the tooling and services they already had over to R2. But the S3 API is designed to be simple: at its core, it provides APIs for upload, download, multipart and metadata operations, and many tools <i>don‚Äôt</i> support the S3 API.</p>
	<p>What if you want to batch clickstream data from your web services so that it‚Äôs efficient (and cost-effective) to query by your analytics team? Or partition data by customer ID, merchant ID, or locale within a structured data format like JSON?</p>
	<p>Well, we want to help solve this problem too, and so we‚Äôre announcing Pipelines, an upcoming streaming ingestion service designed to ingest data at scale, aggregate it, and write it directly to R2, without you having to manage infrastructure, partitions, runners, or worry about durability.</p>
	<p>With Pipelines, creating a globally scalable ingestion endpoint that can ingest tens-of-thousands of events per second doesn‚Äôt require any code:</p>
	<pre class="language-js"><code class="language-js">$ wrangler pipelines create clickstream-ingest-prod --batch-size="1MB" --batch-timeout-secs=120 --batch-on-json-key=".merchantId" --destination-bucket="prod-cs-data"

‚úÖ Successfully created new pipeline "clickstream-ingest-prod"
üì• Created endpoints:
‚û° HTTPS: https://d458dbe698b8eef41837f941d73bc5b3.pipelines.cloudflarestorage.com/clickstream-ingest-prod
‚û° WebSocket: wss://d458dbe698b8eef41837f941d73bc5b3.pipelines.cloudflarestorage.com:8443/clickstream-ingest-prod
‚û° Kafka: d458dbe698b8eef41837f941d73bc5b3.pipelines.cloudflarestorage.com:9092 (topic: clickstream-ingest-prod)</code></pre>
	<p>As you can see here, we‚Äôre already thinking about how to make Pipelines protocol-agnostic: write from a HTTP client, stream events over a WebSocket, and/or redirect your existing Kafka producer (and stop having to manage and scale Kafka) directly to Pipelines.</p>
	<p>But that‚Äôs just the beginning of our vision here. Scalable ingestion and simple batching is one thing, but what about if you have more complex needs? Well, we have a massively scalable compute platform (<a href="https://developers.cloudflare.com/workers">Cloudflare Workers</a>) that can help address this too.</p>
	<p>The code below is just an initial exploration for how we‚Äôre thinking about an API for running transforms over streaming data. If you‚Äôre aware of projects like <a href="https://beam.apache.org/documentation/programming-guide">Apache Beam</a> or <a href="https://flink.apache.org">Flink</a>, this programming model might even look familiar:</p>
	<pre class="language-js"><code class="language-js">export default {    
   // Pipeline handler is invoked when batch criteria are met
   async pipeline(stream: StreamPipeline, env: Env, ctx: ExecutionContext): Promise&lt;StreamingPipeline&gt; {
      // ...
      return stream
         // Type: transform(label: string, transformFunc: TransformFunction): Promise&lt;StreamPipeline&gt;
         // Each transform has a label that is used in metrics to provide
    // per-transform observability and debugging
         .transform("human readable label", (events: Array&lt;StreamEvent&gt;) =&gt; {
            return events.map((e) =&gt; ...)
         })
         .transform("another transform", (events: Array&lt;StreamEvent&gt;) =&gt; {
            return events.map((e) =&gt; ...)
         })
         .writeToR2({
            format: "json",
            bucket: "MY_BUCKET_NAME",
            prefix: somePrefix,
            batchSize: "10MB"
         })
   }
}</code></pre>
	<p>Specifically:</p>
	<ul>
		<li>
			<p>The Worker describes a pipeline of transformations (mapping, reducing, filtering) that operates over each subset of events (records)</p>
		</li>
		<li>
			<p>You can call out to other services ‚Äî including D1 or KV ‚Äî in order to synchronously or asynchronously hydrate data or lookup values during your stream processing</p>
		</li>
		<li>
			<p>We take care of scaling horizontally based on records-per-second and/or any concurrency settings you configure based on processing latency requirements.</p>
		</li>
	</ul>
	<p><b>We‚Äôll be bringing Pipelines into open beta later in 2024</b>, and it will initially launch with support for HTTP ingestion and R2 as a destination (sink), but we‚Äôre already thinking bigger.</p>
	<p>We‚Äôll be sharing more as Pipelines gets closer to release. In the meantime, you can <a href="https://docs.google.com/forms/d/e/1FAIpQLSeuaQ5YZoXJej5h5KoEz6LNrVb7gASJ8msahJg8VmBeC0HEYQ/viewform?usp=sf_link">register your interest and share your use-case</a>, and we‚Äôll reach out when Pipelines reaches open beta.</p>
	<h3>Durable Execution</h3>
	<p>If the term ‚ÄúDurable Execution‚Äù is new to you, don‚Äôt worry: the term comes from the desire to run applications that can resume execution from where they left off, even if the underlying host or compute fails (where the ‚Äúdurable‚Äù part comes from).</p>
	<p>As we‚Äôve continued to build out our data and AI platforms, we‚Äôve been acutely aware that developers need ways to create reliable, repeatable workflows that operate over that data, turn unstructured data into structured data, trigger on fresh data (or periodically), and automatically retry, restart, and export metrics for each step along the way. The industry calls this Durable Execution: we‚Äôre just calling it <i>Workflows</i>.</p>
	<p>What makes Workflows different from other takes on Durable Execution is that we provide the underlying compute as part of the platform. You don‚Äôt have to bring-your-own compute, or worry about scaling it or provisioning it in the right locations. Workflows runs on top of <a href="https://developers.cloudflare.com/workers">Cloudflare Workers</a> ‚Äì you write the workflow, and we take care of the rest.</p>
	<p>Here‚Äôs an early example of writing a Workflow that generates text embeddings using Workers AI and stores them (ready to query) in Vectorize as new content is written to (or updated within) R2.</p>
	<ul>
		<li>
			<p>Each Workflow <i>run</i> is triggered by an Event Notification consumed from a Queue, but could also be triggered by a HTTP request, another Worker, or even scheduled on a timer.</p>
		</li>
		<li>
			<p>Individual <i>steps</i> within the Workflow allow us to define individually retriable units of work: in this case, we‚Äôre reading the new objects from R2, creating text embeddings using Workers AI, and then inserting.</p>
		</li>
		<li>
			<p>State is <i>durably</i> persisted between steps: each step can emit state, and Workflows will automatically persist that so that any underlying failures, uncaught exceptions or network retries can resume execution from the last successful step.</p>
		</li>
		<li>
			<p>Every call to step() automatically emits metrics associated with the unique Workflow run, making it easier to debug within each step and/or break down your application into its smallest units of execution, without having to worry about observability.</p>
		</li>
	</ul>
	<p>Step-by-step, it looks like this:</p>
	<figure class="kg-card kg-image-card kg-width-wide">

		<img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/6OCCJMjL6NCXeeVTD4aNeb/371610f75006bd4a97c62e842bc83b72/image4-5.png" alt="" class="kg-image" width="1999" height="1020" loading="lazy">

	</figure>
	<p>Transforming this series of steps into real code, here‚Äôs what this would look like with Workflows:</p>
	<pre class="language-js"><code class="language-js">import { Ai } from "@cloudflare/ai";
import { Workflow } from "cloudflare:workers";

export interface Env {
  R2: R2Bucket;
  AI: any;
  VECTOR_INDEX: VectorizeIndex;
}

export default class extends Workflow {
  async run(event: Event) {
    const ai = new Ai(this.env.AI);

    // List of keys to fetch from our incoming event notification
    const keysToFetch = event.messages.map((val) =&gt; {
      return val.object.key;
    });

    // The return value of each step is stored (the "durable" part
    // of "durable execution")
    // This ensures that state can be persisted between steps, reducing
    // the need to recompute results ($$, time) should subsequent
    // steps fail.
    const inputs = await this.ctx.run(
      // Each step has a user-defined label
      // Metrics are emitted as each step runs (to success or failure)
// with this label attached and available within per-Workflow
// analytics in near-real-time.
"read objects from R2", async () =&gt; {
      const objects = [];

      for (const key of keysToFetch) {
        const object = await this.env.R2.get(key);
        objects.push(await object.text());
      }

      return objects;
    });


    // Persist the output of this step.
    const embeddings = await this.ctx.run(
      "generate embeddings",
      async () =&gt; {
        const { data } = await ai.run("@cf/baai/bge-small-en-v1.5", {
          text: inputs,
        });

        if (data.length) {
          return data;
        } else {
          // Uncaught exceptions trigger an automatic retry of the step
          // Retries and timeouts have sane defaults and can be overridden
    // per step
          throw new Error("Failed to generate embeddings");
        }
      },
      {
        retries: {
          limit: 5,
          delayMs: 1000,
          backoff: "exponential",
        },
      }
    );

    await this.ctx.run("insert vectors", async () =&gt; {
      const vectors = [];

      keysToFetch.forEach((key, index) =&gt; {
        vectors.push({
          id: crypto.randomUUID(),
          // Our embeddings from the previous step
          values: embeddings[index].values, 
          // The path to each R2 object to map back to during
 	    // vector search
          metadata: { r2Path: key },
        });
      });

      return this.env.VECTOR_INDEX.upsert(vectors);
    });
  }
}</code></pre>
	<p>This is just one example of what a Workflow can do. The ability to durably execute an application, modeled as a series of steps, applies to a wide number of domains. You can apply this model of execution to a number of use-cases, including:</p>
	<ul>
		<li>
			<p>Deploying software: each step can define a build step and subsequent health check, gating further progress until your deployment meets your criteria for ‚Äúhealthy‚Äù.</p>
		</li>
		<li>
			<p>Post-processing user data: triggering a workflow based on user uploads (e.g. to Cloudflare R2) that then subsequently parses that data asynchronously, redacts PII or sensitive data, writes the sanitized output, and triggers a notification via email, webhook, or mobile push.</p>
		</li>
		<li>
			<p>Payment and batch workflows: aggregating raw customer usage data on a periodic schedule by querying your data warehouse (or <a href="https://developers.cloudflare.com/analytics/analytics-engine">Workers Analytics Engine</a>), triggering usage or spend alerts, and/or generating PDF invoices.</p>
		</li>
	</ul>
	<p>Each of these use cases model tasks that you want to run to completion, minimize redundant retries by persisting intermediate state, and (importantly) easily observe success and failure.</p>
	<p><b>We‚Äôll be sharing more about Workflows during the second quarter of 2024 as we work towards an open (public!) beta</b>. This includes how we‚Äôre thinking about idempotency and interactions with our storage, per-instance observability and metrics, local development, and templates to bootstrap common workflows.</p>
	<h3>Putting it together</h3>
	<p>We‚Äôve often thought of Cloudflare‚Äôs own network as one massively scalable parallel data processing cluster: <a href="https://www.cloudflare.com/network">data centers in 310+ cities</a>, with the ability to run compute close to users and/or <a href="https://smart-placement-demo.pages.dev">close to data</a>, keep it within the bounds of regulatory or compliance requirements, and most importantly, use our massive scale to enable our customers to scale as well.</p>
	<p>Recapping, a fully-fledged data platform needs to enable three things:</p>
	<ol>
		<li>
			<p>Ingesting data: getting data into the platform (in the right format, from the right sources)</p>
		</li>
		<li>
			<p>Storing data: securely, reliably, and durably.</p>
		</li>
		<li>
			<p>Querying data: understanding and extracting insights from the data, and/or transforming it for use by other tools.</p>
		</li>
	</ol>
	<p>When we launched R2 we tackled the second part, but knew that we‚Äôd need to follow up with the first and third parts in order to make it easier for developers to get data in and make use of it.</p>
	<p>If we look at how we can build a system that helps us solve each of these three parts together with Pipelines, Event Notifications, R2, and Workflows, we end up with an architecture that resembles this:</p>
	<figure class="kg-card kg-image-card kg-width-wide">

		<img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/15ue6XZBrDyTtfja0z8VS5/baedf1a168731a319a6b14eb14f3ad00/image1-2.png" alt="" class="kg-image" width="1999" height="1020" loading="lazy">

	</figure>
	<p>Specifically, we have Pipelines (1) scaling out to ingest data, batch it, filter it, and then durably store it in R2 (2) in a format that‚Äôs ready and optimized for querying. Workflows, ClickHouse, Databricks, or the query engine of your choice can then query (3) that data as soon as it‚Äôs ready ‚Äî with ‚Äúready‚Äù being automatically triggered by an Event Notification <i>as soon as the data is ingested and written to R2</i>.</p>
	<p>There‚Äôs no need to poll, no need to batch after the fact, no need to have your query engine slow down on data that wasn‚Äôt pre-aggregated or filtered, and no need to manage and scale infrastructure in order to keep up with load or data jurisdiction requirements. Create a Pipeline, write your data directly to R2, and query directly from it.</p>
	<p>If you‚Äôre also looking at this and wondering about the costs of moving this data around, then we‚Äôre holding to one important principle: zero egress fees across all of our data products. Just as we set the stage for this with <a href="https://blog.cloudflare.com/introducing-r2-object-storage">our R2 object storage</a>, we intend to apply this to every data product we‚Äôre building, Pipelines included.</p>
	<h3>Start Building</h3>
	<p>We‚Äôve shared a lot of what we‚Äôre building so that developers have an opportunity to provide feedback (including via our <a href="https://discord.cloudflare.com">Developer Discord</a>), share use-cases, and think about how to build their <i>next</i> application on Cloudflare.</p>
</div>