{
	"footerBlurb": "Cloudflare's connectivity cloud protects <a target='_blank' href='https://www.cloudflare.com/network-services/' rel='noreferrer'>entire corporate networks</a>, helps customers build <a target='_blank' href='https://workers.cloudflare.com/' rel='noreferrer'>Internet-scale applications efficiently</a>, accelerates any <a target='_blank' href='https://www.cloudflare.com/performance/accelerate-internet-applications/' rel='noreferrer'>website or Internet application</a>, <a target='_blank' href='https://www.cloudflare.com/ddos/' rel='noreferrer'>wards off DDoS attacks</a>, keeps <a target='_blank' href='https://www.cloudflare.com/application-security/' rel='noreferrer'>hackers at bay</a>, and can help you on <a target='_blank' href='https://www.cloudflare.com/products/zero-trust/' rel='noreferrer'>your journey to Zero Trust</a>.<br/><br/>Visit <a target='_blank' href='https://one.one.one.one/' rel='noreferrer'>1.1.1.1</a> from any device to get started with our free app that makes your Internet faster and safer.<br/><br/>To learn more about our mission to help build a better Internet, <a target='_blank' href='https://www.cloudflare.com/learning/what-is-cloudflare/' rel='noreferrer'>start here</a>. If you&apos;re looking for a new career direction, check out <a target='_blank' href='http://www.cloudflare.com/careers' rel='noreferrer'>our open positions</a>.",
	"initialReadingTime": "6",
	"locale": "en-us",
	"localesAvailable": [
		"ko-kr"
	],
	"post": {
		"authors": [
			{
				"name": "Junho Choi",
				"slug": "junho",
				"bio": null,
				"profile_image": "https://cf-assets.www.cloudflare.com/slt3lc6tev37/RjmCfqvELYZkV1MQj99y0/8572367ec3f3330f25b2f4d6bc816ab0/junho.jpg",
				"location": null,
				"website": "https://saturnsoft.net",
				"twitter": "@junhochoi",
				"facebook": null
			}
		],
		"excerpt": "Cloudflare recently shipped improved upload speeds across our network for clients using HTTP/2. This post describes our journey from troubleshooting an issue to fixing it and delivering faster upload speeds to the global Internet.",
		"feature_image": "https://cf-assets.www.cloudflare.com/slt3lc6tev37/1npVYpgA27awEVgHkjfnbR/74576983eb1b1bd05361ae2ac3d2064b/delivering-http-2-upload-speed-improvements.png",
		"featured": false,
		"html": "\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/5ppm94raHeMnhgW9XULeSz/21ce9c33dad6fdd912aaeb39ee3e0090/Speed-header_2x-1.png\" alt=\"\" class=\"kg-image\" width=\"2832\" height=\"1472\" loading=\"lazy\"/>\n            \n            </figure><p>Cloudflare recently shipped improved <i>upload</i> speeds across our network for clients using HTTP/2. This post describes our journey from troubleshooting an issue to fixing it and delivering faster upload speeds to the global Internet.</p><p>We <a href=\"/test-your-home-network-performance/\">launched</a> <a href=\"/test-your-home-network-performance/\">speed.cloudflare.com</a> in May 2020 to give our users insight into how well their networks perform. The test provides download, upload and latency tests. Soon after release, we received reports from a small number of users that sometimes upload speeds were underreported. Our investigation determined that it seemed to happen with end users that had high upload bandwidth available (several hundreds Mbps class cable modem or fiber service). Our speed tests are performed via browser JavaScript, and most browsers use HTTP/2 by default. We found that HTTP/2 upload speeds were sometimes much slower than HTTP/1.1 (assuming all TLS) when the user had high available upload bandwidth.</p><p>Upload speed is more important than ever, especially for people using home broadband connections. As many people have been forced to work from home they’re using their broadband connections differently than before. Prior to the pandemic broadband traffic was very asymmetric (you downloaded way more than you uploaded… think listening to music, or streaming a movie), but now we’re seeing an increase in uploading as people video conference from home or create content from their home office.</p>\n          <div class=\"flex anchor relative\">\n            <h3 id=\"initial-tests\">Initial Tests</h3>\n            <a href=\"#initial-tests\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n              <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n            </a>\n          </div>\n        <p>User reports were focused on particularly fast home networks. I set up a <code>dummynet</code> network simulator to test upload speed in a controlled environment. I launched a linux VM running our code inside my Macbook Pro and set up a <a href=\"https://www.saturnsoft.net/tech/2020/06/07/macosx-dummynet/\">dummynet between the VM and Mac host</a>.  Measuring upload speed is simple – create a file and upload using curl to an endpoint which accepts a request body. I ran the same test 20 times and took a median upload speed (Mbps).</p>\n            <pre class=\"language-shell\"><code class=\"language-shell\">% dd if=/dev/urandom of=test.dat bs=1M count=10\n% curl --http1.1 -w &#039;%{speed_upload}\\n&#039; -sf -o/dev/null --data-binary @test.dat https://edge/upload-endpoint\n% curl --http2 -w &#039;%{speed_upload}\\n&#039; -sf -o/dev/null --data-binary @test.dat https://edge/upload-endpoint</pre></code>\n            <p>Stepping up to uploading a 10MB object over a network which has 200Mbps available bandwidth and 40ms <a href=\"https://www.cloudflare.com/learning/cdn/glossary/round-trip-time-rtt/\">RTT</a>, the result was surprising. Using our production configuration, HTTP/2 upload speed tested at almost half of the same test conditions using HTTP/1.1 (higher is better).</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/5MPf14YbzqYkqUtiu4lNyH/0e6984059354da9c2a73d5539cb20232/Screen-Shot-2020-08-18-at-4.44.56-PM.png\" alt=\"\" class=\"kg-image\" width=\"600\" height=\"368\" loading=\"lazy\"/>\n            \n            </figure><p>The result may differ depending on your network, but the gap is bigger when the network is fast. On a slow network, like my home cable connection (5Mbps upload and 20ms RTT), HTTP/2 upload speed was almost identical to the performance observed with HTTP/1.1.</p>\n          <div class=\"flex anchor relative\">\n            <h3 id=\"receiver-flow-control\">Receiver Flow Control</h3>\n            <a href=\"#receiver-flow-control\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n              <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n            </a>\n          </div>\n        <p>Before we get into more detail on this topic, my intuition suggested the issue was related to receiver flow control. Usually the client (browser or any HTTP client) is the receiver of data, but in the case the client is uploading content to the server, the server is the receiver of data. And the receiver needs some type of flow control of the receive buffer.</p><p>How we handle receiver flow control differs between HTTP/1.1 and HTTP/2. For example, HTTP/1.1 doesn’t define protocol-level receiver flow control since there is no multiplexing of requests in the connection and it’s up to the TCP layer which handles receiving data. Note that most of the modern OS TCP stacks have auto tuning of the receive buffer (we will revisit that later) and they tune based on the current <a href=\"https://en.wikipedia.org/wiki/Bandwidth-delay_product\">BDP (bandwidth-delay product)</a>.</p><p>In the case of HTTP/2, there is a <a href=\"https://tools.ietf.org/html/rfc7540#section-5.2\">stream-level flow control</a> mechanism because the protocol supports multiplexing of streams. Each HTTP/2 stream has its own flow control window and there is connection level flow control for all streams in the connection. If it’s too tight, the sender will be blocked by the flow control. If it’s too loose we may end up wasting memory for buffering. So keeping it optimal is important when implementing flow control and the most optimal strategy is to keep the receive buffer matching the current BDP. BDP represents the maximum bytes in flight in the given network and can be used as an optimal buffer size.</p>\n          <div class=\"flex anchor relative\">\n            <h3 id=\"how-nginx-handles-the-request-body-buffer\">How NGINX handles the request body buffer</h3>\n            <a href=\"#how-nginx-handles-the-request-body-buffer\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n              <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n            </a>\n          </div>\n        <p>Initially I tried to find a parameter which controls NGINX upload buffering and tried to see if tuning the values improved the result. There are a couple of parameters which are related to uploading a request body.</p><ul><li><p><code>[proxy_buffering](http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_buffering)</code></p></li><li><p><code>[client_body_buffer_size](http://nginx.org/en/docs/http/ngx_http_core_module.html#client_body_buffer_size)</code></p></li></ul><p>And this is HTTP/2 specific:</p><ul><li><p><code>[http2_body_preread_size](http://nginx.org/en/docs/http/ngx_http_v2_module.html#http2_body_preread_size)</code></p></li></ul><p>Cloudflare does not use the <code>proxy_request_buffering</code> directive, so it can be immediately discounted.  <code>client_body_buffer_size</code> is the size of the request body buffer which is used regardless of the protocol, so this one applies to HTTP/1.1 and HTTP/2 as well.</p><p>When looking into the code, here is how it works:</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/5FFejc0ITjeuf1tZChITtW/cf5f7482421b44b0d2f15152ff55d61c/Screen-Shot-2020-08-10-at-3.51.05-PM.png\" alt=\"\" class=\"kg-image\" width=\"1020\" height=\"399\" loading=\"lazy\"/>\n            \n            </figure><ul><li><p>HTTP/1.1: use <code>client_body_buffer_size</code> buffer as a buffer between upstream and the client, simply repeating reading and writing using this buffer.</p></li><li><p>HTTP/2: since we need a flow control window update for the HTTP/2 DATA frame, there are two parameters:</p><ul><li><p><code>http2_body_preread_size</code>: it specifies the size of the initial request body read before it starts to send to the upstream.</p></li><li><p><code>client_body_buffer_size</code>: it specifies the size of the request body buffer.</p></li><li><p>Those two parameters are used for allocating a request body buffer during uploading. Here is a brief summary of how unbuffered upload works:</p><ul><li><p>Allocate a single request body buffer which size is a maximum of <code>http2_body_preread_size</code> and <code>client_body_buffer_size</code>. This means if <code>http2_body_preread_size</code> is 64KB and <code>client_body_buffer_size</code> is 128KB, then a 128KB buffer is allocated. We use 128KB for <code>client_body_buffer_size</code>.</p></li><li><p>HTTP/2 Settings <code>INITIAL_WINDOW_SIZE</code> of the stream is set to <code>http2_body_preread_size</code> and we use 64KB as a default (<a href=\"https://tools.ietf.org/html/rfc7540#section-6.9.2\">the RFC7540 default value</a>).</p></li><li><p>HTTP/2 module reads up to <code>http2_body_preread_size</code> before sending it to upstream.</p></li><li><p>After flushing the preread buffer, keep reading from the client and write to upstream and send <code>WINDOW_UPDATE</code> frame back to the client when necessary until the request body is fully received.</p></li></ul></li></ul></li></ul><p>To summarise what this means: HTTP/1.1 simply uses a single buffer, so TCP socket buffers do the flow control. However with HTTP/2, the application layer also has receiver flow control and NGINX uses a fixed size buffer for the receiver. This limits upload speed when the current link has a BDP larger than the current request body buffer size. So the bottleneck is HTTP/2 flow control when the buffer size is too tight.</p>\n          <div class=\"flex anchor relative\">\n            <h3 id=\"were-going-to-need-a-bigger-buffer\">We're going to need a bigger buffer?</h3>\n            <a href=\"#were-going-to-need-a-bigger-buffer\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n              <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n            </a>\n          </div>\n        <p>In theory, bigger buffer sizes should avoid upload bottlenecks, so I tried a few out by running my tests again. The previous chart result is now labelled &quot;prod&quot; and plotted alongside HTTP/2 tests with <code>client_body_buffer_size</code> set to 256KB, 512KB and 1024KB:</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/3ViLVb0T1Ku13wI6wjCicy/09adebfc2a859542825559032a61c600/10MB-1.png\" alt=\"\" class=\"kg-image\" width=\"604\" height=\"372\" loading=\"lazy\"/>\n            \n            </figure><p>It appears 512KB is an optimal value for <code>client_body_buffer_size</code>.</p><p>What if I test with some other network parameter? Here is when RTT is 10ms, in this case, 256KB looks optimal.</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/5nxstBv8ApAJdbBCOhXFSQ/52b5e5501233ffd1fc29f842e591522d/10MB-2.png\" alt=\"\" class=\"kg-image\" width=\"604\" height=\"371\" loading=\"lazy\"/>\n            \n            </figure><p>Both cases look much better than current 128KB and get a similar performance to HTTP/1.1 or even better. However, it seems like the optimal buffer size is a moving target and furthermore having too large a buffer size can hurt the performance: we need a smart way to find the optimal buffer size.</p>\n          <div class=\"flex anchor relative\">\n            <h2 id=\"autotuning-request-body-buffer-size\">Autotuning request body buffer size</h2>\n            <a href=\"#autotuning-request-body-buffer-size\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n              <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n            </a>\n          </div>\n          <p>One of the ideas that can help this kind of situation is autotuning. For example, modern TCP stacks autotune their receive buffers automatically. In production, our edge also has <a href=\"https://sysctl-explorer.net/net/ipv4/tcp_moderate_rcvbuf/\">TCP receiver buffer autotuning</a> enabled by default.</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">net.ipv4.tcp_moderate_rcvbuf = 1</pre></code>\n            <p>But in case of HTTP/2, TCP buffer autotuning is not very effective because the HTTP/2 layer is doing its own flow control and the existing 128KB was too small for a high BDP link. At this point, I decided to pursue autotuning HTTP/2 receive buffer sizing as well, similar to what TCP does.</p><p>The basic idea is that NGINX doubles the size of HTTP/2 request body buffer size based on its BDP. Here is an algorithm currently implemented in our version of NGINX:</p><ul><li><p>Allocate a request body buffer as explained above.</p></li><li><p>For every RTT (using linux <code>tcp_info</code>), update the current BDP.</p></li><li><p>Double the request body buffer size when the current BDP &gt; (receiver window / 4).</p></li></ul>\n          <div class=\"flex anchor relative\">\n            <h2 id=\"test-result\">Test Result</h2>\n            <a href=\"#test-result\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n              <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n            </a>\n          </div>\n          \n          <div class=\"flex anchor relative\">\n            <h3 id=\"lab-test\">Lab Test</h3>\n            <a href=\"#lab-test\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n              <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n            </a>\n          </div>\n        <p>Here is a test result when HTTP/2 autotune upload is enabled (still using <code>client_body_buffer_size</code> 128KB). You can see &quot;h2 autotune&quot; is doing pretty well – similar or even slightly faster than HTTP/1.1 speed (that&#39;s the initial goal). It might be slightly worse than a hand-picked optimal buffer size for given conditions, but you can see now NGINX picks up optimal buffer size automatically based on network conditions.</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/3Sl1rRUKOPiCoT5YeDqU6g/d98e69bdeab350ed312ffea8409f65fb/10MB-3.png\" alt=\"\" class=\"kg-image\" width=\"604\" height=\"372\" loading=\"lazy\"/>\n            \n            </figure>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/3keSehjT2WOcQVXA4QmDiy/dd5cfc026fe0aed6db3dc40132eedcd0/10MB-4.png\" alt=\"\" class=\"kg-image\" width=\"604\" height=\"370\" loading=\"lazy\"/>\n            \n            </figure>\n          <div class=\"flex anchor relative\">\n            <h3 id=\"production-test\">Production test</h3>\n            <a href=\"#production-test\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n              <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n            </a>\n          </div>\n        <p>After we deployed this feature, I ran similar tests against our production edge, uploading a 10MB file from well connected client nodes to our edge. I created a Linux VM instance in Google Cloud and ran the upload test where the network is very fast (a few Gbps) and low latency (&lt;10ms).</p><p>Here is when I run the test in the Google Cloud Belgium region to our CDG (Paris) PoP which has 7ms RTT. This looks very good with almost 3x improvement.</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/MgWAhBWB91UiGKOWULns6/26341245c67320b83ac0bfbebf89d1f9/10MB-5.png\" alt=\"\" class=\"kg-image\" width=\"600\" height=\"368\" loading=\"lazy\"/>\n            \n            </figure><p>I also tested between the Google Cloud Tokyo region and our NRT (Tokyo) PoP, which had a 2.3ms RTT. Although this is not realistic for home users, the results are interesting. A 128KB fixed buffer performs well, but HTTP/2 with buffer autotune outperforms even HTTP/1.1.</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/33aKI4Dguh46tXuIV9INyv/a1b8e022c44f61ae85a85119d1890016/10MB-6.png\" alt=\"\" class=\"kg-image\" width=\"599\" height=\"369\" loading=\"lazy\"/>\n            \n            </figure>\n          <div class=\"flex anchor relative\">\n            <h2 id=\"summary\">Summary</h2>\n            <a href=\"#summary\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n              <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n            </a>\n          </div>\n          <p>HTTP/2 upload buffer autotuning is now fully deployed in the Cloudflare edge. Customers should now benefit from improved upload performance for all HTTP/2 connections, including speed tests on speed.cloudflare.com. Autotuning upload buffer size logic works well for most cases, and now HTTP/2 upload is much faster than before! When we think about the performance we usually tend to think about download speed or latency reduction, but faster uploading can also help users working from home when they need a large amount of upload, such as photo/video sharing apps, content creation, video conferencing or self broadcasting.</p><p>Many thanks to <a href=\"/author/lucas/\">Lucas Pardue</a> and <a href=\"/author/rustam-lalkaka/\">Rustam Lalkaka</a> for great feedback and suggestions on the article.</p><p>Update: we made this patch available to public and sent to NGINX upstream. You can find it <a href=\"http://mailman.nginx.org/pipermail/nginx-devel/2020-August/013436.html\">here</a>.</p>",
		"id": "77KoGnJkqSvxBbUjDx9PhH",
		"localeList": {
			"name": "Delivering HTTP/2 upload speed improvements Config",
			"enUS": "English for Locale",
			"zhCN": "No Page for Locale",
			"zhHansCN": "No Page for Locale",
			"zhTW": "No Page for Locale",
			"frFR": "No Page for Locale",
			"deDE": "No Page for Locale",
			"itIT": "No Page for Locale",
			"jaJP": "No Page for Locale",
			"koKR": "Translated for Locale",
			"ptBR": "No Page for Locale",
			"esLA": "No Page for Locale",
			"esES": "No Page for Locale",
			"enAU": "No Page for Locale",
			"enCA": "No Page for Locale",
			"enIN": "No Page for Locale",
			"enGB": "No Page for Locale",
			"idID": "No Page for Locale",
			"ruRU": "No Page for Locale",
			"svSE": "No Page for Locale",
			"viVN": "No Page for Locale",
			"plPL": "No Page for Locale",
			"arAR": "No Page for Locale",
			"nlNL": "No Page for Locale",
			"thTH": "No Page for Locale",
			"trTR": "No Page for Locale",
			"heIL": "No Page for Locale",
			"lvLV": "No Page for Locale",
			"etEE": "No Page for Locale",
			"ltLT": "No Page for Locale"
		},
		"meta_description": "Cloudflare recently shipped improved upload speeds across our network for clients using HTTP/2. This post describes our journey from troubleshooting an issue to fixing it and delivering faster upload speeds to the global Internet.",
		"metadata": {
			"title": "Delivering HTTP/2 upload speed improvements",
			"description": "Cloudflare recently shipped improved upload speeds across our network for clients using HTTP/2. This post describes our journey from troubleshooting an issue to fixing it and delivering faster upload speeds to the global Internet.",
			"imgPreview": "https://cf-assets.www.cloudflare.com/slt3lc6tev37/6rhdmwJ46Se4KmDBBsDMPk/fbf1cbe18b868250c773b06ed50143a4/delivering-http-2-upload-speed-improvements-VHezVJ.png"
		},
		"primary_author": {},
		"published_at": "2020-08-24T12:00:00.000+01:00",
		"slug": "delivering-http-2-upload-speed-improvements",
		"tags": [
			{
				"id": "3lxBe7kJ3IDjqgqMaBU1QJ",
				"name": "HTTP2",
				"slug": "http2"
			},
			{
				"id": "3TaNOBp8EhAE6eCfshf1tK",
				"name": "Cloudflare Network",
				"slug": "cloudflare-network"
			},
			{
				"id": "1U6ifhBwTuaJ2w4pjNOzNT",
				"name": "Network",
				"slug": "network"
			}
		],
		"title": "Delivering HTTP/2 upload speed improvements",
		"updated_at": "2024-08-27T01:59:23.957Z",
		"url": "https://blog.cloudflare.com/delivering-http-2-upload-speed-improvements"
	},
	"translations": {
		"posts.by": "By",
		"footer.gdpr": "GDPR",
		"lang_blurb1": "This post is also available in {lang1}.",
		"lang_blurb2": "This post is also available in {lang1} and {lang2}.",
		"lang_blurb3": "This post is also available in {lang1}, {lang2} and {lang3}.",
		"footer.press": "Press",
		"header.title": "The Cloudflare Blog",
		"search.clear": "Clear",
		"search.filter": "Filter",
		"search.source": "Source",
		"footer.careers": "Careers",
		"footer.company": "Company",
		"footer.support": "Support",
		"footer.the_net": "theNet",
		"search.filters": "Filters",
		"footer.our_team": "Our team",
		"footer.webinars": "Webinars",
		"page.more_posts": "More posts",
		"posts.time_read": "{time} min read",
		"search.language": "Language",
		"footer.community": "Community",
		"footer.resources": "Resources",
		"footer.solutions": "Solutions",
		"footer.trademark": "Trademark",
		"header.subscribe": "Subscribe",
		"footer.compliance": "Compliance",
		"footer.free_plans": "Free plans",
		"footer.impact_ESG": "Impact/ESG",
		"posts.follow_on_X": "Follow on X",
		"footer.help_center": "Help center",
		"footer.network_map": "Network Map",
		"header.please_wait": "Please Wait",
		"page.related_posts": "Related posts",
		"search.result_stat": "Results <strong>{search_range}</strong> of <strong>{search_total}</strong> for <strong>{search_keyword}</strong>",
		"footer.case_studies": "Case Studies",
		"footer.connect_2024": "Connect 2024",
		"footer.terms_of_use": "Terms of Use",
		"footer.white_papers": "White Papers",
		"footer.cloudflare_tv": "Cloudflare TV",
		"footer.community_hub": "Community Hub",
		"footer.compare_plans": "Compare plans",
		"footer.contact_sales": "Contact Sales",
		"header.contact_sales": "Contact Sales",
		"header.email_address": "Email Address",
		"page.error.not_found": "Page not found",
		"footer.developer_docs": "Developer docs",
		"footer.privacy_policy": "Privacy Policy",
		"footer.request_a_demo": "Request a demo",
		"page.continue_reading": "Continue reading",
		"footer.analysts_report": "Analyst reports",
		"footer.for_enterprises": "For enterprises",
		"footer.getting_started": "Getting Started",
		"footer.learning_center": "Learning Center",
		"footer.project_galileo": "Project Galileo",
		"pagination.newer_posts": "Newer Posts",
		"pagination.older_posts": "Older Posts",
		"posts.social_buttons.x": "Discuss on X",
		"search.source_location": "Source/Location",
		"footer.about_cloudflare": "About Cloudflare",
		"footer.athenian_project": "Athenian Project",
		"footer.become_a_partner": "Become a partner",
		"footer.cloudflare_radar": "Cloudflare Radar",
		"footer.network_services": "Network services",
		"footer.trust_and_safety": "Trust & Safety",
		"header.get_started_free": "Get Started Free",
		"page.search.placeholder": "Search Cloudflare",
		"footer.cloudflare_status": "Cloudflare Status",
		"footer.cookie_preference": "Cookie Preferences",
		"header.valid_email_error": "Must be valid email.",
		"footer.connectivity_cloud": "Connectivity cloud",
		"footer.developer_services": "Developer services",
		"footer.investor_relations": "Investor relations",
		"page.not_found.error_code": "Error Code: 404",
		"footer.logos_and_press_kit": "Logos & press kit",
		"footer.application_services": "Application services",
		"footer.get_a_recommendation": "Get a recommendation",
		"posts.social_buttons.reddit": "Discuss on Reddit",
		"footer.sse_and_sase_services": "SSE and SASE services",
		"page.not_found.outdated_link": "You may have used an outdated link, or you may have typed the address incorrectly.",
		"footer.report_security_issues": "Report Security Issues",
		"page.error.error_message_page": "Sorry, we can't find the page you are looking for.",
		"header.subscribe_notifications": "Subscribe to receive notifications of new posts:",
		"footer.cloudflare_for_campaigns": "Cloudflare for Campaigns",
		"header.subscription_confimation": "Subscription confirmed. Thank you for subscribing!",
		"posts.social_buttons.hackernews": "Discuss on Hacker News",
		"footer.diversity_equity_inclusion": "Diversity, equity & inclusion",
		"footer.critical_infrastructure_defense_project": "Critical Infrastructure Defense Project"
	}
}