<div class="mb2 gray5">17 min read</div>
<div class="mt4">This post is also available in <a href="https://blog.cloudflare.com/zh-cn/big-pineapple-intro">简体中文</a>, <a href="https://blog.cloudflare.com/fr-fr/big-pineapple-intro">Français</a>, <a href="https://blog.cloudflare.com/de-de/big-pineapple-intro">Deutsch</a>, <a href="https://blog.cloudflare.com/ja-jp/big-pineapple-intro">日本語</a>, <a href="https://blog.cloudflare.com/ko-kr/big-pineapple-intro">한국어</a>, <a href="https://blog.cloudflare.com/es-es/big-pineapple-intro">Español</a> and <a href="https://blog.cloudflare.com/zh-tw/big-pineapple-intro">繁體中文</a>.</div>
<div class="post-content lh-copy gray1">
	<p></p>
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/4tun8W7xGXu4HnA6zxJK7b/7fda6aa6d5da9c5108ba71f249f8620f/image1-11.png" alt="" class="kg-image" width="1200" height="628" loading="lazy">

	</figure>
	<p>≈</p>
	<p>On April 1, 2018, Cloudflare <a href="https://blog.cloudflare.com/dns-resolver-1-1-1-1">announced</a> the 1.1.1.1 public DNS resolver. Over the years, we added the <a href="https://1.1.1.1/help">debug page</a> for troubleshooting, global <a href="https://1.1.1.1/purge-cache">cache purge</a>, 0 TTL for zones on Cloudflare, <a href="https://blog.cloudflare.com/encrypting-dns-end-to-end">Upstream TLS</a>, and <a href="https://blog.cloudflare.com/introducing-1-1-1-1-for-families">1.1.1.1 for families</a> to the platform. In this post, we would like to share some behind the scenes details and changes.</p>
	<p>When the project started, <a href="https://www.knot-resolver.cz">Knot Resolver</a> was chosen as the DNS resolver. We started building a whole system on top of it, so that it could fit Cloudflare's use case. Having a battle tested DNS recursive resolver, as well as a DNSSEC validator, was fantastic because we could spend our energy elsewhere, instead of worrying about the DNS protocol implementation.</p>
	<p>Knot Resolver is quite flexible in terms of its Lua-based plugin system. It allowed us to quickly extend the core functionality to support various product features, like DoH/DoT, logging, BPF-based attack mitigation, cache sharing, and iteration logic override. As the <a href="https://mobile.twitter.com/eastdakota/status/1103800276102729729">traffic grew</a>, we reached certain limitations.</p>
	<h2>Lessons we learned</h2>
	<p>Before going any deeper, let’s first have a bird’s-eye view of a simplified Cloudflare data center setup, which could help us understand what we are going to talk about later. At Cloudflare, every server is identical: the software stack running on one server is exactly the same as on another server, only the configuration may be different. This setup greatly reduces the complexity of fleet maintenance.</p>
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/346cYMKrtotPKZx6GcoDMX/881e0b136c86c6a806e7c0df54cb6e39/colo_kresd.png" alt="" class="kg-image" width="1600" height="406" loading="lazy">

	</figure>
	<p>Figure 1 Data center layout</p>
	<p>The resolver runs as a daemon process, kresd, and it doesn’t work alone. Requests, specifically DNS requests, are load-balanced to the servers inside a data center by <a href="https://blog.cloudflare.com/unimog-cloudflares-edge-load-balancer">Unimog</a>. DoH requests are terminated at our TLS terminator. Configs and other small pieces of data can be delivered worldwide by <a href="https://blog.cloudflare.com/introducing-quicksilver-configuration-distribution-at-internet-scale">Quicksilver</a> in seconds. With all the help, the resolver can concentrate on its own goal - resolving DNS queries, and not worrying about transport protocol details. Now let’s talk about 3 key areas we wanted to improve here - blocking I/O in plugins, a more efficient use of cache space, and plugin isolation.</p>
	<h3>Callbacks blocking the event loop</h3>
	<p>Knot Resolver has a very flexible plugin system for extending its core functionality. The plugins are called modules, and they are based on callbacks. At certain points during request processing, these callbacks will be invoked with current query context. This gives a module the ability to inspect, modify, and even produce requests / responses. By design, these callbacks are supposed to be simple, in order to avoid blocking the underlying event loop. This matters because the service is single threaded, and the event loop is in charge of serving many requests at the same time. So even just one request being held up in a callback means that no other concurrent requests can be progressed until the callback finishes.</p>
	<p>The setup worked well enough for us until we needed to do blocking operations, for example, to pull data from Quicksilver before responding to the client.</p>
	<h3>Cache efficiency</h3>
	<p>As requests for a domain could land on any node inside a data center, it would be wasteful to repetitively resolve a query when another node already has the answer. By intuition, the latency could be improved if the cache could be shared among the servers, and so we created a cache module which multicasted the newly added cache entries. Nodes inside the same data center could then subscribe to the events and update their local cache.</p>
	<p>The default cache implementation in Knot Resolver is <a href="https://www.symas.com/lmdb">LMDB</a>. It is fast and reliable for small to medium deployments. But in our case, cache eviction shortly became a problem. The cache itself doesn’t track for any TTL, popularity, etc. When it’s full, it just clears all the entries and starts over. Scenarios like zone enumeration could fill the cache with data that is unlikely to be retrieved later.</p>
	<p>Furthermore, our multicast cache module made it worse by amplifying the less useful data to all the nodes, and led them to the cache high watermark at the same time. Then we saw a latency spike because all the nodes dropped the cache and started over around the same time.</p>
	<h3>Module isolation</h3>
	<p>With the list of Lua modules increasing, debugging issues became increasingly difficult. This is because a single Lua state is shared among all the modules, so one misbehaving module could affect another. For example, when something went wrong inside the Lua state, like having too many coroutines, or being out of memory, we got lucky if the program just crashed, but the resulting stack traces were hard to read. It is also difficult to forcibly tear down, or upgrade, a running module as it not only has state in the Lua runtime, but also FFI, so memory safety is not guaranteed.</p>
	<h2>Hello BigPineapple</h2>
	<p>We didn’t find any existing software that would meet our somewhat niche requirements, so eventually we started building something ourselves. The first attempt was to <a href="https://github.com/vavrusa/rust-kres">wrap Knot Resolver's core</a> with a thin service written in Rust (modified <a href="https://github.com/jedisct1/edgedns">edgedns</a>).</p>
	<p>This proved to be difficult due to having to constantly convert between the storage, and C/FFI types, and some other quirks (for example, the ABI for looking up records from cache expects the returned records to be immutable until the next call, or the end of the read transaction). But we learned a lot from trying to implement this sort of split functionality where the host (the service) provides some resources to the guest (resolver core library), and how we would make that interface better.</p>
	<p>In the later iterations, we replaced the entire recursive library with a new one based around an async runtime; and a redesigned module system was added to it, sneakily rewriting the service into Rust over time as we swapped out more and more components. That async runtime was <a href="https://tokio.rs">tokio</a>, which offered a neat thread pool interface for running both non-blocking and blocking tasks, as well as a good ecosystem for working with other crates (Rust libraries).</p>
	<p>After that, as the futures combinators became tedious, we started converting everything to async/await. This was before the async/await feature that landed in Rust 1.39, which led us to use nightly (Rust beta) for a while and had <a href="https://areweasyncyet.rs">some hiccups</a>. When the async/await stabilized, it enabled us to write our request processing routine ergonomically, similar to Go.</p>
	<p>All the tasks can be run concurrently, and certain I/O heavy ones can be broken down into smaller pieces, to benefit from a more granular scheduling. As the runtime executes tasks on a threadpool, instead of a single thread, it also benefits from work stealing. This avoids a problem we previously had, where a single request taking a lot of time to process, that blocks all the other requests on the event loop.</p>
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/72roTqirnpOZTjQpp36q4t/ed63d1f381d3b92a7e84ad20d63428c2/blog_server.png" alt="" class="kg-image" width="1600" height="725" loading="lazy">

	</figure>
	<p>Figure 2 Components overview</p>
	<p>Finally, we forged a platform that we are happy with, and we call it <b>BigPineapple</b>. The figure above shows an overview of its main components and the data flow between them. Inside BigPineapple, the server module gets inbound requests from the client, validates and transforms them into unified frame streams, which can then be processed by the worker module. The worker module has a set of workers, whose task is to figure out the answer to the question in the request. Each worker interacts with the cache module to check if the answer is there and still valid, otherwise it drives the recursor module to recursively iterate the query. The recursor doesn’t do any I/O, when it needs anything, it delegates the sub-task to the conductor module. The conductor then uses outbound queries to get the information from upstream nameservers. Through the whole process, some modules can interact with the Sandbox module, to extend its functionality by running the plugins inside.</p>
	<p>Let’s look at some of them in more detail, and see how they helped us overcome the problems we had before.</p>
	<h3>Updated I/O architecture</h3>
	<p>A DNS resolver can be seen as an agent between a client and several authoritative nameservers: it receives requests from the client, recursively fetches data from the upstream nameservers, then composes the responses and sends them back to the client. So it has both inbound and outbound traffic, which are handled by the server and the conductor component respectively.</p>
	<p>The server listens on a list of interfaces using different transport protocols. These are later abstracted into streams of “frames”. Each frame is a high level representation of a DNS message, with some extra metadata. Underneath, it can be a UDP packet, a segment of TCP stream, or the payload of a HTTP request, but they are all processed the same way. The frame is then converted into an asynchronous task, which in turn is picked up by a set of workers in charge of resolving these tasks. The finished tasks are converted back into responses, and sent back to the client.</p>
	<p>This “frame” abstraction over the protocols and their encodings simplified the logic used to regulate the frame sources, such as enforcing fairness to prevent starving and controlling pacing to protect the server from being overwhelmed. One of the things we’ve learned with the previous implementations is that, for a service open to the public, a peak performance of the I/O matters less than the ability to pace clients fairly. This is mainly because the time and computational cost of each recursive request is vastly different (for example a cache hit from a cache miss), and it’s difficult to guess it beforehand. The cache misses in recursive service not only consume Cloudflare’s resources, but also the resources of the authoritative nameservers being queried, so we need to be mindful of that.</p>
	<p>On the other side of the server is the conductor, which manages all the outbound connections. It helps to answer some questions before reaching out to the upstream: Which is the fastest nameserver to connect to in terms of latency? What to do if all the nameservers are not reachable? What protocol to use for the connection, and are there any <a href="https://engineering.fb.com/2018/12/21/security/dns-over-tls">better options</a>? The conductor is able to make these decisions by tracking the upstream server’s metrics, such as <a href="https://www.cloudflare.com/learning/cdn/glossary/round-trip-time-rtt">RTT</a>, QoS, etc. With that knowledge, it can also guess for things like upstream capacity, UDP packet loss, and take necessary actions, e.g. retry when it thinks the previous UDP packet didn’t reach the upstream.</p>
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/3oLCVKn5GkZd5SmRnuYSmL/c6fc57f79b72c1cd36008ae48c38e7dc/conductor-1-.png" alt="" class="kg-image" width="1600" height="1030" loading="lazy">

	</figure>
	<p>Figure 3 I/O conductor</p>
	<p>Figure 3 shows a simplified data flow about the conductor. It is called by the exchanger mentioned above, with upstream requests as input. The requests will be deduplicated first: meaning in a small window, if a lot of requests come to the conductor and ask for the same question, only one of them will pass, the others are put into a waiting queue. This is common when a cache entry expires, and can reduce unnecessary network traffic. Then based on the request and upstream metrics, the connection instructor either picks an open connection if available, or generates a set of parameters. With these parameters, the I/O executor is able to connect to the upstream directly, or even take a route via another Cloudflare data center using our <a href="https://blog.cloudflare.com/argo">Argo Smart Routing technology</a>!</p>
	<h3>The cache</h3>
	<p>Caching in a recursive service is critical as a server can return a cached response in under one millisecond, while it will be hundreds of milliseconds to respond on a cache miss. As the memory is a finite resource (and also a shared resource in Cloudflare’s architecture), more efficient use of space for cache was one of the key areas we wanted to improve. The new cache is implemented with a cache replacement data structure (<a href="https://en.wikipedia.org/wiki/Adaptive_replacement_cache">ARC</a>), instead of a KV store. This makes good use of the space on a single node, as less popular entries are progressively evicted, and the data structure is resistant to scans.</p>
	<p>Moreover, instead of duplicating the cache across the whole data center with multicast, as we did before, BigPineapple is aware of its peer nodes in the same data center, and relays queries from one node to another if it cannot find an entry in its own cache. This is done by consistent hashing the queries onto the healthy nodes in each data center. So, for example, queries for the same registered domain go through the same subset of nodes, which not only increases the cache hit ratio, but also helps the infrastructure cache, which stores information about performance and features of nameservers.</p>
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/5Ddu8jHnfdToCysh4urr1V/b13a37129129ef99499f22fca50e1d0d/colo_3_bp.png" alt="" class="kg-image" width="1600" height="789" loading="lazy">

	</figure>
	<p>Figure 4 Updated data center layout</p>
	<h3>Async recursive library</h3>
	<p>The recursive library is the DNS brain of BigPineapple, as it knows how to find the answer to the question in the query. Starting from the root, it breaks down the client query into subqueries, and uses them to collect knowledge recursively from various authoritative nameservers on the internet. The product of this process is the answer. Thanks to the async/await it can be abstracted as a function like such:</p>
	<pre class="language-bash"><code class="language-bash">async fn resolve(Request, Exchanger) → Result&lt;Response&gt;;</code></pre>
	<p>The function contains all the logic necessary to generate a response to a given request, but it doesn’t do any I/O on its own. Instead, we pass an Exchanger trait (Rust interface) that knows how to exchange DNS messages with upstream authoritative nameservers asynchronously. The exchanger is usually called at various await points - for example, when a recursion starts, one of the first things it does is that it looks up the closest cached delegation for the domain. If it doesn’t have the final delegation in cache, it needs to ask what nameservers are responsible for this domain and wait for the response, before it can proceed any further.</p>
	<p>Thanks to this design, which decouples the “waiting for some responses” part from the recursive DNS logic, it is much easier to test by providing a mock implementation of the exchanger. In addition, it makes the recursive iteration code (and DNSSEC validation logic in particular) much more readable, as it’s written sequentially instead of being scattered across many callbacks.</p>
	<p>Fun fact: writing a DNS recursive resolver from scratch is not fun at all!</p>
	<p>Not only because of the complexity of DNSSEC validation, but also because of the necessary “workarounds” needed for various RFC incompatible servers, forwarders, firewalls, etc. So we ported <a href="https://github.com/CZ-NIC/deckard">deckard</a> into Rust to help test it. Additionally, when we started migrating over to this new async recursive library, we first ran it in “shadow” mode: processing real world query samples from the production service, and comparing differences. We’ve done this in the past on Cloudflare’s authoritative DNS service as well. It is slightly more difficult for a recursive service due to the fact that a recursive service has to look up all the data over the Internet, and authoritative nameservers often give different answers for the same query due to localization, load balancing and such, leading to many false positives.</p>
	<p>In December 2019, we finally enabled the new service on a public test endpoint (see the <a href="https://community.cloudflare.com/t/help-us-test-a-new-version-of-1-1-1-1-public-dns-resolver/137078">announcement</a>) to iron out remaining issues before slowly migrating the production endpoints to the new service. Even after all that, we continued to find edge cases with the DNS recursion (and DNSSEC validation in particular), but fixing and reproducing these issues has become much easier due to the new architecture of the library.</p>
	<h3>Sandboxed plugins</h3>
	<p>Having the ability to extend the core DNS functionality on the fly is important for us, thus BigPineapple has its redesigned plugin system. Before, the Lua plugins run in the same memory space as the service itself, and are generally free to do what they want. This is convenient, as we can freely pass memory references between the service and modules using C/FFI. For example, to read a response directly from cache without having to copy to a buffer first. But it is also dangerous, as the module can read uninitialized memory, call a host ABI using a wrong function signature, block on a local socket, or do other undesirable things, in addition the service doesn’t have a way to restrict these behaviors.</p>
	<p>So we looked at replacing the embedded Lua runtime with JavaScript, or native modules, but around the same time, embedded runtimes for WebAssembly (Wasm for short) started to appear. Two nice properties of WebAssembly programs are that it allows us to write them in the same language as the rest of the service, and that they run in an isolated memory space. So we started modeling the guest/host interface around the limitations of WebAssembly modules, to see how that would work.</p>
	<p>BigPineapple’s Wasm runtime is currently powered by <a href="https://wasmer.io">Wasmer</a>. We tried several runtimes over time like <a href="https://wasmtime.dev">Wasmtime</a>, <a href="https://wavm.github.io">WAVM</a> in the beginning, and found Wasmer was simpler to use in our case. The runtime allows each module to run in its own instance, with an isolated memory and a signal trap, which naturally solved the module isolation problem we described before. In addition to this, we can have multiple instances of the same module running at the same time. Being controlled carefully, the apps can be hot swapped from one instance to another without missing a single request! This is great because the apps can be upgraded on the fly without a server restart. Given that the Wasm programs are distributed via Quicksilver, BigPineapple’s functionality can be safely changed worldwide within a few seconds!</p>
	<p>To better understand the WebAssembly sandbox, several terms need to be introduced first:</p>
	<ul>
		<li>
			<p>Host: the program which runs the Wasm runtime. Similar to a kernel, it has full control through the runtime over the guest applications.</p>
		</li>
		<li>
			<p>Guest application: the Wasm program inside the sandbox. Within a restricted environment, it can only access its own memory space, which is provided by the runtime, and call the imported Host calls. We call it an app for short.</p>
		</li>
		<li>
			<p>Host call: the functions defined in the host that can be imported by the guest. Comparable to syscall, it’s the only way guest apps can access the resources outside the sandbox.</p>
		</li>
		<li>
			<p>Guest runtime: a library for guest applications to easily interact with the host. It implements some common interfaces, so an app can just use async, socket, log and tracing without knowing the underlying details.</p>
		</li>
	</ul>
	<p>Now it’s time to dive into the sandbox, so stay awhile and listen. First let’s start from the guest side, and see what a common app lifespan looks like. With the help of the guest runtime, guest apps can be written similar to regular programs. So like other executables, an app begins with a start function as an entrypoint, which is called by the host upon loading. It is also provided with arguments as from the command line. At this point, the instance normally does some initialization, and more importantly, registers callback functions for different query phases. This is because in a recursive resolver, a query has to go through several phases before it gathers enough information to produce a response, for example a cache lookup, or making subrequests to resolve a delegation chain for the domain, so being able to tie into these phases is necessary for the apps to be useful for different use cases. The start function can also run some background tasks to supplement the phase callbacks, and store global state. For example - report metrics, or pre-fetch shared data from external sources, etc. Again, just like how we write a normal program.</p>
	<p>But where do the program arguments come from? How could a guest app send log and metrics? The answer is, external functions.</p>
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/2rMKxgaKCweGenTEf3kQ9U/d906a07de4668a94dd7c686c429e0282/sandbox-1-.png" alt="" class="kg-image" width="1600" height="1362" loading="lazy">

	</figure>
	<p>Figure 5 Wasm based Sandbox</p>
	<p>In figure 5, we can see a barrier in the middle, which is the sandbox boundary, that separates the guest from the host. The only way one side can reach out to the other, is via a set of functions exported by the peer beforehand. As in the picture, the “hostcalls” are exported by the host, imported and called by the guest; while the “trampoline” are guest functions that the host has knowledge of.</p>
	<p>It is called <a href="https://en.wikipedia.org/wiki/Trampoline_(computing)">trampoline</a> because it is used to invoke a function or a closure inside a guest instance that’s not exported. The phase callbacks are one example of why we need a trampoline function: each callback returns a closure, and therefore can’t be exported on instantiation. So a guest app wants to register a callback, it calls a host call with the callback address “<code>hostcall_register_callback(pre_cache, #30987)</code>”, when the callback needs to be invoked, the host cannot just call that pointer as it’s pointing to the guest’s memory space. What it can do instead is, to leverage one of the aforementioned trampolines, and give it the address of the callback closure: “<code>trampoline_call(#30987)</code>”.</p>
	<p><b>Isolation overhead</b>Like a coin that has two sides, the new sandbox does come with some additional overhead. The portability and isolation that WebAssembly offers bring extra cost. Here, we'll list two examples.</p>
	<p>Firstly, guest apps are not allowed to read host memory. The way it works is the guest provides a memory region via a host call, then the host writes the data into the guest memory space. This introduces a memory copy that would not be needed if we were outside the sandbox. The bad news is, in our use case, the guest apps are supposed to do something on the query and/or the response, so they almost always need to read data from the host on every single request. The good news, on the other hand, is that during a request life cycle, the data won’t change. So we pre-allocate a bulk of memory in the guest memory space right after the guest app instantiates. The allocated memory is not going to be used, but instead serves to occupy a hole in the address space. Once the host gets the address details, it maps a shared memory region with the common data needed by the guest into the guest’s space. When the guest code starts to execute, it can just access the data in the shared memory overlay, and no copy is needed.</p>
	<p>Another issue we ran into was when we wanted to add support for a modern protocol, <a href="https://blog.cloudflare.com/oblivious-dns">oDoH</a>, into BigPineapple. The main job of it is to decrypt the client query, resolve it, then encrypt the answers before sending it back. By design, this doesn’t belong to core DNS, and should instead be extended with a Wasm app. However, the WebAssembly instruction set doesn’t provide some crypto primitives, such as AES and SHA-2, which prevents it from getting the benefit of host hardware. There is ongoing work to bring this functionality to Wasm with <a href="https://github.com/WebAssembly/wasi-crypto">WASI-crypto</a>. Until then, our solution for this is to simply delegate the <a href="https://blog.cloudflare.com/hybrid-public-key-encryption">HPKE</a> to the host via host calls, and we already saw 4x performance improvements, compared to doing it inside Wasm.</p>
	<p><b>Async in Wasm</b>Remember the problem we talked about before that the callbacks could block the event loop? Essentially, the problem is how to run the sandboxed code asynchronously. Because no matter how complex the request processing callback is, if it can yield, we can put an upper bound on how long it is allowed to block. Luckily, Rust’s async framework is both elegant and lightweight. It gives us the opportunity to use a set of guest calls to implement the “Future”s.</p>
	<p>In Rust, a Future is a building block for asynchronous computations. From the user’s perspective, in order to make an asynchronous program, one has to take care of two things: implement a pollable function that drives the state transition, and place a waker as a callback to wake itself up, when the pollable function should be called again due to some external event (e.g. time passes, socket becomes readable, and so on). The former is to be able to progress the program gradually, e.g. read buffered data from I/O and return a new state indicating the status of the task: either finished, or yielded. The latter is useful in case of task yielding, as it will trigger the Future to be polled when the conditions that the task was waiting for are fulfilled, instead of busy looping until it’s complete.</p>
	<p>Let’s see how this is implemented in our sandbox. For a scenario when the guest needs to do some I/O, it has to do so via the host calls, as it is inside a restricted environment. Assuming the host provides a set of simplified host calls which mirror the basic socket operations: open, read, write, and close, the guest can have its pseudo poller defined as below:</p>
	<pre class="language-bash"><code class="language-bash">fn poll(&amp;mut self, wake: fn()) -&gt; Poll {
	match hostcall_socket_read(self.sock, self.buffer) {
    	    HostOk  =&gt; Poll::Ready,
    	    HostEof =&gt; Poll::Pending,
	}
}</code></pre>
	<p>Here the host call reads data from a socket into a buffer, depending on its return value, the function can move itself to one of the states we mentioned above: finished(Ready), or yielded(Pending). The magic happens inside the host call. Remember in figure 5, that it is the only way to access resources? The guest app doesn’t own the socket, but it can acquire a “<code>handle” via “hostcall_socket_open</code>”, which will in turn create a socket on the host side, and return a handle. The handle can be anything in theory, but in practice using integer socket handles map well to file descriptors on the host side, or indices in a <a href="https://www.cloudflare.com/learning/ai/what-is-vector-database">vector</a> or slab. By referencing the returned handle, the guest app is able to remotely control the real socket. As the host side is fully asynchronous, it can simply relay the socket state to the guest. If you noticed that the waker function isn’t used above, well done! That’s because when the host call is called, it not only starts opening a socket, but also registers the current waker to be called then the socket is opened (or fails to do so). So when the socket becomes ready, the host task will be woken up, it will find the corresponding guest task from its context, and wakes it up using the trampoline function as shown in figure 5. There are other cases where a guest task needs to wait for another guest task, an async mutex for example. The mechanism here is similar: using host calls to register wakers.</p>
	<p>All of these complicated things are encapsulated in our guest async runtime, with easy to use API, so the guest apps get access to regular async functions without having to worry about the underlying details.</p>
	<h2>(Not) The End</h2>
	<p>Hopefully, this blog post gave you a general idea of the innovative platform that powers 1.1.1.1. It is still evolving. As of today, several of our products, such as <a href="https://blog.cloudflare.com/introducing-1-1-1-1-for-families">1.1.1.1 for Families</a>, <a href="https://blog.cloudflare.com/the-as112-project">AS112</a>, and <a href="https://www.cloudflare.com/products/zero-trust/gateway">Gateway DNS</a>, are supported by guest apps running on BigPineapple. We are looking forward to bringing new technologies into it. If you have any ideas, please let us know in the <a href="https://community.cloudflare.com/c/zero-trust/dns-1111/47">community</a> or via <a href="mailto:resolver@cloudflare.com">email</a>.</p>
</div>