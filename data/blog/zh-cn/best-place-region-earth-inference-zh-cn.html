<div class="mb2 gray5">13 min read</div>
<div class="post-content lh-copy gray1">
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2023/09/unnamed-2.png" class="kg-image" alt="The best place on Region: Earth for inference" loading="lazy" width="1600" height="900"></figure>
	<p>如今，有超过 100 万开发人员利用 Cloudflare 的 Workers 平台来构建复杂的全栈应用程序，而这在以前是不可能实现的。</p>
	<p>当然，Workers 最初并不是这样的。它始于一个今天这样的日子，并以此开启了如今的<a href="https://blog.cloudflare.com/introducing-cloudflare-workers">生日周</a>。在 Workers 推出之时，它可能并没有如今这么多繁杂的功能，但如果您试着使用一次，就会产生这样的感觉：“这是不同的，它将改变一切”。突然之间，从一片空白到构建一个完全可扩展的全球应用程序，只需要几秒钟时间就能够实现，而不是几小时、几天、几周甚至几个月。这是以不同方式构建应用程序的开始。</p>
	<p>如果您在过去几个月内玩过生成式 AI，可能就会有类似的感觉。对一些朋友和同事进行调查后得知，我们的“顿悟”时刻都略有点不同，但此时整个行业的总体情绪是一致的——这是不同的，它将改变一切。</p>
	<p>今天，我们很高兴地发布一系列公告，我们相信它们会对未来计算领域产生重大影响，堪比 Workers 曾经带来的影响。这些公告是：</p>
	<ul>
		<li><strong>Workers AI</strong>（以前称为 Constellation），<strong>其在 Cloudflare 全球网络上的 NVIDIA GPU 上运行</strong>，将无服务器模型引入 AI——只需按使用量付费，让开发人员把更少的时间花费在基础结构上，而把更多的时间放在应用程序上。</li>
		<li><strong>Vectorize，我们的矢量数据库</strong>，其使索引和存储矢量变得简单、快速且经济实惠，以支持不仅需要访问正在运行的模型，还需要访问定制数据的用例。</li>
		<li><strong>AI Gateway</strong> 为组织提供<strong>缓存、限速和观察</strong> AI 部署的工具，无论其在何处运行。</li>
	</ul>
	<p>但这还不是全部。</p>
	<p>做大事是一项团队活动，我们不想单打独斗。就像我们做的很多事情一样，我们站在巨人的肩膀上。我们很高兴能与该领域的一些大公司合作：<strong>NVIDIA、Microsoft、Hugging Face 和 Meta</strong>。</p>
	<p>我们今天的发布仅仅只是 Cloudflare AI 领域旅程的开始，就像六年前的 Workers 所做的那样。虽然我们鼓励您深入了解我们今天发布的每一个公告（您不会失望的！），但我们也想借此机会退后一步，为您提供一些我们对于 AI 的更广泛的愿景，以及这些公告如何与这些愿景相适配。</p>
	<h3 id="%E6%8E%A8%E7%90%86%EF%BC%9Aai-%E5%B7%A5%E4%BD%9C%E8%B4%9F%E8%BD%BD%E7%9A%84%E6%9C%AA%E6%9D%A5">推理：AI 工作负载的未来</h3>
	<p>AI 涉及两个主要过程：训练和推理。</p>
	<p>训练生成式 AI 模型是一个长期运行（有时长达数月）的计算密集型过程，最终会产生一个模型。因此，训练工作负载最适合在传统的集中式云位置中运行。鉴于最近在获得对 GPU 的长期访问方面面临的挑战，导致公司转向多云，我们讨论了 R2 可以提供基本服务的方式，以消除从任何计算云访问训练数据的<a href="https://www.cloudflare.com/learning/cloud/what-are-data-egress-fees" target="_blank">出口费用</a>。但这不是我们今天要讨论的内容。</p>
	<p>虽然训练需要大量的前期资源，但更普遍的 AI 相关计算任务是推理。如果您最近向 ChatGPT 提出过问题、要求其生成了图像或翻译了一些文本，那么您就已经执行了推理任务。由于每次调用都需要进行推理（而不仅仅是一次），我们预计推理将成为与 AI 相关的主要工作负载。</p>
	<p>如果说训练最适合集中式云，那么推理的最佳场所是哪里？</p>
	<h3 id="%E7%BD%91%E7%BB%9C%E2%80%94%E2%80%94%E6%8E%A8%E7%90%86%E7%9A%84%E2%80%9C%E6%9C%80%E4%BD%B3%E5%9C%BA%E6%89%80%E2%80%9D">网络——推理的“最佳场所”</h3>
	<p>推理的显著特点是，推理的另一端通常有一个用户在等待。也就是说，这是一项对延迟敏感的任务。</p>
	<p>您可能会认为，对延迟敏感的任务最好放在设备上。在某些情况下可能是这样，但这存在一些问题。首先，设备上的硬件没有那么强大。电池寿命。</p>
	<p>另一方面，您可以使用集中式云计算。与设备不同，在集中式云位置运行的硬件马力十足。当然，问题在于它距离用户有几百毫秒的距离。有时，他们甚至跨越国界，这也带来了一系列挑战。</p>
	<p>因此，设备的功能不够强大，集中式云又过于遥远。这使得网络成为推理的最佳选择。距离不远，计算能力足够，恰到好处。</p>
	<h3 id="%E7%AC%AC%E4%B8%80%E4%B8%AA%E6%8E%A8%E7%90%86%E4%BA%91%EF%BC%8C%E5%9C%A8-region-earth-%E4%B8%8A%E8%BF%90%E8%A1%8C">第一个推理云，在 Region Earth 上运行</h3>
	<p>我们在构建开发人员平台的过程中学到的一个经验是，在网络规模下运行应用程序不仅有助于优化性能和规模（当然这也是一个很好的优势！），更重要的是，它为开发人员创建了正确的抽象层次，使他们能够快速移动。</p>
	<h3 id="workers-ai-%E7%94%A8%E4%BA%8E%E6%97%A0%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%8E%A8%E7%90%86">Workers AI 用于无服务器推理</h3>
	<p>在我们宣布推出 <a href="https://blog.cloudflare.com/workers-ai">Workers AI</a> 的同时，我们将推出第一个真正的无服务器 GPU 云，其与 Region Earth 完美匹配。无需机器学习专业知识，也不用翻遍 GPU。只需选择我们提供的型号之一即可开始。</p>
	<p>我们在设计 Workers AI 时花了很多心思，让部署模型的体验尽可能顺畅。</p>
	<p>如果您在 2023 年部署任何模型，其中一个模型很可能就是 <a href="https://www.cloudflare.com/learning/ai/what-is-large-language-model" target="_blank">LLM</a>。</p>
	<h3 id="vectorize-%E7%94%A8%E4%BA%8E%E5%AD%98%E5%82%A8%E7%9F%A2%E9%87%8F%EF%BC%81">Vectorize 用于存储矢量！</h3>
	<p>要构建端到端的 AI 聊天机器人，还需要一种方法来为用户提供 UI、解析要传递给用户的信息语料库（例如产品目录）、使用模型将其转换为嵌入式信息，并将其存储在某个地方。到目前为止，我们提供了前面两项所需的产品，但后者（存储嵌入）需要一个独特的解决方案：矢量数据库。</p>
	<p>正如我们宣布 Workers 时一样，我们不久后也宣布了 Workers KV——如果无法访问状态，您对计算几乎无能为力。AI 也是如此，要构建有意义的 AI 用例，您需要为 AI 提供对状态的访问权限。这就是<a href="https://www.cloudflare.com/learning/ai/what-is-vector-database" target="_blank">矢量数据库</a>发挥作用的地方，也是为什么今天我们同步推出我们自己的矢量数据库 Vectorize。</p>
	<h3 id="ai-gateway-%E7%94%A8%E4%BA%8E-ai-%E9%83%A8%E7%BD%B2%E7%9A%84%E7%BC%93%E5%AD%98%E3%80%81%E9%80%9F%E7%8E%87%E9%99%90%E5%88%B6%E5%92%8C%E5%8F%AF%E8%A7%81%E6%80%A7">AI Gateway 用于 AI 部署的缓存、速率限制和可见性</h3>
	<p>在 Cloudflare，当我们着手改进某项事物时，第一步始终是衡量它——如果无法衡量，更遑论改进？当我们听说客户在努力控制 AI 部署成本时，我们就在想我们该如何解决这个问题——衡量它，然后改进它。</p>
	<p>我们的 AI Gateway 可帮助您做到这两点！</p>
	<p>实时观察功能增强了主动管理能力，使监控、调试和微调 AI 部署变得更加容易。利用它来缓存、限制速率和监控 AI 部署，对于优化性能和有效管理成本至关重要。它可以缓存频繁使用的 AI 响应，减少延迟并提高系统可靠性，而速率限制则可确保高效的资源分配，减轻 AI 成本飙升带来的挑战。</p>
	<h3 id="%E4%B8%8E-meta-%E5%90%88%E4%BD%9C%EF%BC%8C%E5%B0%86-llama-2-%E5%BC%95%E5%85%A5%E6%88%91%E4%BB%AC%E7%9A%84%E5%85%A8%E7%90%83%E7%BD%91%E7%BB%9C">与 Meta 合作，将 Llama 2 引入我们的全球网络</h3>
	<p>直到最近，获得 LLM 的唯一途径还是调用专有模型。从时间、计算和财务资源方面来说，培训 LLM 都是一项重大投资，因此大多数开发人员都无法获得。Meta 发布的开源 LLM Llama 2 带来了令人兴奋的转变，允许开发人员运行和部署自己的 LLM。当然，有一个小细节除外，那就是您仍然需要使用 GPU 才能实现这一功能。</p>
	<p>通过将 Llama 2 作为 Workers AI 目录的一部分提供，我们希望让每个开发人员都能使用 LLM，而无需任何配置。</p>
	<p>当然，运行模型只是 AI 应用程序的一个组成部分。</p>
	<h3 id="%E5%88%A9%E7%94%A8-onnx-%E8%BF%90%E8%A1%8C%E6%97%B6%EF%BC%8C%E8%AE%A9%E5%BC%80%E5%8F%91%E4%BA%BA%E5%91%98%E5%9C%A8%E4%BA%91%E3%80%81%E8%BE%B9%E7%BC%98%E5%92%8C%E8%AE%BE%E5%A4%87%E4%B9%8B%E9%97%B4%E6%97%A0%E7%BC%9D%E7%A7%BB%E5%8A%A8">利用 ONNX 运行时，让开发人员在云、边缘和设备之间无缝移动</h3>
	<p>虽然边缘可能是解决其中许多问题的最佳位置，但我们预计，应用程序将继续部署在设备、边缘和集中式云范围内的其他位置。</p>
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2023/09/image1-22.png" class="kg-image" alt="" loading="lazy" width="1801" height="752"></figure>
	<p>以自动驾驶汽车为例，当您做出每毫秒都至关重要的决策时，您需要在设备上做出这些决策。相反，如果您希望运行千亿个参数版本的模型，集中式云将更适合您的工作负载。</p>
	<p>那么问题来了：如何在这些位置之间顺利穿梭？</p>
	<p>自从我们最初发布 Constellation（现在称为 Workers AI）以来，我们特别兴奋的一项技术是 ONNX 运行时。ONNX 运行时为运行模型创建了一个标准化环境，从而使在不同位置运行各种模型成为可能。</p>
	<p>我们已经说过，边缘是运行推理本身的绝佳场所，但它也可以作为路由层，帮助引导工作负载在所有三个位置顺利穿梭，这取决于使用案例以及您想要优化的内容——无论是延迟、准确性、成本、合规性还是隐私。</p>
	<h3 id="%E4%B8%8E-hugging-face-%E5%90%88%E4%BD%9C%EF%BC%8C%E6%8F%90%E4%BE%9B%E8%A7%A6%E6%89%8B%E5%8F%AF%E5%8F%8A%E7%9A%84%E4%BC%98%E5%8C%96%E6%A8%A1%E5%9E%8B">与 Hugging Face 合作，提供触手可及的优化模型</h3>
	<p>当然，没有什么比满足开发人员的需求更能帮助开发人员更快地完成任务了。因此，我们<a href="https://blog.cloudflare.com/partnering-with-hugging-face-deploying-ai-easier-affordable">与 Hugging Face</a>合作，将无服务器推理引入可用模型，让开发人员直接从中进行探索。</p>
	<h3 id="%E4%B8%8E-databricks-%E5%90%88%E4%BD%9C%E5%88%B6%E4%BD%9C-ai-%E6%A8%A1%E5%9E%8B">与 Databricks 合作制作 AI 模型</h3>
	<p>我们将与 Databricks 合作，为数据科学家和工程师带来 MLflow 的强大功能。MLflow 是一个用于管理端到端机器学习生命周期的开源平台，双方的合作将使用户更容易大规模部署和管理 ML 模型。通过此次合作，基于 Cloudflare Workers AI 进行构建的开发人员将能够利用与 MLFlow 兼容的模型，轻松部署到 Cloudflare 的全球网络中。开发人员可以使用 MLflow 将模型高效地打包、实施、直接部署到 Cloudflare 的无服务器开发人员平台并进行跟踪。</p>
	<p>不会让 CIO、CFO 或总法律顾问彻夜难眠的 AI</p>
	<p>AI 领域的发展日新月异，为开发人员提供他们所需的工具非常重要，但如果需要考虑很多重要因素，就很难快速发展。合规性、成本、隐私性如何？</p>
	<h3 id="%E5%90%88%E8%A7%84%E5%8F%8B%E5%A5%BD%E5%9E%8B-ai">合规友好型 AI</h3>
	<p>尽管我们大多数人不愿意考虑这一点，但 AI 和数据驻留正越来越受到政府的监管。由于政府要求在本地处理数据或将居民数据存储在国内，企业也必须在推理工作负载运行的环境中考虑这一点。在延迟方面，网络边缘提供了尽可能宽的范围。而说到合规性，一个横跨 300 座城市的网络和我们的数据本地化套件等产品的强大之处在于，它能够实现 AI 部署本地化所需的粒度。</p>
	<h3 id="%E9%A2%84%E7%AE%97%E5%8F%8B%E5%A5%BD%E5%9E%8B-ai">预算友好型 AI</h3>
	<p>与我们许多正在尝试 AI 的朋友和同事交谈时，大家似乎都有一个共鸣——AI 很昂贵。在将任何东西投入生产或从中实现价值之前，很容易让成本流失。我们 AI 平台的目的是让您能够负担得起成本，但更重要的是，我们只对您使用的部分收费。无论您是直接使用 Workers AI，还是使用我们的 AI Gateway，我们都希望提供必要的可见性和工具，以防止不知不觉增加 AI 支出。</p>
	<h3 id="%E9%9A%90%E7%A7%81%E5%8F%8B%E5%A5%BD%E5%9E%8B-ai">隐私友好型 AI</h3>
	<p>如果您将 AI 置于客户体验和业务运营的前沿和中心，那么您会想要确保通过 AI 运行的任何数据都在安全的地方。与 Cloudflare 一贯的做法一样，我们采取隐私第一的方针。我们可以向客户保证，我们不会使用通过 Cloudflare 传递的任何客户数据进行推理来训练大型语言模型。</p>
	<h3 id="%E5%AE%9E%E9%99%85%E4%B8%8A%EF%BC%8C%E6%88%91%E4%BB%AC%E6%89%8D%E5%88%9A%E5%88%9A%E5%BC%80%E5%A7%8B">实际上，我们才刚刚开始</h3>
	<p>我们刚刚开始使用人工智能，朋友们，我们正在进行一场疯狂的旅程！随着我们不断发掘这项技术的优势，我们不禁对未来的无限可能感到敬畏和惊叹。从彻底改变医疗保健到改变我们的工作方式，AI 正准备以我们从未想到过的方式改变游戏规则。各位，请系好安全带，因为 AI 的未来比以往任何时候都更加光明——我们已经迫不及待地想要知道接下来会发生什么！</p>
	<p>这条总结信息可能是由 AI 生成的，但其情感是真实的——这仅仅是个开始，我们迫不及待地想看到大家的构建成果。</p>
</div>