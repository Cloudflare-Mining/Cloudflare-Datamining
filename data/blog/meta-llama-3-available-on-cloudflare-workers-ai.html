<div class="mb2 gray5">2 min read</div>
<div class="post-content lh-copy gray1">
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/7oXcXb7KqFE8KtPFFQ375R/1baa0f1d79027dd1c8c125e059890140/image2-1-2.png" alt="Meta Llama 3 available on Cloudflare Workers AI" class="kg-image" width="1800" height="1013" loading="lazy">

	</figure>
	<p>We are thrilled to give developers around the world the ability to build AI applications with Meta Llama 3 using Workers AI. We are proud to be a launch partner with Meta for their newest 8B Llama 3 model, and excited to continue our partnership to bring the best of open-source models to our inference platform.</p>
	<h2>Workers AI</h2>
	<p><a href="https://blog.cloudflare.com/workers-ai">Workers AI’s initial launch</a> in beta included support for Llama 2, as it was one of the most requested open source models from the developer community. Since that initial launch, we’ve seen developers build all kinds of innovative applications including knowledge sharing <a href="https://workers.cloudflare.com/built-with/projects/ai.moda">chatbots</a>, creative <a href="https://workers.cloudflare.com/built-with/projects/Audioflare">content generation</a>, and automation for <a href="https://workers.cloudflare.com/built-with/projects/Azule">various workflows</a>. &nbsp;</p>
	<p>At Cloudflare, we know developers want simplicity and flexibility, with the ability to build with multiple AI models while optimizing for accuracy, performance, and cost, among other factors. Our goal is to make it as easy as possible for developers to use their models of choice without having to worry about the complexities of hosting or deploying models.</p>
	<p>As soon as we learned about the development of Llama 3 from our partners at Meta, we knew developers would want to start building with it as quickly as possible. Workers AI’s serverless inference platform makes it extremely easy and cost-effective to start using the latest large language models (LLMs). Meta’s commitment to developing and growing an open AI-ecosystem makes it possible for customers of all sizes to use AI at scale in production. All it takes is a few lines of code to run inference to Llama 3:</p>
	<pre class="language-js"><code class="language-js">export interface Env {
  // If you set another name in wrangler.toml as the value for 'binding',
  // replace "AI" with the variable name you defined.
  AI: any;
}

export default {
  async fetch(request: Request, env: Env) {
    const response = await env.AI.run('@cf/meta/llama-3-8b-instruct', {
        messages: [
{role: "user", content: "What is the origin of the phrase Hello, World?"}
	 ]
      }
    );

    return new Response(JSON.stringify(response));
  },
};</code></pre>
	<h2>Built with Meta Llama 3</h2>
	<p>Llama 3 offers leading performance on a wide range of industry benchmarks. You can learn more about the architecture and improvements on Meta’s <a href="https://ai.meta.com/blog/meta-llama-3">blog post</a>. Cloudflare Workers AI supports <a href="https://developers.cloudflare.com/workers-ai/models/llama-3-8b-instruct">Llama 3 8B</a>, including the instruction fine-tuned model.</p>
	<p>Meta’s testing shows that Llama 3 is the most advanced open LLM today on <a href="https://github.com/meta-llama/llama3/blob/main/eval_details.md?cf_target_id=1F7E4663A460CE17F25CF8ADDF6AB9F1">evaluation benchmarks</a> such as MMLU, GPQA, HumanEval, GSM-8K, and MATH. Llama 3 was trained on an increased number of training tokens (15T), allowing the model to have a better grasp on language intricacies. Larger context windows doubles the capacity of Llama 2, and allows the model to better understand lengthy passages with rich contextual data. Although the model supports a context window of 8k, we currently only support 2.8k but are looking to support 8k context windows through quantized models soon. As well, the new model introduces an efficient new <a href="https://github.com/openai/tiktoken">tiktoken</a>-based tokenizer with a vocabulary of 128k tokens, encoding more characters per token, and achieving better performance on English and multilingual benchmarks. This means that there are 4 times as many parameters in the embedding and output layers, making the model larger than the previous Llama 2 generation of models.</p>
	<p>Under the hood, Llama 3 uses <a href="https://arxiv.org/abs/2305.13245">grouped-query attention</a> (GQA), which improves inference efficiency for longer sequences and also renders their 8B model architecturally equivalent to <a href="https://developers.cloudflare.com/workers-ai/models/mistral-7b-instruct-v0.1">Mistral-7B</a>. For tokenization, it uses byte-level <a href="https://huggingface.co/learn/nlp-course/en/chapter6/5">byte-pair encoding (BPE)</a>, similar to OpenAI’s GPT tokenizers. This allows tokens to represent any arbitrary byte sequence — even those without a valid utf-8 encoding. This makes the end-to-end model much more flexible in its representation of language, and leads to improved performance.</p>
	<p>Along with the base Llama 3 models, Meta has released a suite of offerings with tools such as <a href="https://ai.meta.com/blog/meta-llama-3">Llama Guard 2, Code Shield, and CyberSec Eval 2,</a> which we are hoping to release on our Workers AI platform shortly.</p>
	<h2>Try it out now</h2>
	<p>Meta Llama 3 8B is available in the <a href="https://developers.cloudflare.com/workers-ai/models">Workers AI Model Catalog</a> today! Check out the <a href="https://developers.cloudflare.com/workers-ai/models/llama-3-8b-instruct">documentation here</a> and as always if you want to share your experiences or learn more, join us in the <a href="https://discord.cloudflare.com">Developer Discord</a>.</p>
</div>