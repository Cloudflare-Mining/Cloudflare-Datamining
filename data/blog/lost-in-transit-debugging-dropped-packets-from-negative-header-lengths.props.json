{
	"footerBlurb": "Cloudflare's connectivity cloud protects <a target='_blank' href='https://www.cloudflare.com/network-services/' rel='noreferrer'>entire corporate networks</a>, helps customers build <a target='_blank' href='https://workers.cloudflare.com/' rel='noreferrer'>Internet-scale applications efficiently</a>, accelerates any <a target='_blank' href='https://www.cloudflare.com/performance/accelerate-internet-applications/' rel='noreferrer'>website or Internet application</a>, <a target='_blank' href='https://www.cloudflare.com/ddos/' rel='noreferrer'>wards off DDoS attacks</a>, keeps <a target='_blank' href='https://www.cloudflare.com/application-security/' rel='noreferrer'>hackers at bay</a>, and can help you on <a target='_blank' href='https://www.cloudflare.com/products/zero-trust/' rel='noreferrer'>your journey to Zero Trust</a>.<br/><br/>Visit <a target='_blank' href='https://one.one.one.one/' rel='noreferrer'>1.1.1.1</a> from any device to get started with our free app that makes your Internet faster and safer.<br/><br/>To learn more about our mission to help build a better Internet, <a target='_blank' href='https://www.cloudflare.com/learning/what-is-cloudflare/' rel='noreferrer'>start here</a>. If you&apos;re looking for a new career direction, check out <a target='_blank' href='http://www.cloudflare.com/careers' rel='noreferrer'>our open positions</a>.",
	"initialReadingTime": "8",
	"locale": "en-us",
	"localesAvailable": [],
	"post": {
		"authors": [
			{
				"name": "Terin Stock",
				"slug": "terin-stock",
				"bio": null,
				"profile_image": "https://cf-assets.www.cloudflare.com/slt3lc6tev37/32boe5rGxrPP5kgQgNIN0K/15cbaa98dea11dc328cbea2d39139402/terin-stock.png",
				"location": null,
				"website": null,
				"twitter": "@terinjokes",
				"facebook": null
			}
		],
		"excerpt": "In this post, we'll provide some insight into the process of investigating networking issues and how to begin debugging issues in the kernel using pwru and kprobe tracepoints",
		"feature_image": "https://cf-assets.www.cloudflare.com/slt3lc6tev37/NSwPO77JLWkzE7F1ZwSNH/8df78ddc03863fb714e2e0bb70ac421d/lost-in-transit-debugging-dropped-packets-from-negative-header-lengths.png",
		"featured": false,
		"html": "\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/4tokKH49S2rHSl2HiAEeHl/9f822e144bfca9d2571a4ef2f0b7660f/image3-29.png\" alt=\"Lost in transit: debugging dropped packets from negative header lengths.\" class=\"kg-image\" width=\"1921\" height=\"1080\" loading=\"lazy\"/>\n            \n            </figure><p>Previously, I wrote about building <a href=\"/high-availability-load-balancers-with-maglev/\">network load balancers with the maglev scheduler</a>, which we use for ingress into our Kubernetes clusters. At the time of that post we were using Foo-over-UDP encapsulation with virtual interfaces, one for each Internet Protocol version for each worker node.</p><p>To reduce operational toil managing the traffic director nodes, we&#39;ve recently switched to using IP Virtual Server&#39;s (IPVS) native support for encapsulation. Much to our surprise, instead of a smooth change, we instead observed significant drops in bandwidth and failing API requests. In this post I&#39;ll discuss the impact observed, the multi-week search for the root cause, and the ultimate fix.</p>\n          <div class=\"flex anchor relative\">\n            <h3 id=\"recap-and-the-change\">Recap and the change</h3>\n            <a href=\"#recap-and-the-change\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n              <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n            </a>\n          </div>\n        <p>To support our requirements we&#39;ve been creating virtual interfaces on our traffic directors configured to encapsulate traffic with Foo-Over-UDP (FOU). In this encapsulation new UDP and IP headers are added to the original packet. When the worker node receives this packet, the kernel removes the outer headers and injects the inner packet back into the network stack. Each virtual interface would be assigned a private IP, which would be configured to send traffic to these private IPs in &quot;direct&quot; mode.</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/166RG9mgcpaP80xGDieN46/faa8855a23e4ccc115f96613ad4a358d/image4-24.png\" alt=\"\" class=\"kg-image\" width=\"590\" height=\"512\" loading=\"lazy\"/>\n            \n            </figure><p>This configuration presents several problems for our operations teams.</p><p>For example, each Kubernetes worker node needs a separate virtual interface on the traffic director, and each of the interfaces requires their own private IP. The pairs of virtual interfaces and private IPs were only used by this system, but still needed to be tracked in our configuration management system. To ensure the interfaces were created and configured properly on each director we had to run complex health checks, which added to the lag between provisioning a new worker node and the director being ready to send it traffic. Finally, the header for FOU also lacks a way to signal the &quot;next protocol&quot; of the inner packet, requiring a separate virtual interface for IPv4 and IPv6. Each of these problems individually contributed a small amount of toil, but taken together, gave us impetus to find a better alternative.</p><p>In the time since we had originally implemented this system, IPVS has added native support for encapsulation. This would allow us to eliminate provisioning virtual interfaces (and their corresponding private IPs), and be able to use newly provisioned workers without delay.</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/4CLprcnKtcCYzpCCnDkjl2/55c2f94d5c6a4edb01f1d3e288807e7b/image1-40.png\" alt=\"\" class=\"kg-image\" width=\"590\" height=\"512\" loading=\"lazy\"/>\n            \n            </figure><p>IPVS doesn&#39;t support Foo-over-UDP as an encapsulation type. As part of this project we switched to a similar option, <a href=\"https://datatracker.ietf.org/doc/html/draft-ietf-intarea-gue-09\">Generic UDP Encapsulation</a> (GUE). This encapsulation option does include the &quot;next protocol&quot;, allowing one listener on the worker nodes to support both IPv4 and IPv6.</p>\n          <div class=\"flex anchor relative\">\n            <h3 id=\"what-went-wrong\">What went wrong?</h3>\n            <a href=\"#what-went-wrong\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n              <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n            </a>\n          </div>\n        <p>When we make changes to our Kubernetes clusters, we go through several layers of testing. This change had been deployed to the traffic directors in front of our staging environments, where it had been running for several weeks. However, due to the nature of this bug, the type of traffic to our staging environment did not trigger the underlying bug.</p><p>We began a slow rollout of this change to one production cluster, and after a few hours we began observing issues reaching services behind Kubernetes Load Balancers. The behavior observed was very interesting: the vast majority of requests had no issues, but a small percentage of requests corresponding to large HTTP request payloads or gRPC had significant latency. However, large responses had no corresponding latency increase. There was no corresponding increase in latency seen to any requests to our ingress controllers, though we could observe a small drop in overall requests per second.</p>\n            <figure class=\"kg-card kg-image-card kg-width-wide\">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/LhkHSdh6ronJadcny3OGI/1ce6d785ab3783926dec79f62e5debcc/image6-9.png\" alt=\"\" class=\"kg-image\" width=\"1908\" height=\"296\" loading=\"lazy\"/>\n            \n            </figure><p>Through debugging after the incident, we discovered that the traffic directors were dropping packets that exceeded the Internet maximum transmission unit (MTU) of 1500 bytes, despite the packets being smaller than the actual MTU configured in our internal networks. Once dropped, the original host would fragment and resend packets until they were small enough to pass through the traffic directors. Dropping one packet is inconvenient, but unlikely to be noticed. However, when making a request with large payloads it is very likely that all packets would be dropped and need to be individually fragmented and resent.</p><p>When every packet is dropped and has to be Â resent, the network latency can add up to several seconds, exceeding the request timeouts configured by applications. This would cause the request to fail. necessitating retries by applications. As more traffic directors were reconfigured, these retries were less likely to succeed, leading to slower processing and causing the backlog of queued work to increase.</p><p>As you can see this small, but consistent, number of dropped packets could cause a domino effect into much larger problems. Once it became clear there was a problem, we reverted traffic directors to their previous configuration, and this quickly restored traffic to previous levels. From this we knew something about the change caused this problem.</p>\n          <div class=\"flex anchor relative\">\n            <h3 id=\"finding-the-culprit\">Finding the culprit</h3>\n            <a href=\"#finding-the-culprit\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n              <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n            </a>\n          </div>\n        <p>With the symptoms of the issues identified, we started to try to understand the root cause. Once the root cause was understood, we could come up with a satisfactory solution.</p><p>Knowing the packets were larger than the Internet MTU, our first thought was that this was a misconfiguration of the machine in our configuration management tool. However, we found the interface MTUs were all set as expected, and there were no overriding MTUs in the routing table. We also found that sending packets from the director that exceeded the Internet MTU worked fine with no drops.</p><p>Cilium has developed a debugging tool <a href=\"https://github.com/cilium/pwru\">pwru</a>, short for &quot;packet, where are you?&quot;, that uses eBPF to aid in debugging the kernel networking state. We used pwru in our staging environment and found the location where the packet had been dropped.</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">sudo pwru --output-skb --output-tuple --per-cpu-buffer 2097152 --output-file pwru.log</pre></code>\n            <p>This captures tracing data for all packets that reach the traffic director, and saves the trace data to &quot;pwru.log&quot;. There are filters built into <code>pwru</code> to select matching packets, but unfortunately they could not be used as the packet was being modified by the encapsulation. Instead, we used grep afterwards to find a matching packet, and then filtered farther based on the pointer in the first column.</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">0xffff947712f34400      9        [&lt;empty&gt;]               packet_rcv netns=4026531840 mark=0x0 ifindex=4 proto=8 mtu=1600 len=1512 172.70.2.6:49756-&gt;198.51.100.150:8000(tcp)\n0xffff947712f34400      9        [&lt;empty&gt;]                 skb_push netns=4026531840 mark=0x0 ifindex=4 proto=8 mtu=1600 len=1512 172.70.2.6:49756-&gt;198.51.100.150:8000(tcp)\n0xffff947712f34400      9        [&lt;empty&gt;]              consume_skb netns=4026531840 mark=0x0 ifindex=4 proto=8 mtu=1600 len=1512 172.70.2.6:49756-&gt;198.51.100.150:8000(tcp)\n[ ... snip ... ]\n0xffff947712f34400      9        [&lt;empty&gt;]         inet_gso_segment netns=4026531840 mark=0x0 ifindex=7 proto=8 mtu=1600 len=1544 172.70.4.34:33259-&gt;172.70.64.149:5501(udp)\n0xffff947712f34400      9        [&lt;empty&gt;]        udp4_ufo_fragment netns=4026531840 mark=0x0 ifindex=7 proto=8 mtu=1600 len=1524 172.70.4.34:33259-&gt;172.70.64.149:5501(udp)\n0xffff947712f34400      9        [&lt;empty&gt;]   skb_udp_tunnel_segment netns=4026531840 mark=0x0 ifindex=7 proto=8 mtu=1600 len=1524 172.70.4.34:33259-&gt;172.70.64.149:5501(udp)\n0xffff947712f34400      9        [&lt;empty&gt;] kfree_skb_reason(SKB_DROP_REASON_NOT_SPECIFIED) netns=4026531840 mark=0x0 ifindex=7 proto=8 mtu=1600 len=1558 172.70.4.34:33259-&gt;172.70.64.149:5501(udp)</pre></code>\n            <p>Looking at the lines logged for this packet, we can observe some behavior. We originally received a TCP packet for the load balancer IP. However, when the packet is dropped, it is a UDP packet destined for the worker&#39;s IP on the port we use for GUE. We can surmise that the packet was being processed and encapsulated by IPVS, and form a theory it was being dropped on the egress path from the director node. We could also see that when the packet was dropped, the packet was still smaller than the calculated MTU.</p><p>We can visualize this change by applying this information to our GUE encapsulation diagram shown earlier. The byte totals of the encapsulated packet is 1544, matching the length <code>pwru</code> logged entering <code>inet_gso_segment</code> above.</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/4oBm7taQSl2U4UH3reulvJ/bb92576e1ad92ea2d91e6b80926fbb3f/image2-38.png\" alt=\"\" class=\"kg-image\" width=\"631\" height=\"845\" loading=\"lazy\"/>\n            \n            </figure><p>The trace above tells us what kernel functions are entered, but does not tell us if or how the program flow left. This means we don&#39;t know in which function kfree_skb_reason was called. Fortunately <code>pwru</code> can print a stacktrace when functions are entered.</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">0xffff9868b7232e00 \t19       \t[pwru] kfree_skb_reason(SKB_DROP_REASON_NOT_SPECIFIED) netns=4026531840 mark=0x0 ifindex=7 proto=8 mtu=1600 len=1558 172.70.4.34:63336-&gt;172.70.72.206:5501(udp)\nkfree_skb_reason\nvalidate_xmit_skb\n__dev_queue_xmit\nip_finish_output2\nip_vs_tunnel_xmit   \t[ip_vs]\nip_vs_in_hook   [ip_vs]\nnf_hook_slow\nip_local_deliver\nip_sublist_rcv_finish\nip_sublist_rcv\nip_list_rcv\n__netif_receive_skb_list_core\nnetif_receive_skb_list_internal\nnapi_complete_done\nmlx5e_napi_poll [mlx5_core]\n__napi_poll\nnet_rx_action\n__softirqentry_text_start\n__irq_exit_rcu\ncommon_interrupt\nasm_common_interrupt\naudit_filter_syscall\n__audit_syscall_exit\nsyscall_exit_work\nsyscall_exit_to_user_mode\ndo_syscall_64\nentry_SYSCALL_64_after_hwframe</pre></code>\n            <p>This <code>stacktrace</code> shows that <code>kfree_skb_reason</code> was called from within the <code>validate_xmit_skb</code> function and this is called from <code>ip_vs_tunnel_xmit</code>. However, when looking at the <a href=\"https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/net/core/dev.c?h=v6.1.32&id=76ba310227d2490018c271f1ecabb6c0a3212eb0#n3660\">implementation of validate_xmit_skb</a>, we find there are three paths to <code>kfree_skb</code>. How can we determine which path is taken?</p><p>In addition to the eBPF support used by pwru, Linux has support for attaching dynamic tracepoints using <code>perf-probe</code>. After installing the kernel source code and debugging symbols, we can ask the <code>kprobe</code> mechanism what lines of <code>validate_xmit_skb</code> we can create a dynamic tracepoint. It prints the line numbers for the line we can attach a tracepoint onto.</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/6ddSe06sv8S2w4lAXfZGFi/5e200ead8487644f7aa4572fb9f6ef98/image5-16.png\" alt=\"\" class=\"kg-image\" width=\"870\" height=\"966\" loading=\"lazy\"/>\n            \n            </figure><p>Unfortunately, we can&#39;t create a tracepoint on the goto lines, but we can attach tracepoints around them, using the context to determine how control flowed through this function. In addition to specifying a line number, additional arguments can be specified to include local variables. The skb variable is a pointer to a structure that represents this packet, which can be logged to separate packets in case more than one is being processed at a time.</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">sudo perf probe --add &#039;validate_xmit_skb:17 skb&#039;\nsudo perf probe --add &#039;validate_xmit_skb:20 skb&#039;\nsudo perf probe --add &#039;validate_xmit_skb:24 skb&#039;\nsudo perf probe --add &#039;validate_xmit_skb:32 skb&#039;</pre></code>\n            <p>Access to these tracepoints could be recorded by using <code>perf-record</code> and providing the tracepoint names given when they were added.</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">sudo perf record -a -e &#039;probe:validate_xmit_skb_L17,probe:validate_xmit_skb_L20,probe:validate_xmit_skb_L24,probe:validate_xmit_skb_L32&#039;</pre></code>\n            <p>The tests can be rerun so some packets are logged, before using <code>perf-script</code> to read the generated &quot;perf.data&quot; file. When inspecting the output file, we found that all the packets that were dropped were coming from the control flow of <code>netif_needs_gso</code> succeeding (that is, from the goto on line 18). We continued to create and record tracepoints, following the failing control flow, until execution reached <a href=\"https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/net/ipv4/udp_offload.c?h=v6.1.32&id=76ba310227d2490018c271f1ecabb6c0a3212eb0#n15\"><code>__skb_udp_tunnel_segment</code></a>.</p><p>When <code>netif_needs_gso</code> returns false, we do not see packet drops and no problems are reported. It returns true when the packet is larger than the maximum segment size (MSS) advertised by our peer in the connection handshake. For IPv4, the MSS is usually 40 bytes smaller than the MTU (though this can be adjusted by the application or system configuration). For most packets the traffic directors see, the MSS is 40 bytes less than the Internet MTU of 1500, or in this case 1460.</p><p>The tracepoints in this function showed that control flow was leaving through the error case on line 33: that kernel was unable to allocate memory for the tunnel header. GUE was designed to have a minimal tunnel header, so this seemed suspicious. We updated the probe to also record the calculated <code>tnl_hlen</code>, and reran the tests. To our surprise the value recorded by the probes was &quot;-2&quot;. Huh, somehow the encapsulated packet got smaller?</p>\n            <pre class=\"language-c++\"><code class=\"language-c++\">static struct sk_buff *__skb_udp_tunnel_segment(struct sk_buff *skb,\n    netdev_features_t features,\n    struct sk_buff *(*gso_inner_segment)(struct sk_buff *skb,\n   \t\t\t\t  \tnetdev_features_t features),\n    __be16 new_protocol, bool is_ipv6)\n{\n    int tnl_hlen = skb_inner_mac_header(skb) - skb_transport_header(skb); // tunnel header length computed here\n    bool remcsum, need_csum, offload_csum, gso_partial;\n    struct sk_buff *segs = ERR_PTR(-EINVAL);\n    struct udphdr *uh = udp_hdr(skb);\n    u16 mac_offset = skb-&gt;mac_header;\n    __be16 protocol = skb-&gt;protocol;\n    u16 mac_len = skb-&gt;mac_len;\n    int udp_offset, outer_hlen;\n    __wsum partial;\n    bool need_ipsec;\n\n    if (unlikely(!pskb_may_pull(skb, tnl_hlen))) // allocation failed due to negative number.\n   \t goto out;</pre></code>\n            \n          <div class=\"flex anchor relative\">\n            <h3 id=\"ultimate-fix\">Ultimate fix</h3>\n            <a href=\"#ultimate-fix\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n              <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n            </a>\n          </div>\n        <p>At this point the kernel&#39;s behavior was a bit baffling: why would the tunnel header be computed to be a negative number? To answer this question, we added two more probes. The first was added to <code>ip_vs_in_hook</code>, a hook function that is called as packets enter and leave IPVS code. The second probe was added to <code>__dev_queue_xmit</code>, which is called when preparing to ask the hardware device to transmit the packet. To both of these probes we also logged some of the fields of the sk_buff struct by using the <code>&quot;pointer-&gt;field&quot;</code> syntax. These fields are offsets into the packet&#39;s data for the packet&#39;s headers, as well as corresponding offsets for the encapsulated headers.</p><ul><li><p>The <code>mac_header</code> and <code>inner_mac_header</code> are offsets to the packet&#39;s layer two header. For Ethernet this includes the MAC addresses for the frame, but also other metadata such as the EtherType field giving the type the next protocol.</p></li><li><p>The <code>network_header</code> and <code>inner_network_header</code> fields are offsets to the packet&#39;s layer three header. For our purposes, this would be the header for IPv4 or IPv6.</p></li><li><p>The <code>transport_header</code> and <code>inner_transport_header</code> fields are offset to the packet&#39;s layer four header, such as TCP, UDP, or ICMP.</p></li></ul>\n            <pre class=\"language-bash\"><code class=\"language-bash\">sudo perf probe -m ip_vs --add &#039;ip_vs_in_hook dev=skb-&gt;dev-&gt;name:string skb skb-&gt;inner_mac_header skb-&gt;inner_network_header skb-&gt;inner_transport_header skb-&gt;mac_header skb-&gt;network_header skb-&gt;transport_header skb-&gt;ipvs_property skb-&gt;len:u skb-&gt;data_len:u&#039;\nsudo perf probe --add &#039;__dev_queue_xmit dev=skb-&gt;dev-&gt;name:string skb skb-&gt;inner_mac_header skb-&gt;inner_network_header skb-&gt;inner_transport_header skb-&gt;mac_header skb-&gt;network_header skb-&gt;transport_header skb-&gt;len:u skb-&gt;data_len:u&#039;</pre></code>\n            <p>When we review the tracepoints using perf-script, we can notice something interesting with these values.</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">swapper     0 [030] 79090.376151:    probe:ip_vs_in_hook: (ffffffffc0feebe0) dev=&quot;vlan100&quot; skb=0xffff9ca661e90200 inner_mac_header=0x0 inner_network_header=0x0 inner_transport_header=0x0 mac_header=0x44 network_header=0x52 transport_header=0x66 len=1512 data_len=12\nswapper     0 [030] 79090.376153:    probe:ip_vs_in_hook: (ffffffffc0feebe0) dev=&quot;vlan100&quot; skb=0xffff9ca661e90200 inner_mac_header=0x44 inner_network_header=0x52 inner_transport_header=0x66 mac_header=0x44 network_header=0x32 transport_header=0x46 len=1544 data_len=12\nswapper     0 [030] 79090.376155: probe:__dev_queue_xmit: (ffffffff85070e60) dev=&quot;vlan100&quot; skb=0xffff9ca661e90200 inner_mac_header=0x44 inner_network_header=0x52 inner_transport_header=0x66 mac_header=0x44 network_header=0x32 transport_header=0x46 len=1558 data_len=12</pre></code>\n            <p>When the packet reaches <code>ip_vs_in_hook</code> on the way into IPVS, it only has outer packet headers. This makes sense, as the packet hasn&#39;t been encapsulated by IPVS yet. When the same hook is called again as the packet leaves IPVS, we see the encapsulation is already completed. We also find that the outer MAC header and the inner MAC header are at the same offset. Knowing how the tunnel header length is calculated in <code>__skb_udp_tunnel_segment</code>, we can also see where &quot;-2&quot; comes from. The <code>inner_mac_header</code> is at offset 0x44, while the <code>transport_header</code> is at offset 0x46.</p><p>When packets pass through network interfaces, the code for the interface reserves some space for the MAC header. For example, on an Ethernet interface some space is reserved for the Ethernet headers. FOU and GUE do not use link-layer addressing like Ethernet so no space is needed to be reserved. When we were using the virtual interfaces with FOU before it was correctly handling this case by setting the inner MAC header offset to the same as the inner network offset, effectively making it zero bytes long.</p><p>When using the encapsulation inside IPVS, this was not being accounted for, resulting in the inner MAC header offset being invalid. When packets did not need to be segmented this was a harmless bug. When segmenting, however, the tunnel header size needed to be known to duplicate it to all segmented packets.</p><p>I created a patch to correct the MAC header offset in IPVS&#39;s encapsulation code. The correction to the inner header offsets needed to be duplicated for IPv4 and IPv6.</p>\n            <pre class=\"language-diff\"><code class=\"language-diff\">diff --git a/net/netfilter/ipvs/ip_vs_xmit.c b/net/netfilter/ipvs/ip_vs_xmit.c\nindex c7652da78c88..9193e109e6b3 100644\n--- a/net/netfilter/ipvs/ip_vs_xmit.c\n+++ b/net/netfilter/ipvs/ip_vs_xmit.c\n@@ -1207,6 +1207,7 @@ ip_vs_tunnel_xmit(struct sk_buff *skb, struct ip_vs_conn *cp,\n \tskb-&gt;transport_header = skb-&gt;network_header;\n \n \tskb_set_inner_ipproto(skb, next_protocol);\n+\tskb_set_inner_mac_header(skb, skb_inner_network_offset(skb));\n \n \tif (tun_type == IP_VS_CONN_F_TUNNEL_TYPE_GUE) {\n \t\tbool check = false;\n@@ -1349,6 +1350,7 @@ ip_vs_tunnel_xmit_v6(struct sk_buff *skb, struct ip_vs_conn *cp,\n \tskb-&gt;transport_header = skb-&gt;network_header;\n \n \tskb_set_inner_ipproto(skb, next_protocol);\n+\tskb_set_inner_mac_header(skb, skb_inner_network_offset(skb));\n \n \tif (tun_type == IP_VS_CONN_F_TUNNEL_TYPE_GUE) {\n \t\tbool check = false;</pre></code>\n            <p>When the patch was included in our kernel and deployed the difference in end-to-end request latency was immediately noticeable. We also no longer observed packet drops for requests with large payloads.</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/slt3lc6tev37/4SVrX3EiwZFhiRRkN1BMar/ebde4d7e548dadf33369d92c94cf1efc/image7-8.png\" alt=\"\" class=\"kg-image\" width=\"775\" height=\"450\" loading=\"lazy\"/>\n            \n            </figure><p>I&#39;ve <a href=\"https://lore.kernel.org/all/20230609205842.2333727-1-terin@cloudflare.com/T/#u\">submitted the Linux kernel patch upstream</a>, which is currently queued for inclusion in future releases of the kernel.</p>\n          <div class=\"flex anchor relative\">\n            <h3 id=\"conclusion\">Conclusion</h3>\n            <a href=\"#conclusion\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n              <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n            </a>\n          </div>\n        <p>Through this post, we hoped to have provided some insight into the process of investigating networking issues and how to begin debugging issues in the kernel using <code>pwru</code> and <code>kprobe</code> tracepoints. This investigation also led to a Linux kernel patch upstream. It also allowed us to safely roll out IPVS&#39;s native encapsulation.</p>",
		"id": "4XFiBPDirEDvUVaOc0IHH8",
		"localeList": {
			"name": "Lost in transit: debugging dropped packets from negative header lengths Config",
			"enUS": "English for Locale",
			"zhCN": "No Page for Locale",
			"zhHansCN": "No Page for Locale",
			"zhTW": "No Page for Locale",
			"frFR": "No Page for Locale",
			"deDE": "No Page for Locale",
			"itIT": "No Page for Locale",
			"jaJP": "No Page for Locale",
			"koKR": "No Page for Locale",
			"ptBR": "No Page for Locale",
			"esLA": "No Page for Locale",
			"esES": "No Page for Locale",
			"enAU": "No Page for Locale",
			"enCA": "No Page for Locale",
			"enIN": "No Page for Locale",
			"enGB": "No Page for Locale",
			"idID": "No Page for Locale",
			"ruRU": "No Page for Locale",
			"svSE": "No Page for Locale",
			"viVN": "No Page for Locale",
			"plPL": "No Page for Locale",
			"arAR": "No Page for Locale",
			"nlNL": "No Page for Locale",
			"thTH": "No Page for Locale",
			"trTR": "No Page for Locale",
			"heIL": "No Page for Locale",
			"lvLV": "No Page for Locale",
			"etEE": "No Page for Locale",
			"ltLT": "No Page for Locale"
		},
		"meta_description": "In this post, we'll provide some insight into the process of investigating networking issues and how to begin debugging issues in the kernel using pwru and kprobe tracepoints.",
		"metadata": {
			"title": "Lost in transit: debugging dropped packets from negative header lengths",
			"description": "In this post, we'll provide some insight into the process of investigating networking issues and how to begin debugging issues in the kernel using pwru and kprobe tracepoints.",
			"imgPreview": "https://cf-assets.www.cloudflare.com/slt3lc6tev37/6aly1sY1mNREFCGTQCDLmD/555aff21d703e52a6e2ea42d5160b713/lost-in-transit-debugging-dropped-packets-from-negative-header-lengths-X7UqyQ.png"
		},
		"primary_author": {},
		"published_at": "2023-06-26T14:00:56.000+01:00",
		"slug": "lost-in-transit-debugging-dropped-packets-from-negative-header-lengths",
		"tags": [
			{
				"id": "1AngXfNV5gW3bR8pmkPLKi",
				"name": "UDP",
				"slug": "udp"
			},
			{
				"id": "2UVIYusJwlvsmPYl2AvSuR",
				"name": "Deep Dive",
				"slug": "deep-dive"
			}
		],
		"title": "Lost in transit: debugging dropped packets from negative header lengths",
		"updated_at": "2024-08-27T01:08:33.211Z",
		"url": "https://blog.cloudflare.com/lost-in-transit-debugging-dropped-packets-from-negative-header-lengths"
	},
	"translations": {
		"posts.by": "By",
		"footer.gdpr": "GDPR",
		"lang_blurb1": "This post is also available in {lang1}.",
		"lang_blurb2": "This post is also available in {lang1} and {lang2}.",
		"lang_blurb3": "This post is also available in {lang1}, {lang2} and {lang3}.",
		"footer.press": "Press",
		"header.title": "The Cloudflare Blog",
		"search.clear": "Clear",
		"search.filter": "Filter",
		"search.source": "Source",
		"footer.careers": "Careers",
		"footer.company": "Company",
		"footer.support": "Support",
		"footer.the_net": "theNet",
		"search.filters": "Filters",
		"footer.our_team": "Our team",
		"footer.webinars": "Webinars",
		"page.more_posts": "More posts",
		"posts.time_read": "{time} min read",
		"search.language": "Language",
		"footer.community": "Community",
		"footer.resources": "Resources",
		"footer.solutions": "Solutions",
		"footer.trademark": "Trademark",
		"header.subscribe": "Subscribe",
		"footer.compliance": "Compliance",
		"footer.free_plans": "Free plans",
		"footer.impact_ESG": "Impact/ESG",
		"posts.follow_on_X": "Follow on X",
		"footer.help_center": "Help center",
		"footer.network_map": "Network Map",
		"header.please_wait": "Please Wait",
		"page.related_posts": "Related posts",
		"search.result_stat": "Results <strong>{search_range}</strong> of <strong>{search_total}</strong> for <strong>{search_keyword}</strong>",
		"footer.case_studies": "Case Studies",
		"footer.connect_2024": "Connect 2024",
		"footer.terms_of_use": "Terms of Use",
		"footer.white_papers": "White Papers",
		"footer.cloudflare_tv": "Cloudflare TV",
		"footer.community_hub": "Community Hub",
		"footer.compare_plans": "Compare plans",
		"footer.contact_sales": "Contact Sales",
		"header.contact_sales": "Contact Sales",
		"header.email_address": "Email Address",
		"page.error.not_found": "Page not found",
		"footer.developer_docs": "Developer docs",
		"footer.privacy_policy": "Privacy Policy",
		"footer.request_a_demo": "Request a demo",
		"page.continue_reading": "Continue reading",
		"footer.analysts_report": "Analyst reports",
		"footer.for_enterprises": "For enterprises",
		"footer.getting_started": "Getting Started",
		"footer.learning_center": "Learning Center",
		"footer.project_galileo": "Project Galileo",
		"pagination.newer_posts": "Newer Posts",
		"pagination.older_posts": "Older Posts",
		"posts.social_buttons.x": "Discuss on X",
		"search.source_location": "Source/Location",
		"footer.about_cloudflare": "About Cloudflare",
		"footer.athenian_project": "Athenian Project",
		"footer.become_a_partner": "Become a partner",
		"footer.cloudflare_radar": "Cloudflare Radar",
		"footer.network_services": "Network services",
		"footer.trust_and_safety": "Trust & Safety",
		"header.get_started_free": "Get Started Free",
		"page.search.placeholder": "Search Cloudflare",
		"footer.cloudflare_status": "Cloudflare Status",
		"footer.cookie_preference": "Cookie Preferences",
		"header.valid_email_error": "Must be valid email.",
		"footer.connectivity_cloud": "Connectivity cloud",
		"footer.developer_services": "Developer services",
		"footer.investor_relations": "Investor relations",
		"page.not_found.error_code": "Error Code: 404",
		"footer.logos_and_press_kit": "Logos & press kit",
		"footer.application_services": "Application services",
		"footer.get_a_recommendation": "Get a recommendation",
		"posts.social_buttons.reddit": "Discuss on Reddit",
		"footer.sse_and_sase_services": "SSE and SASE services",
		"page.not_found.outdated_link": "You may have used an outdated link, or you may have typed the address incorrectly.",
		"footer.report_security_issues": "Report Security Issues",
		"page.error.error_message_page": "Sorry, we can't find the page you are looking for.",
		"header.subscribe_notifications": "Subscribe to receive notifications of new posts:",
		"footer.cloudflare_for_campaigns": "Cloudflare for Campaigns",
		"header.subscription_confimation": "Subscription confirmed. Thank you for subscribing!",
		"posts.social_buttons.hackernews": "Discuss on Hacker News",
		"footer.diversity_equity_inclusion": "Diversity, equity & inclusion",
		"footer.critical_infrastructure_defense_project": "Critical Infrastructure Defense Project"
	}
}