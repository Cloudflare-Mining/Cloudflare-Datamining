{
	"footerBlurb": "Cloudflare's connectivity cloud protects <a target='_blank' href='https://www.cloudflare.com/network-services/' rel='noreferrer'>entire corporate networks</a>, helps customers build <a target='_blank' href='https://workers.cloudflare.com/' rel='noreferrer'>Internet-scale applications efficiently</a>, accelerates any <a target='_blank' href='https://www.cloudflare.com/performance/accelerate-internet-applications/' rel='noreferrer'>website or Internet application</a>, <a target='_blank' href='https://www.cloudflare.com/ddos/' rel='noreferrer'>wards off DDoS attacks</a>, keeps <a target='_blank' href='https://www.cloudflare.com/application-security/' rel='noreferrer'>hackers at bay</a>, and can help you on <a target='_blank' href='https://www.cloudflare.com/products/zero-trust/' rel='noreferrer'>your journey to Zero Trust</a>.<br/><br/>Visit <a target='_blank' href='https://one.one.one.one/' rel='noreferrer'>1.1.1.1</a> from any device to get started with our free app that makes your Internet faster and safer.<br/><br/>To learn more about our mission to help build a better Internet, <a target='_blank' href='https://www.cloudflare.com/learning/what-is-cloudflare/' rel='noreferrer'>start here</a>. If you&apos;re looking for a new career direction, check out <a target='_blank' href='http://www.cloudflare.com/careers' rel='noreferrer'>our open positions</a>.",
	"initialReadingTime": "5",
	"locale": "en-us",
	"localesAvailable": [],
	"post": {
		"authors": [
			{
				"name": "Taylor Smith",
				"slug": "tsmith",
				"bio": null,
				"profile_image": "https://cf-assets.www.cloudflare.com/zkvhlag99gkb/6FeCG1X2NGusCwWiMYDUW/e7e9bb9563dff808944171181243753b/tsmith.jpg",
				"location": "Austin, TX",
				"website": null,
				"twitter": null,
				"facebook": null,
				"publiclyIndex": true
			}
		],
		"excerpt": "How I used Workers AI to translate Cloudflare Stream’s auto-generated captions and what I learned along the way.",
		"feature_image": "https://cf-assets.www.cloudflare.com/zkvhlag99gkb/5g9A2jHh3yQhXLKBXvPOfq/75ab157a09dbfb9d123738332bd7888d/image1.png",
		"featured": false,
		"html": "<div style=\"position: relative; padding-top: 56.25%;\">\n  <iframe\n    src=\"https://customer-eq7kiuol0tk9chox.cloudflarestream.com/13297d6aa7c112b771c8d25d16fd3155/iframe?preload=true&defaultTextTrack=es&poster=https%3A%2F%2Fcustomer-eq7kiuol0tk9chox.cloudflarestream.com%2F13297d6aa7c112b771c8d25d16fd3155%2Fthumbnails%2Fthumbnail.jpg%3Ftime%3D%26height%3D600\"\n    loading=\"lazy\"\n    style=\"border: none; position: absolute; top: 0; left: 0; height: 100%; width: 100%;\"\n    allow=\"accelerometer; gyroscope; autoplay; encrypted-media; picture-in-picture;\"\n    allowfullscreen=\"true\"\n  ></iframe>\n</div>\n<p></p><p><a href=\"https://www.cloudflare.com/products/cloudflare-stream\"><u>Cloudflare Stream</u></a> launched AI-powered <a href=\"https://blog.cloudflare.com/stream-automatic-captions-with-ai\"><u>automated captions</u></a> to transcribe English in on-demand videos in March 2024. Customers&#39; immediate next questions were about other languages — both <i>transcribing</i> audio from other languages, and <i>translating</i> captions to make subtitles for other languages. As the Stream Product Manager, I&#39;ve thought a lot about how we might tackle these, but I wondered…</p><p><b>What if I just translated a generated </b><a href=\"https://en.wikipedia.org/wiki/WebVTT\"><b><u>VTT</u></b></a><b> (caption file)? Can we do that?</b> I hoped to use <a href=\"https://www.cloudflare.com/developer-platform/products/workers-ai/\"><u>Workers AI</u></a> to conduct a quick experiment to learn more about the problem space, challenges we may find, and what platform capabilities we can leverage.</p><p>There is a <a href=\"https://github.com/elizabethsiegle/cfworkers-ai-translate\"><u>sample translator demo</u></a> in Workers documentation that uses the “<a href=\"https://developers.cloudflare.com/workers-ai/models/m2m100-1.2b/\"><u>m2m100-1.2b</u></a>” Many-to-Many multilingual translation model to translate short input strings. I decided to start there and try using it to translate some of the English captions in my Stream library into Spanish.</p>\n    <div class=\"flex anchor relative\">\n      <h2 id=\"selecting-test-content\">Selecting test content</h2>\n      <a href=\"#selecting-test-content\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>I started with my <a href=\"https://customer-eq7kiuol0tk9chox.cloudflarestream.com/13297d6aa7c112b771c8d25d16fd3155/iframe?defaultTextTrack=en\"><u>short demo video announcing</u></a> the transcription feature. I wanted a Worker that could read the VTT captions file from Stream, isolate the text content, and run it through the model as-is.</p><p>The first step was parsing the input. A VTT file is a text file that contains a sequence of numbered “cues,” each with a number, a start and end time, and text content. </p>\n            <pre class=\"language-Rust\"><code class=\"language-Rust\">WEBVTT\nX-TIMESTAMP-MAP=LOCAL:00:00:00.000,MPEGTS:900000\n \n1\n00:00:00.000 --&gt; 00:00:02.580\nGood morning, I&#039;m Taylor Smith,\n \n2\n00:00:02.580 --&gt; 00:00:03.520\nthe Product Manager for Cloudflare\n \n3\n00:00:03.520 --&gt; 00:00:04.460\nStream. This is a quick\n \n4\n00:00:04.460 --&gt; 00:00:06.040\ndemo of our AI-powered automatic\n \n5\n00:00:06.040 --&gt; 00:00:07.580\nsubtitles feature. These subtitles\n \n6\n00:00:07.580 --&gt; 00:00:09.420\nwere generated with Cloudflare WorkersAI\n \n7\n00:00:09.420 --&gt; 00:00:10.860\nand the Whisper Model,\n \n8\n00:00:10.860 --&gt; 00:00:12.020\nnot handwritten, and it took\n \n9\n00:00:12.020 --&gt; 00:00:13.940\njust a few seconds.</pre></code>\n            \n    <div class=\"flex anchor relative\">\n      <h2 id=\"parsing-the-input\">Parsing the input</h2>\n      <a href=\"#parsing-the-input\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>I started with a simple Worker that would fetch the VTT from Stream directly, run it through a <a href=\"https://github.com/tsmith512/vtt-translate/blob/trunk/src/index.ts#L54\"><u>function I wrote to deconstruct the cues</u></a>, and return the timestamps and original text in an easier to review format.</p>\n            <pre class=\"language-JavaScript\"><code class=\"language-JavaScript\">export default {\n  async fetch(request: Request, env: Env, ctx): Promise&lt;Response&gt; {\n    // Step One: Get our input.\n    const input = await fetch(PLACEHOLDER_VTT_URL)\n      .then(res =&gt; res.text());\n \n    // Step Two: Parse the VTT file and get the text\n    const captions = vttToCues(input);\n \n    // Done: Return what we have.\n    return new Response(captions.map(c =&gt;\n      (`#${c.number}: ${c.start} --&gt; ${c.end}: ${c.content.toString()}`)\n    ).join(&#039;\\n&#039;));\n  },\n};</pre></code>\n            <p>That returned this text:</p>\n            <pre class=\"language-Rust\"><code class=\"language-Rust\">#1: 0 --&gt; 2.58: Good morning, I&#039;m Taylor Smith,\n#2: 2.58 --&gt; 3.52: the Product Manager for Cloudflare\n#3: 3.52 --&gt; 4.46: Stream. This is a quick\n#4: 4.46 --&gt; 6.04: demo of our AI-powered automatic\n#5: 6.04 --&gt; 7.58: subtitles feature. These subtitles\n#6: 7.58 --&gt; 9.42: were generated with Cloudflare WorkersAI\n#7: 9.42 --&gt; 10.86: and the Whisper Model,\n#8: 10.86 --&gt; 12.02: not handwritten, and it took\n#9: 12.02 --&gt; 13.94: just a few seconds.</pre></code>\n            \n    <div class=\"flex anchor relative\">\n      <h2 id=\"ai-ify\">AI-ify</h2>\n      <a href=\"#ai-ify\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>As a proof of concept, I adapted a snippet from the demo into my Worker. In the example, the target language and input text are extracted from the user’s request. In my experiment, I decided to hardcode the languages. Also, I had an array of input objects, one for each cue, not just a string. After interpreting the caption input <i>but before returning a response</i>, I used a map callback to parallelize all the AI.run() calls to translate each cue, so they could execute asynchronously and in-place, then awaited them all to resolve. Ultimately, the AI inference call itself is the simplest part of the script.</p>\n            <pre class=\"language-JavaScript\"><code class=\"language-JavaScript\">await Promise.all(captions.map(async (q) =&gt; {\n  const translation = await env.AI.run(\n    &quot;@cf/meta/m2m100-1.2b&quot;,\n    {\n      text: q.content,\n      source_lang: &quot;en&quot;,\n      target_lang: &quot;es&quot;,\n    }\n  );\n \n  q.content = translation?.translated_text ?? q.content;\n}));</pre></code>\n            <p>Then the script returns the translated output in the format from before.</p><p>Of course, this is not a scalable or error-tolerant approach for production use because it doesn’t make affordances for rate limiting, failures, or processing bigger throughput. But for a few minutes of tinkering, it taught me a lot.</p>\n            <pre class=\"language-Rust\"><code class=\"language-Rust\">#1: 0 --&gt; 2.58: Buen día, soy Taylor Smith.\n#2: 2.58 --&gt; 3.52: El gerente de producto de Cloudflare\n#3: 3.52 --&gt; 4.46: Rápido, esto es rápido\n#4: 4.46 --&gt; 6.04: La demostración de nuestro automático AI-powered\n#5: 6.04 --&gt; 7.58: Los subtítulos, estos subtítulos\n#6: 7.58 --&gt; 9.42: Generado con Cloudflare WorkersAI\n#7: 9.42 --&gt; 10.86: y el modelo de susurro,\n#8: 10.86 --&gt; 12.02: No se escribió, y se tomó\n#9: 12.02 --&gt; 13.94: Sólo unos segundos.</pre></code>\n            <p>A few immediate observations: first, these results came back surprisingly quickly and the Workers AI code worked on the first try! Second, evaluating the quality of translation results is going to depend on having team members with expertise in those languages. Because — third, as a novice Spanish speaker, I can tell this output has some issues.</p><p>Cues 1 and 2 are okay, but 3 is not (“Fast, this is fast” from “[Cloudflare] Stream. This is a quick…”). Cues 5 through 9 had several idiomatic and grammatical issues, too. I theorized that this is because Stream splits the English captions into groups of 4 or 5 words to make them easy to <i>read</i> quickly in the overlay. But that also means sentences and grammatical constructs are interrupted. When those fragments go to the translation model, there isn’t enough context.</p>\n    <div class=\"flex anchor relative\">\n      <h2 id=\"consolidating-sentences\">Consolidating sentences</h2>\n      <a href=\"#consolidating-sentences\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>I speculated that reconstructing sentences would be the most effective way to improve translation quality, so I made that the one problem I attempted to solve within this exploration. I added a rough <a href=\"https://github.com/tsmith512/vtt-translate/blob/trunk/src/index.ts#L132C7-L218\"><u>pre-processor</u></a> in the Worker that tries to merge caption cues together and then splits them at sentence boundaries instead. In the process, it also adjusts the timing of the resulting cues to cover the same approximate timeframe.</p><p>Looking at each cue in order:</p>\n            <pre class=\"language-JavaScript\"><code class=\"language-JavaScript\">// Break this cue up by sentence-ending punctuation.\nconst sentences = thisCue.content.split(/(?&lt;=[.?!]+)/g);\n\n// Cut here? We have one fragment and it has a sentence terminator.\nconst cut = sentences.length === 1 &amp;&amp; thisCue.content.match(/[.?!]/);</pre></code>\n            <p>But if there’s a cue that splits into multiple sentences, cut it up and split the timing. Leave the final fragment to roll into the next cue:</p>\n            <pre class=\"language-JavaScript\"><code class=\"language-JavaScript\">else if (sentences.length &gt; 1) {\n  // Save the last fragment for later\n  const nextContent = sentences.pop();\n\n  // Put holdover content and all-but-last fragment into the content\n  newContent += &#039; &#039; + sentences.join(&#039; &#039;);\n\n  const thisLength = (thisCue.end - thisCue.start) / 2;\n\n    result.push({\n      number: newNumber,\n      start: newStart,\n      end: thisCue.start + (thisLength / 2), // End this cue early\n      content: newContent,\n    });\n\n    // … then treat the next cue as a holdover\n    cueLength = 1;\n    newContent = nextContent;\n    // Start the next consolidated cue halfway into this cue&#039;s original duration\n    newStart = thisCue.start + (thisLength / 2) + 0.001;\n    // Set the next consolidated cue&#039;s number to this cue&#039;s number\n    newNumber = thisCue.number;\n  }\n}</pre></code>\n            <p>Applying that to the input, it generates sentence-grouped output, visualized here in green:</p>\n          <figure class=\"kg-card kg-image-card\">\n          <Image src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/1MzmQ0KAJBntBrqgwGAqTd/035d044fc9e70c9933c1406074de52b9/image2.png\" alt=\"\" class=\"kg-image\" width=\"1560\" height=\"903\" loading=\"lazy\"/>\n          </figure><p>There are only 3 “new” cues, each starts at the beginning of a sentence. The consolidated cues are longer and might be harder to read when overlaid on a video, but they are complete grammatical units:</p>\n            <pre class=\"language-Rust\"><code class=\"language-Rust\">#1: 0 --&gt; 3.755:  Good morning, I&#039;m Taylor Smith, the Product Manager for Cloudflare Stream.\n#3: 3.756 --&gt; 6.425:  This is a quick demo of our AI-powered automatic subtitles feature.\n#5: 6.426 --&gt; 12.5:  These subtitles were generated with Cloudflare Workers AI and the Whisper Model, not handwritten, and it took just a few seconds.</pre></code>\n            <p>Translating this “prepared” input the same way as before:</p>\n            <pre class=\"language-Rust\"><code class=\"language-Rust\">#1: 0 --&gt; 3.755: Buen día, soy Taylor Smith, el gerente de producto de Cloudflare Stream.\n#3: 3.756 --&gt; 6.425: Esta es una demostración rápida de nuestra función de subtítulos automáticos alimentados por IA.\n#5: 6.426 --&gt; 12.5: Estos subtítulos fueron generados con Cloudflare WorkersAI y el Modelo Whisper, no escritos a mano, y solo tomó unos segundos.</pre></code>\n            <p>¡Mucho mejor! [Much better!]</p>\n    <div class=\"flex anchor relative\">\n      <h2 id=\"re-exporting-to-vtt\">Re-exporting to VTT</h2>\n      <a href=\"#re-exporting-to-vtt\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>To use these translated captions on a video, they need to be <a href=\"https://github.com/tsmith512/vtt-translate/blob/trunk/src/index.ts#L228-L238\"><u>formatted back into a VTT</u></a> with renumbered cues and properly formatted timestamps. Ultimately, the solution should <a href=\"https://developers.cloudflare.com/stream/edit-videos/adding-captions/#upload-a-file\"><u>automatically upload them back to Stream</u></a>, too, but that is an established process, so I set it aside as out of scope. The final VTT result from my Worker is this:</p>\n            <pre class=\"language-Rust\"><code class=\"language-Rust\">WEBVTT\n \n1\n00:00:00.000 --&gt; 00:00:03.754\nBuen día, soy Taylor Smith, el gerente de producto de Cloudflare Stream.\n \n2\n00:00:03.755 --&gt; 00:00:06.424\nEsta es una demostración rápida de nuestra función de subtítulos automáticos alimentados por IA.\n \n3\n00:00:06.426 --&gt; 00:00:12.500\nEstos subtítulos fueron generados con Cloudflare WorkersAI y el Modelo Whisper, no escritos a mano, y solo tomó unos segundos.</pre></code>\n            <p>I saved it to a file locally and, using the Cloudflare Dashboard, I added it to the video which you may have noticed embedded at the top of this post! Captions can also be <a href=\"https://developers.cloudflare.com/stream/edit-videos/adding-captions/#upload-a-file\"><u>uploaded via the API</u></a>.</p>\n    <div class=\"flex anchor relative\">\n      <h2 id=\"more-testing-and-what-i-learned\">More testing and what I learned</h2>\n      <a href=\"#more-testing-and-what-i-learned\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>I tested this script on a variety of videos from many sources, including short social media clips, 30-minute video diaries, and even a few clips with some specialized vocabulary. Ultimately, I was surprised at the level of prototype I was able to build on my first afternoon with Workers AI. The translation results were very promising! In the process, I learned a few key things that I will be bringing back to product planning for Stream:</p><p><b>We have the tools.</b> Workers AI has a model called &quot;<a href=\"https://developers.cloudflare.com/workers-ai/models/m2m100-1.2b/\"><u>m2m100-1.2b</u></a>&quot; from Hugging Face that can do text translations between many languages. We can use it to translate the plain text cues from VTT files — whether we generate them or they are user-supplied. We’ll keep an eye out for new models as they are added, too.</p><p><b>Quality is prone to &quot;copy-of-a-copy&quot; effect.</b> When auto-translating captions that were auto-transcribed, issues that impact the English transcription have a huge downstream impact on the translation. Editing the source transcription improves quality <i>a lot</i>.</p><p><b>Good grammar and punctuation counts.</b> Translations are significantly improved if the source content is grammatically correct and punctuated properly. Punctuation is often missing when captions are auto-generated, but not always  — I would like to learn more about how to predict that and if there are ways we can increase punctuation in the output of transcription jobs. My cue consolidator experiment returns giant walls of text if there’s no punctuation on the input.</p><p><b>Translate full sentences when possible.</b> We split our transcriptions into cues of about 5 words for several reasons. However, this produces lower quality output when translated because it breaks grammatical constructs. Translation results are better with full sentences or at least complete fragments. This is doable, but easier said than done, particularly as we look toward support for additional input languages that use punctuation differently.</p><p><b>We will have blind spots when evaluating quality.</b> Everyone on our team was able to adequately evaluate English <i>transcriptions</i>. Sanity-checking the quality of <i>translations</i> will require team members who are familiar with those languages. We state disclaimers about transcription quality and offer tips to improve it, but at least we know what we&#39;re looking at. For translations, we may not know how far off we are in many cases. How many readers of this article objected to the first translation sample above?</p><p><b>Clear UI and API design will be important for these related but distinct workflows.</b> There are two different flows being requested by Stream customers: &quot;My audio is in English, please make translated subtitles&quot; alongside &quot;My audio is in another language, please transcribe captions as-is.&quot; We will need to carefully consider how we shape user-facing interactions to make it really clear to a user what they are asking us to do.</p><p><b>Workers AI is really easy to use.</b> Sheepishly, I will admit: although I read Stream&#39;s code for the transcription feature, this was the first time I&#39;ve ever used Workers AI on my own, and it was definitely the easiest part of this experiment!</p><p>Finally, as a product manager, it is important I remain focused on the outcome. From a certain point of view, this experiment is a bit of an <a href=\"https://en.wikipedia.org/wiki/XY_problem\"><u>XY Problem</u></a>. The <i>need</i> is &quot;I have audio in one language and I want subtitles in another.&quot; Are there other avenues worth looking into besides &quot;transcribe to captions, then restructure and translate those captions?&quot; Quite possibly. But this experiment with Workers AI helped me identify some potential challenges to plan for and opportunities to get excited about!</p><p>I’ve cleaned up and shared the sample code I used in this experiment at <a href=\"https://github.com/tsmith512/vtt-translate/\"><u>https://github.com/tsmith512/vtt-translate/</u></a>. Try it out and share your experience!</p>",
		"id": "6OAfYNDjjJBccE1gFIVrnu",
		"localeList": {
			"name": "blog-english-only",
			"enUS": "English for Locale",
			"zhCN": "No Page for Locale",
			"zhHansCN": "No Page for Locale",
			"zhTW": "No Page for Locale",
			"frFR": "No Page for Locale",
			"deDE": "No Page for Locale",
			"itIT": "No Page for Locale",
			"jaJP": "No Page for Locale",
			"koKR": "No Page for Locale",
			"ptBR": "No Page for Locale",
			"esLA": "No Page for Locale",
			"esES": "No Page for Locale",
			"enAU": "No Page for Locale",
			"enCA": "No Page for Locale",
			"enIN": "No Page for Locale",
			"enGB": "No Page for Locale",
			"idID": "No Page for Locale",
			"ruRU": "No Page for Locale",
			"svSE": "No Page for Locale",
			"viVN": "No Page for Locale",
			"plPL": "No Page for Locale",
			"arAR": "No Page for Locale",
			"nlNL": "No Page for Locale",
			"thTH": "No Page for Locale",
			"trTR": "No Page for Locale",
			"heIL": "No Page for Locale",
			"lvLV": "No Page for Locale",
			"etEE": "No Page for Locale",
			"ltLT": "No Page for Locale"
		},
		"meta_description": "How I used Workers AI to translate Cloudflare Stream’s auto-generated captions and what I learned along the way.",
		"metadata": {
			"title": "Un experimento rápido: translating Cloudflare Stream captions with Workers AI",
			"description": "How I used Workers AI to translate Cloudflare Stream’s auto-generated captions and what I learned along the way.",
			"imgPreview": "https://cf-assets.www.cloudflare.com/zkvhlag99gkb/1jxmEMtgmRjqJZ3yvlLlas/af6f9f70fa96b9594e09d412f79be3e0/Un_experimento_ra_pido-_translating_Cloudflare_Stream_captions_with_Workers_AI-OG.png"
		},
		"primary_author": {},
		"publicly_index": true,
		"published_at": "2024-12-24T14:00+00:00",
		"slug": "un-experimento-rapido-translating-cloudflare-stream-captions-with-workers-ai",
		"tags": [
			{
				"id": "7mVMfcxp4tMqKil9jj8BJa",
				"name": "Cloudflare Stream",
				"slug": "cloudflare-stream"
			},
			{
				"id": "6hbkItfupogJP3aRDAq6v8",
				"name": "Cloudflare Workers",
				"slug": "workers"
			},
			{
				"id": "1Wf1Dpb2AFicG44jpRT29y",
				"name": "Workers AI",
				"slug": "workers-ai"
			}
		],
		"title": "Un experimento rápido: translating Cloudflare Stream captions with Workers AI",
		"updated_at": "2024-12-24T14:00:02.945Z",
		"url": "https://blog.cloudflare.com/un-experimento-rapido-translating-cloudflare-stream-captions-with-workers-ai"
	},
	"translations": {
		"posts.by": "By",
		"footer.gdpr": "GDPR",
		"lang_blurb1": "This post is also available in {lang1}.",
		"lang_blurb2": "This post is also available in {lang1} and {lang2}.",
		"lang_blurb3": "This post is also available in {lang1}, {lang2} and {lang3}.",
		"footer.press": "Press",
		"header.title": "The Cloudflare Blog",
		"search.clear": "Clear",
		"search.filter": "Filter",
		"search.source": "Source",
		"footer.careers": "Careers",
		"footer.company": "Company",
		"footer.support": "Support",
		"footer.the_net": "theNet",
		"search.filters": "Filters",
		"footer.our_team": "Our team",
		"footer.webinars": "Webinars",
		"page.more_posts": "More posts",
		"posts.time_read": "{time} min read",
		"search.language": "Language",
		"footer.community": "Community",
		"footer.resources": "Resources",
		"footer.solutions": "Solutions",
		"footer.trademark": "Trademark",
		"header.subscribe": "Subscribe",
		"footer.compliance": "Compliance",
		"footer.free_plans": "Free plans",
		"footer.impact_ESG": "Impact/ESG",
		"posts.follow_on_X": "Follow on X",
		"footer.help_center": "Help center",
		"footer.network_map": "Network Map",
		"header.please_wait": "Please Wait",
		"page.related_posts": "Related posts",
		"search.result_stat": "Results <strong>{search_range}</strong> of <strong>{search_total}</strong> for <strong>{search_keyword}</strong>",
		"footer.case_studies": "Case Studies",
		"footer.connect_2024": "Connect 2024",
		"footer.terms_of_use": "Terms of Use",
		"footer.white_papers": "White Papers",
		"footer.cloudflare_tv": "Cloudflare TV",
		"footer.community_hub": "Community Hub",
		"footer.compare_plans": "Compare plans",
		"footer.contact_sales": "Contact Sales",
		"header.contact_sales": "Contact Sales",
		"header.email_address": "Email Address",
		"page.error.not_found": "Page not found",
		"footer.developer_docs": "Developer docs",
		"footer.privacy_policy": "Privacy Policy",
		"footer.request_a_demo": "Request a demo",
		"page.continue_reading": "Continue reading",
		"footer.analysts_report": "Analyst reports",
		"footer.for_enterprises": "For enterprises",
		"footer.getting_started": "Getting Started",
		"footer.learning_center": "Learning Center",
		"footer.project_galileo": "Project Galileo",
		"pagination.newer_posts": "Newer Posts",
		"pagination.older_posts": "Older Posts",
		"posts.social_buttons.x": "Discuss on X",
		"search.icon_aria_label": "Search",
		"search.source_location": "Source/Location",
		"footer.about_cloudflare": "About Cloudflare",
		"footer.athenian_project": "Athenian Project",
		"footer.become_a_partner": "Become a partner",
		"footer.cloudflare_radar": "Cloudflare Radar",
		"footer.network_services": "Network services",
		"footer.trust_and_safety": "Trust & Safety",
		"header.get_started_free": "Get Started Free",
		"page.search.placeholder": "Search Cloudflare",
		"footer.cloudflare_status": "Cloudflare Status",
		"footer.cookie_preference": "Cookie Preferences",
		"header.valid_email_error": "Must be valid email.",
		"search.result_stat_empty": "Results <strong>{search_range}</strong> of <strong>{search_total}</strong>",
		"footer.connectivity_cloud": "Connectivity cloud",
		"footer.developer_services": "Developer services",
		"footer.investor_relations": "Investor relations",
		"page.not_found.error_code": "Error Code: 404",
		"search.autocomplete_title": "Insert a query. Press enter to send",
		"footer.logos_and_press_kit": "Logos & press kit",
		"footer.application_services": "Application services",
		"footer.get_a_recommendation": "Get a recommendation",
		"posts.social_buttons.reddit": "Discuss on Reddit",
		"footer.sse_and_sase_services": "SSE and SASE services",
		"page.not_found.outdated_link": "You may have used an outdated link, or you may have typed the address incorrectly.",
		"footer.report_security_issues": "Report Security Issues",
		"page.error.error_message_page": "Sorry, we can't find the page you are looking for.",
		"header.subscribe_notifications": "Subscribe to receive notifications of new posts:",
		"footer.cloudflare_for_campaigns": "Cloudflare for Campaigns",
		"header.subscription_confimation": "Subscription confirmed. Thank you for subscribing!",
		"posts.social_buttons.hackernews": "Discuss on Hacker News",
		"footer.diversity_equity_inclusion": "Diversity, equity & inclusion",
		"footer.critical_infrastructure_defense_project": "Critical Infrastructure Defense Project"
	}
}