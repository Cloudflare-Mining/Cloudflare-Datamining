{
	"footerBlurb": "Cloudflare's connectivity cloud protects <a target='_blank' href='https://www.cloudflare.com/network-services/' rel='noreferrer'>entire corporate networks</a>, helps customers build <a target='_blank' href='https://workers.cloudflare.com/' rel='noreferrer'>Internet-scale applications efficiently</a>, accelerates any <a target='_blank' href='https://www.cloudflare.com/performance/accelerate-internet-applications/' rel='noreferrer'>website or Internet application</a>, <a target='_blank' href='https://www.cloudflare.com/ddos/' rel='noreferrer'>wards off DDoS attacks</a>, keeps <a target='_blank' href='https://www.cloudflare.com/application-security/' rel='noreferrer'>hackers at bay</a>, and can help you on <a target='_blank' href='https://www.cloudflare.com/products/zero-trust/' rel='noreferrer'>your journey to Zero Trust</a>.<br/><br/>Visit <a target='_blank' href='https://one.one.one.one/' rel='noreferrer'>1.1.1.1</a> from any device to get started with our free app that makes your Internet faster and safer.<br/><br/>To learn more about our mission to help build a better Internet, <a target='_blank' href='https://www.cloudflare.com/learning/what-is-cloudflare/' rel='noreferrer'>start here</a>. If you&apos;re looking for a new career direction, check out <a target='_blank' href='http://www.cloudflare.com/careers' rel='noreferrer'>our open positions</a>.",
	"initialReadingTime": "14",
	"locale": "en-us",
	"localesAvailable": [],
	"post": {
		"authors": [
			{
				"name": "Mike Freemon",
				"slug": "mike-freemon",
				"bio": null,
				"profile_image": "https://cf-assets.www.cloudflare.com/zkvhlag99gkb/6bCvuBC6oIcnc6Gi9RoKkp/cc24dc4c02a11d1d2758f8df495a6ff9/mike-freemon.jpeg",
				"location": null,
				"website": null,
				"twitter": null,
				"facebook": null,
				"publiclyIndex": true
			}
		],
		"excerpt": "In this post, we describe how we modified the Linux kernel to optimize for both low latency and high throughput concurrently",
		"feature_image": "https://cf-assets.www.cloudflare.com/zkvhlag99gkb/507iK0gADvrSqjlYRoVuvQ/a499609894e22b725d1f3ad483bd073e/optimizing-tcp-for-high-throughput-and-low-latency.png",
		"featured": false,
		"html": "\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/3HNI6ivdZlqIiHle3kw74q/7dcfbb260b4f50415f1909df397b52f6/BLOG-1004-header.png\" alt=\"\" class=\"kg-image\" width=\"1600\" height=\"900\" loading=\"lazy\"/>\n            \n            </figure><p>Here at Cloudflare we&#39;re constantly working on improving our service. Our engineers are looking at hundreds of parameters of our traffic, making sure that we get better all the time.</p><p>One of the core numbers we keep a close eye on is HTTP request latency, which is important for many of our products. We regard latency spikes as bugs to be fixed. One example is the 2017 story of <a href=\"/the-sad-state-of-linux-socket-balancing/\">&quot;Why does one NGINX worker take all the load?&quot;</a>, where we optimized our TCP Accept queues to improve overall latency of TCP sockets waiting for accept().</p><p>Performance tuning is a holistic endeavor, and we monitor and continuously improve a range of other performance metrics as well, including throughput. Sometimes, tradeoffs have to be made. Such a case occurred in 2015, when a latency spike was discovered in our processing of HTTP requests. The solution at the time was to set tcp_rmem to 4 MiB, which minimizes the amount of time the kernel spends on TCP collapse processing. It was this collapse processing that was causing the latency spikes. Later in this post we discuss TCP collapse processing in more detail.</p><p>The tradeoff is that using a low value for tcp_rmem limits TCP throughput over high latency links. The following graph shows the maximum throughput as a function of network latency for a window size of 2 MiB. Note that the 2 MiB corresponds to a tcp_rmem value of 4 MiB due to the tcp_adv_win_scale setting in effect at the time.</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/1mRCX3pD7U0Yzn1QeHDDhL/cef49d60e7e32c23d364cbbc65dd50e7/image10-5.png\" alt=\"\" class=\"kg-image\" width=\"787\" height=\"480\" loading=\"lazy\"/>\n            \n            </figure><p>For the Cloudflare products then in existence, this was not a major problem, as connections terminate and content is served from nearby servers due to our BGP anycast routing.</p><p>Since then, we have added new products, such as Magic WAN, WARP, Spectrum, Gateway, and others. These represent new types of use cases and traffic flows.</p><p>For example, imagine you&#39;re a typical Magic WAN customer. You have connected all of your worldwide offices together using the Cloudflare global network. While Time to First Byte still matters, Magic WAN office-to-office traffic also needs good throughput. For example, a lot of traffic over these corporate connections will be file sharing using protocols such as SMB. These are <a href=\"https://en.wikipedia.org/wiki/Elephant_flow\">elephant flows</a> over <a href=\"https://datatracker.ietf.org/doc/html/rfc1072\">long fat networks</a>. Throughput is the metric every eyeball watches as they are downloading files.</p><p>We need to continue to provide world-class low latency while simultaneously providing high throughput over high-latency connections.</p><p>Before we begin, let’s introduce the players in our game.</p><p><b>TCP receive window</b> is the maximum number of unacknowledged user payload bytes the sender should transmit (bytes-in-flight) at any point in time. The size of the receive window can and does go up and down during the course of a TCP session. It is a mechanism whereby the receiver can tell the sender to stop sending if the sent packets cannot be successfully received because the receive buffers are full. It is this receive window that often limits throughput over high-latency networks.</p><p><b>net.ipv4.tcp_adv_win_scale</b> is a (non-intuitive) number used to account for the overhead needed by Linux to process packets. The receive window is specified in terms of user payload bytes. Linux needs additional memory beyond that to track other data associated with packets it is processing.</p><p>The value of the receive window changes during the lifetime of a TCP session, depending on a number of factors. The maximum value that the receive window can be is limited by the amount of free memory available in the receive buffer, according to this table:</p><table><tr><td><p>tcp_adv_win_scale</p></td><td><p>TCP window size</p></td></tr><tr><td><p>4</p></td><td><p>15/16 * available memory in receive buffer</p></td></tr><tr><td><p>3</p></td><td><p>⅞ * available memory in receive buffer</p></td></tr><tr><td><p>2</p></td><td><p>¾ * available memory in receive buffer</p></td></tr><tr><td><p>1</p></td><td><p>½ * available memory in receive buffer</p></td></tr><tr><td><p>0</p></td><td><p>available memory in receive buffer</p></td></tr><tr><td><p>-1</p></td><td><p>½ * available memory in receive buffer</p></td></tr><tr><td><p>-2</p></td><td><p>¼ * available memory in receive buffer</p></td></tr><tr><td><p>-3</p></td><td><p>⅛ * available memory in receive buffer</p></td></tr></table><p>We can intuitively (and correctly) understand that the amount of available memory in the receive buffer is the difference between the used memory and the maximum limit. But what is the maximum size a receive buffer can be? The answer is sk_rcvbuf.</p><p><b>sk_rcvbuf</b> is a per-socket field that specifies the maximum amount of memory that a receive buffer can allocate. This can be set programmatically with the socket option SO_RCVBUF. This can sometimes be useful to do, for localhost TCP sessions, for example, but in general the use of SO_RCVBUF is not recommended.</p><p>So how is sk_rcvbuf set? The most appropriate value for that depends on the latency of the TCP session and other factors. This makes it difficult for L7 applications to know how to set these values correctly, as they will be different for every TCP session. The solution to this problem is Linux autotuning.</p>\n    <div class=\"flex anchor relative\">\n      <h2 id=\"linux-autotuning\">Linux autotuning</h2>\n      <a href=\"#linux-autotuning\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>Linux autotuning is logic in the Linux kernel that adjusts the buffer size limits and the receive window based on actual packet processing. It takes into consideration a number of things including TCP session <a href=\"https://www.cloudflare.com/learning/cdn/glossary/round-trip-time-rtt/\">RTT</a>, L7 read rates, and the amount of available host memory.</p><p>Autotuning can sometimes seem mysterious, but it is actually fairly straightforward.</p><p>The central idea is that Linux can track the rate at which the local application is reading data off of the receive queue. It also knows the session RTT. Because Linux knows these things, it can automatically increase the buffers and receive window until it reaches the point at which the application layer or network bottleneck links are the constraint on throughput (and not host buffer settings). At the same time, autotuning prevents slow local readers from having excessively large receive queues. The way autotuning does that is by limiting the receive window and its corresponding receive buffer to an appropriate size for each socket.</p><p>The values set by autotuning can be seen via the Linux “<code>ss</code>” command from the <code>iproute</code> package (e.g. “<code>ss -tmi</code>”).  The relevant output fields from that command are:</p><p><b>Recv-Q</b> is the number of user payload bytes not yet read by the local application.</p><p><b>rcv_ssthresh</b> is the window clamp, a.k.a. the maximum receive window size. This value is not known to the sender. The sender receives only the current window size, via the TCP header field. A closely-related field in the kernel, tp-&gt;window_clamp, is the maximum window size allowable based on the amount of available memory. rcv_ssthresh is the receiver-side slow-start threshold value.</p><p><b>skmem_r</b> is the actual amount of memory that is allocated, which includes not only user payload (Recv-Q) but also additional memory needed by Linux to process the packet (packet metadata). This is known within the kernel as sk_rmem_alloc.</p><p>Note that there are other buffers associated with a socket, so skmem_r does not represent the total memory that a socket might have allocated. Those other buffers are not involved in the issues presented in this post.</p><p><b>skmem_rb</b> is the maximum amount of memory that could be allocated by the socket for the receive buffer. This is higher than rcv_ssthresh to account for memory needed for packet processing that is not packet data. Autotuning can increase this value (up to tcp_rmem max) based on how fast the L7 application is able to read data from the socket and the RTT of the session. This is known within the kernel as sk_rcvbuf.</p><p><b>rcv_space</b> is the high water mark of the rate of the local application reading from the receive buffer during any RTT. This is used internally within the kernel to adjust sk_rcvbuf.</p><p>Earlier we mentioned a setting called tcp_rmem. <b>net.ipv4.tcp_rmem</b> consists of three values, but in this document we are always referring to the third value (except where noted). It is a global setting that specifies the maximum amount of memory that any TCP receive buffer can allocate, i.e. the maximum permissible value that autotuning can use for sk_rcvbuf. This is essentially just a failsafe for autotuning, and under normal circumstances should play only a minor role in TCP memory management.</p><p>It’s worth mentioning that receive buffer memory is not preallocated. Memory is allocated based on actual packets arriving and sitting in the receive queue. It’s also important to realize that filling up a receive queue is not one of the criteria that autotuning uses to increase sk_rcvbuf. Indeed, preventing this type of excessive buffering (<a href=\"https://en.wikipedia.org/wiki/Bufferbloat\">bufferbloat</a>) is one of the benefits of autotuning.</p>\n    <div class=\"flex anchor relative\">\n      <h2 id=\"whats-the-problem\">What’s the problem?</h2>\n      <a href=\"#whats-the-problem\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>The problem is that we must have a large TCP receive window for high <a href=\"https://en.wikipedia.org/wiki/Bandwidth-delay_product\">BDP</a> sessions. This is directly at odds with the latency spike problem mentioned above.</p><p>Something has to give. The laws of physics (speed of light in glass, etc.) dictate that we must use large window sizes. There is no way to get around that. So we are forced to solve the latency spikes differently.</p>\n    <div class=\"flex anchor relative\">\n      <h2 id=\"a-brief-recap-of-the-latency-spike-problem\">A brief recap of the latency spike problem</h2>\n      <a href=\"#a-brief-recap-of-the-latency-spike-problem\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>Sometimes a TCP session will fill up its receive buffers. When that happens, the Linux kernel will attempt to reduce the amount of memory the receive queue is using by performing what amounts to a “defragmentation” of memory. This is called collapsing the queue. Collapsing the queue takes time, which is what drives up HTTP request latency.</p><p>We do not want to spend time collapsing TCP queues.</p><p>Why do receive queues fill up to the point where they hit the maximum memory limit? The usual situation is when the local application starts out reading data from the receive queue at one rate (triggering autotuning to raise the max receive window), followed by the local application slowing down its reading from the receive queue. This is valid behavior, and we need to handle it correctly.</p>\n    <div class=\"flex anchor relative\">\n      <h2 id=\"selecting-sysctl-values\">Selecting sysctl values</h2>\n      <a href=\"#selecting-sysctl-values\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>Before exploring solutions, let’s first decide what we need as the maximum TCP window size.</p><p>As we have seen above in the discussion about BDP, the window size is determined based upon the RTT and desired throughput of the connection.</p><p>Because Linux autotuning will adjust correctly for sessions with lower RTTs and bottleneck links with lower throughput, all we need to be concerned about are the maximums.</p><p>For latency, we have chosen 300 ms as the maximum expected latency, as that is the measured latency between our Zurich and Sydney facilities. It seems reasonable enough as a worst-case latency under normal circumstances.</p><p>For throughput, although we have very fast and modern hardware on the Cloudflare global network, we don’t expect a single TCP session to saturate the hardware. We have arbitrarily chosen 3500 mbps as the highest supported throughput for our highest latency TCP sessions.</p><p>The calculation for those numbers results in a BDP of 131MB, which we round to the more aesthetic value of 128 MiB.</p><p>Recall that allocation of TCP memory includes metadata overhead in addition to packet data. The ratio of actual amount of memory allocated to user payload size varies, depending on NIC driver settings, packet size, and other factors. For full-sized packets on some of our hardware, we have measured average allocations up to 3 times the packet data size. In order to reduce the frequency of TCP collapse on our servers, we set tcp_adv_win_scale to -2. From the table above, we know that the max window size will be ¼ of the max buffer space.</p><p>We end up with the following sysctl values:</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">net.ipv4.tcp_rmem = 8192 262144 536870912\nnet.ipv4.tcp_wmem = 4096 16384 536870912\nnet.ipv4.tcp_adv_win_scale = -2</pre></code>\n            <p>A tcp_rmem of 512MiB and tcp_adv_win_scale of -2 results in a maximum window size that autotuning can set of 128 MiB, our desired value.</p>\n    <div class=\"flex anchor relative\">\n      <h2 id=\"disabling-tcp-collapse\">Disabling TCP collapse</h2>\n      <a href=\"#disabling-tcp-collapse\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>Patient: Doctor, it hurts when we collapse the TCP receive queue.</p><p>Doctor: Then don’t do that!</p><p>Generally speaking, when a packet arrives at a buffer when the buffer is full, the packet gets dropped. In the case of these receive buffers, Linux tries to “save the packet” when the buffer is full by collapsing the receive queue. Frequently this is successful, but it is not guaranteed to be, and it takes time.</p><p>There are no problems created by immediately just dropping the packet instead of trying to save it. The receive queue is full anyway, so the local receiver application still has data to read. The sender’s congestion control will notice the drop and/or ZeroWindow and will respond appropriately. Everything will continue working as designed.</p><p>At present, there is no setting provided by Linux to disable the TCP collapse. We developed an in-house patch to the kernel to disable the TCP collapse logic.</p>\n    <div class=\"flex anchor relative\">\n      <h2 id=\"kernel-patch-attempt-1\">Kernel patch – Attempt #1</h2>\n      <a href=\"#kernel-patch-attempt-1\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>The kernel patch for our first attempt was straightforward. At the top of tcp_try_rmem_schedule(), if the memory allocation fails, we simply return (after pred_flag = 0 and tcp_sack_reset()), thus completely skipping the tcp_collapse and related logic.</p><p>It didn’t work.</p><p>Although we eliminated the latency spikes while using large buffer limits, we did not observe the throughput we expected.</p><p>One of the realizations we made as we investigated the situation was that standard network benchmarking tools such as iperf3 and similar do not expose the problem we are trying to solve. iperf3 does not fill the receive queue. Linux autotuning does not open the TCP window large enough. Autotuning is working perfectly for our well-behaved benchmarking program.</p><p>We need application-layer software that is slightly less well-behaved, one that exercises the autotuning logic under test. So we wrote one.</p>\n    <div class=\"flex anchor relative\">\n      <h2 id=\"a-new-benchmarking-tool\">A new benchmarking tool</h2>\n      <a href=\"#a-new-benchmarking-tool\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>Anomalies were seen during our “Attempt #1” that negatively impacted throughput. The anomalies were seen only under certain specific conditions, and we realized we needed a better benchmarking tool to detect and measure the performance impact of those anomalies.</p><p>This tool has turned into an invaluable resource during the development of this patch and raised confidence in our solution.</p><p>It consists of two Python programs. The reader opens a TCP session to the daemon, at which point the daemon starts sending user payload as fast as it can, and never stops sending.</p><p>The reader, on the other hand, starts and stops reading in a way to open up the TCP receive window wide open and then repeatedly causes the buffers to fill up completely. More specifically, the reader implemented this logic:</p><ol><li><p>reads as fast as it can, for five seconds</p><ul><li><p>this is called fast mode</p></li><li><p>opens up the window</p></li></ul></li><li><p>calculates 5% of the high watermark of the bytes reader during any previous one second</p></li><li><p>for each second of the next 15 seconds:</p><ul><li><p>this is called slow mode</p></li><li><p>reads that 5% number of bytes, then stops reading</p></li><li><p>sleeps for the remainder of that particular second</p></li><li><p>most of the second consists of no reading at all</p></li></ul></li><li><p>steps 1-3 are repeated in a loop three times, so the entire run is 60 seconds</p></li></ol><p>This has the effect of highlighting any issues in the handling of packets when the buffers repeatedly hit the limit.</p>\n    <div class=\"flex anchor relative\">\n      <h2 id=\"revisiting-default-linux-behavior\">Revisiting default Linux behavior</h2>\n      <a href=\"#revisiting-default-linux-behavior\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>Taking a step back, let’s look at the default Linux behavior. The following is kernel v5.15.16.</p><table><tr><td><p>NIC speed (mbps)</p></td><td><p>RTT (ms)</p></td><td><p>tcp_rmem (MiB)</p></td><td><p>tcp_adv_win_scale</p></td><td><p>tcp_disable_collapse</p></td><td><p>TCP window (MiB)</p></td><td><p>buffer metadata to user payload ratio</p></td><td><p>Prune Called</p></td><td><p>RcvCollapsed</p></td><td><p>RcvQDrop</p></td><td><p>OFODrop</p></td><td><p>Test Result</p></td></tr><tr><td><p>1000</p></td><td><p>300</p></td><td><p>512</p></td><td><p>-2</p></td><td><p>0</p></td><td><p>128</p></td><td><p>4</p></td><td><p>0</p></td><td><p>0</p></td><td><p>0</p></td><td><p>0</p></td><td><p>GOOD</p></td></tr><tr><td><p>1000</p></td><td><p>300</p></td><td><p>256</p></td><td><p>1</p></td><td><p>0</p></td><td><p>128</p></td><td><p>2</p></td><td><p>0</p></td><td><p>0</p></td><td><p>0</p></td><td><p>0</p></td><td><p>GOOD</p></td></tr><tr><td><p>1000</p></td><td><p>300</p></td><td><p>170</p></td><td><p>2</p></td><td><p>0</p></td><td><p>128</p></td><td><p>1.33</p></td><td><p>24</p></td><td><p>490K</p></td><td><p>0</p></td><td><p>0</p></td><td><p>GOOD</p></td></tr><tr><td><p>1000</p></td><td><p>300</p></td><td><p>146</p></td><td><p>3</p></td><td><p>0</p></td><td><p>128</p></td><td><p>1.14</p></td><td><p>57</p></td><td><p>616K</p></td><td><p>0</p></td><td><p>0</p></td><td><p>GOOD</p></td></tr><tr><td><p>1000</p></td><td><p>300</p></td><td><p>137</p></td><td><p>4</p></td><td><p>0</p></td><td><p>128</p></td><td><p>1.07</p></td><td><p>74</p></td><td><p>803K</p></td><td><p>0</p></td><td><p>0</p></td><td><p>GOOD</p></td></tr></table><p>The Linux kernel is effective at freeing up space in order to make room for incoming packets when the receive buffer memory limit is hit. As documented previously, the cost for saving these packets (i.e. not dropping them) is latency.</p><p>However, the latency spikes, in <i>milliseconds</i>, for tcp_try_rmem_schedule(), are:</p><p>tcp_rmem 170 MiB, tcp_adv_win_scale +2 (170p2):</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">@ms:\n[0]       27093 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n[1]           0 |\n[2, 4)        0 |\n[4, 8)        0 |\n[8, 16)       0 |\n[16, 32)      0 |\n[32, 64)     16 |</pre></code>\n            <p>tcp_rmem 146 MiB, tcp_adv_win_scale +3 (146p3):</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">@ms:\n(..., 16)  25984 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n[16, 20)       0 |\n[20, 24)       0 |\n[24, 28)       0 |\n[28, 32)       0 |\n[32, 36)       0 |\n[36, 40)       0 |\n[40, 44)       1 |\n[44, 48)       6 |\n[48, 52)       6 |\n[52, 56)       3 |</pre></code>\n            <p>tcp_rmem 137 MiB, tcp_adv_win_scale +4 (137p4):</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">@ms:\n(..., 16)  37222 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n[16, 20)       0 |\n[20, 24)       0 |\n[24, 28)       0 |\n[28, 32)       0 |\n[32, 36)       0 |\n[36, 40)       1 |\n[40, 44)       8 |\n[44, 48)       2 |</pre></code>\n            <p>These are the latency spikes we cannot have on the Cloudflare global network.</p>\n    <div class=\"flex anchor relative\">\n      <h2 id=\"kernel-patch-attempt-2\">Kernel patch – Attempt #2</h2>\n      <a href=\"#kernel-patch-attempt-2\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>So the “something” that was not working in Attempt #1 was that the receive queue memory limit was hit early on as the flow was just ramping up (when the values for sk_rmem_alloc and sk_rcvbuf were small, ~800KB). This occurred at about the two second mark for 137p4 test (about 2.25 seconds for 170p2).</p><p>In hindsight, we should have noticed that tcp_prune_queue() actually raises sk_rcvbuf when it can. So we modified the patch in response to that, added a guard to allow the collapse to execute when sk_rmem_alloc is less than the threshold value.</p><p><code>net.ipv4.tcp_collapse_max_bytes = 6291456</code></p><p>The next section discusses how we arrived at this value for tcp_collapse_max_bytes.</p><p>The patch is available <a href=\"https://github.com/cloudflare/linux/blob/master/patches/0014-add-a-sysctl-to-enable-disable-tcp_collapse-logic.patch\">here</a>.</p><p>The results with the new patch are as follows:</p><p>oscil – 300ms tests</p><table><tr><td><p>Test</p></td><td><p>RTT (ms)</p></td><td><p>tcp_rmem (MiB)</p></td><td><p>tcp_adv_win_scale</p></td><td><p>tcp_disable_collapse (MiB)</p></td><td><p>NIC speed (mbps)</p></td><td><p>TCP window (MiB)</p></td><td><p>real buffer metadata to user payload ratio</p></td><td><p>RcvCollapsed</p></td><td><p>RcvQDrop</p></td><td><p>OFODrop</p></td><td><p>max latency (us)</p><p>\n</p></td><td><p>Test Result</p></td></tr><tr><td><p>oscil reader</p></td><td><p>300</p></td><td><p>512</p></td><td><p>-2</p></td><td><p>6</p></td><td><p>1000</p></td><td><p>128</p></td><td><p>4</p></td><td><p>0</p></td><td><p>0</p></td><td><p>0</p></td><td><p>12</p></td><td><p>1-941</p><p>941</p><p>941</p></td></tr><tr><td><p>oscil reader</p></td><td><p>300</p></td><td><p>256</p></td><td><p>1</p></td><td><p>6</p></td><td><p>1000</p></td><td><p>128</p></td><td><p>2</p></td><td><p>0</p></td><td><p>0</p></td><td><p>0</p></td><td><p>11</p></td><td><p>1-941</p><p>941</p><p>941</p></td></tr><tr><td><p>oscil reader</p></td><td><p>300</p></td><td><p>170</p></td><td><p>2</p></td><td><p>6</p></td><td><p>1000</p></td><td><p>128</p></td><td><p>1.33</p></td><td><p>0</p></td><td><p>9</p></td><td><p>86</p></td><td><p>11</p></td><td><p>1-941</p><p>36-605</p><p>1-298</p></td></tr><tr><td><p>oscil reader</p></td><td><p>300</p></td><td><p>146</p></td><td><p>3</p></td><td><p>6</p></td><td><p>1000</p></td><td><p>128</p></td><td><p>1.14</p></td><td><p>0</p></td><td><p>7</p></td><td><p>1550</p></td><td><p>16</p></td><td><p>1-940</p><p>2-82</p><p>292-395</p></td></tr><tr><td><p>oscil reader</p></td><td><p>300</p></td><td><p>137</p></td><td><p>4</p></td><td><p>6</p></td><td><p>1000</p></td><td><p>128</p></td><td><p>1.07</p></td><td><p>0</p></td><td><p>10</p></td><td><p>3020</p></td><td><p>9</p></td><td><p>1-940</p><p>2-13</p><p>13-33</p></td></tr></table><p>oscil – 20ms tests</p><table><tr><td><p>Test</p></td><td><p>RTT (ms)</p></td><td><p>tcp_rmem (MiB)</p></td><td><p>tcp_adv_win_scale</p></td><td><p>tcp_disable_collapse (MiB)</p></td><td><p>NIC speed (mbps)</p></td><td><p>TCP window (MiB)</p></td><td><p>real buffer metadata to user payload ratio</p></td><td><p>RcvCollapsed</p></td><td><p>RcvQDrop</p></td><td><p>OFODrop</p></td><td><p>max latency (us)</p><p>\n</p></td><td><p>Test Result</p></td></tr><tr><td><p>oscil reader</p></td><td><p>20</p></td><td><p>512</p></td><td><p>-2</p></td><td><p>6</p></td><td><p>1000</p></td><td><p>128</p></td><td><p>4</p></td><td><p>0</p></td><td><p>0</p></td><td><p>0</p></td><td><p>13</p></td><td><p>795-941</p><p>941</p><p>941</p></td></tr><tr><td><p>oscil reader</p></td><td><p>20</p></td><td><p>256</p></td><td><p>1</p></td><td><p>6</p></td><td><p>1000</p></td><td><p>128</p></td><td><p>2</p></td><td><p>0</p></td><td><p>0</p></td><td><p>0</p></td><td><p>13</p></td><td><p>795-941</p><p>941</p><p>941</p></td></tr><tr><td><p>oscil reader</p></td><td><p>20</p></td><td><p>170</p></td><td><p>2</p></td><td><p>6</p></td><td><p>1000</p></td><td><p>128</p></td><td><p>1.33</p></td><td><p>0</p></td><td><p>0</p></td><td><p>0</p></td><td><p>8</p></td><td><p>795-941</p><p>941</p><p>941</p></td></tr><tr><td><p>oscil reader</p></td><td><p>20</p></td><td><p>146</p></td><td><p>3</p></td><td><p>6</p></td><td><p>1000</p></td><td><p>128</p></td><td><p>1.14</p></td><td><p>0</p></td><td><p>0</p></td><td><p>0</p></td><td><p>7</p></td><td><p>795-941</p><p>941</p><p>941</p></td></tr><tr><td><p>oscil reader</p></td><td><p>20</p></td><td><p>137</p></td><td><p>4</p></td><td><p>6</p></td><td><p>1000</p></td><td><p>128</p></td><td><p>1.07</p></td><td><p>0</p></td><td><p>4</p></td><td><p>196</p></td><td><p>12</p></td><td><p>795-941</p><p>13-941</p><p>941</p></td></tr></table><p>oscil – 0ms tests</p><table><tr><td><p>Test</p></td><td><p>RTT (ms)</p></td><td><p>tcp_rmem (MiB)</p></td><td><p>tcp_adv_win_scale</p></td><td><p>tcp_disable_collapse (MiB)</p></td><td><p>NIC speed (mbps)</p></td><td><p>TCP window (MiB)</p></td><td><p>real buffer metadata to user payload ratio</p></td><td><p>RcvCollapsed</p></td><td><p>RcvQDrop</p></td><td><p>OFODrop</p></td><td><p>max latency (us)</p><p>\n</p></td><td><p>Test Result</p></td></tr><tr><td><p>oscil reader</p></td><td><p>0.3</p></td><td><p>512</p></td><td><p>-2</p></td><td><p>6</p></td><td><p>1000</p></td><td><p>128</p></td><td><p>4</p></td><td><p>0</p></td><td><p>0</p></td><td><p>0</p></td><td><p>9</p></td><td><p>941</p><p>941</p><p>941</p></td></tr><tr><td><p>oscil reader</p></td><td><p>0.3</p></td><td><p>256</p></td><td><p>1</p></td><td><p>6</p></td><td><p>1000</p></td><td><p>128</p></td><td><p>2</p></td><td><p>0</p></td><td><p>0</p></td><td><p>0</p></td><td><p>22</p></td><td><p>941</p><p>941</p><p>941</p></td></tr><tr><td><p>oscil reader</p></td><td><p>0.3</p></td><td><p>170</p></td><td><p>2</p></td><td><p>6</p></td><td><p>1000</p></td><td><p>128</p></td><td><p>1.33</p></td><td><p>0</p></td><td><p>0</p></td><td><p>0</p></td><td><p>8</p></td><td><p>941</p><p>941</p><p>941</p></td></tr><tr><td><p>oscil reader</p></td><td><p>0.3</p></td><td><p>146</p></td><td><p>3</p></td><td><p>6</p></td><td><p>1000</p></td><td><p>128</p></td><td><p>1.14</p></td><td><p>0</p></td><td><p>0</p></td><td><p>0</p></td><td><p>10</p></td><td><p>941</p><p>941</p><p>941</p></td></tr><tr><td><p>oscil reader</p></td><td><p>0.3</p></td><td><p>137</p></td><td><p>4</p></td><td><p>6</p></td><td><p>1000</p></td><td><p>128</p></td><td><p>1.07</p></td><td><p>0</p></td><td><p>0</p></td><td><p>0</p></td><td><p>10</p></td><td><p>941</p><p>941</p><p>941</p></td></tr></table><p>iperf3 – 300 ms tests</p><table><tr><td><p>Test</p></td><td><p>RTT (ms)</p></td><td><p>tcp_rmem (MiB)</p></td><td><p>tcp_adv_win_scale</p></td><td><p>tcp_disable_collapse (MiB)</p></td><td><p>NIC speed (mbps)</p></td><td><p>TCP window (MiB)</p></td><td><p>real buffer metadata to user payload ratio</p></td><td><p>RcvCollapsed</p></td><td><p>RcvQDrop</p></td><td><p>OFODrop</p></td><td><p>max latency (us)</p><p>\n</p></td><td><p>Test Result</p></td></tr><tr><td><p>iperf3</p></td><td><p>300</p></td><td><p>512</p></td><td><p>-2</p></td><td><p>6</p></td><td><p>1000</p></td><td><p>128</p></td><td><p>4</p></td><td><p>0</p></td><td><p>0</p></td><td><p>0</p></td><td><p>7</p></td><td><p>941</p></td></tr><tr><td><p>iperf3</p></td><td><p>300</p></td><td><p>256</p></td><td><p>1</p></td><td><p>6</p></td><td><p>1000</p></td><td><p>128</p></td><td><p>2</p></td><td><p>0</p></td><td><p>0</p></td><td><p>0</p></td><td><p>6</p></td><td><p>941</p></td></tr><tr><td><p>iperf3</p></td><td><p>300</p></td><td><p>170</p></td><td><p>2</p></td><td><p>6</p></td><td><p>1000</p></td><td><p>128</p></td><td><p>1.33</p></td><td><p>0</p></td><td><p>0</p></td><td><p>0</p></td><td><p>9</p></td><td><p>941</p></td></tr><tr><td><p>iperf3</p></td><td><p>300</p></td><td><p>146</p></td><td><p>3</p></td><td><p>6</p></td><td><p>1000</p></td><td><p>128</p></td><td><p>1.14</p></td><td><p>0</p></td><td><p>0</p></td><td><p>0</p></td><td><p>11</p></td><td><p>941</p></td></tr><tr><td><p>iperf3</p></td><td><p>300</p></td><td><p>137</p></td><td><p>4</p></td><td><p>6</p></td><td><p>1000</p></td><td><p>128</p></td><td><p>1.07</p></td><td><p>0</p></td><td><p>0</p></td><td><p>0</p></td><td><p>7</p></td><td><p>941</p></td></tr></table><p>iperf3 – 20 ms tests</p><table><tr><td><p>Test</p></td><td><p>RTT (ms)</p></td><td><p>tcp_rmem (MiB)</p></td><td><p>tcp_adv_win_scale</p></td><td><p>tcp_disable_collapse (MiB)</p></td><td><p>NIC speed (mbps)</p></td><td><p>TCP window (MiB)</p></td><td><p>real buffer metadata to user payload ratio</p></td><td><p>RcvCollapsed</p></td><td><p>RcvQDrop</p></td><td><p>OFODrop</p></td><td><p>max latency (us)</p><p>\n</p></td><td><p>Test Result</p></td></tr><tr><td><p>iperf3</p></td><td><p>20</p></td><td><p>512</p></td><td><p>-2</p></td><td><p>6</p></td><td><p>1000</p></td><td><p>128</p></td><td><p>4</p></td><td><p>0</p></td><td><p>0</p></td><td><p>0</p></td><td><p>7</p></td><td><p>941</p></td></tr><tr><td><p>iperf3</p></td><td><p>20</p></td><td><p>256</p></td><td><p>1</p></td><td><p>6</p></td><td><p>1000</p></td><td><p>128</p></td><td><p>2</p></td><td><p>0</p></td><td><p>0</p></td><td><p>0</p></td><td><p>15</p></td><td><p>941</p></td></tr><tr><td><p>iperf3</p></td><td><p>20</p></td><td><p>170</p></td><td><p>2</p></td><td><p>6</p></td><td><p>1000</p></td><td><p>128</p></td><td><p>1.33</p></td><td><p>0</p></td><td><p>0</p></td><td><p>0</p></td><td><p>7</p></td><td><p>941</p></td></tr><tr><td><p>iperf3</p></td><td><p>20</p></td><td><p>146</p></td><td><p>3</p></td><td><p>6</p></td><td><p>1000</p></td><td><p>128</p></td><td><p>1.14</p></td><td><p>0</p></td><td><p>0</p></td><td><p>0</p></td><td><p>7</p></td><td><p>941</p></td></tr><tr><td><p>iperf3</p></td><td><p>20</p></td><td><p>137</p></td><td><p>4</p></td><td><p>6</p></td><td><p>1000</p></td><td><p>128</p></td><td><p>1.07</p></td><td><p>0</p></td><td><p>0</p></td><td><p>0</p></td><td><p>6</p></td><td><p>941</p></td></tr></table><p>iperf3 – 0ms tests</p><table><tr><td><p>Test</p></td><td><p>RTT (ms)</p></td><td><p>tcp_rmem (MiB)</p></td><td><p>tcp_adv_win_scale</p></td><td><p>tcp_disable_collapse (MiB)</p></td><td><p>NIC speed (mbps)</p></td><td><p>TCP window (MiB)</p></td><td><p>real buffer metadata to user payload ratio</p></td><td><p>RcvCollapsed</p></td><td><p>RcvQDrop</p></td><td><p>OFODrop</p></td><td><p>max latency (us)</p><p>\n</p></td><td><p>Test Result</p></td></tr><tr><td><p>iperf3</p></td><td><p>0.3</p></td><td><p>512</p></td><td><p>-2</p></td><td><p>6</p></td><td><p>1000</p></td><td><p>128</p></td><td><p>4</p></td><td><p>0</p></td><td><p>0</p></td><td><p>0</p></td><td><p>6</p></td><td><p>941</p></td></tr><tr><td><p>iperf3</p></td><td><p>0.3</p></td><td><p>256</p></td><td><p>1</p></td><td><p>6</p></td><td><p>1000</p></td><td><p>128</p></td><td><p>2</p></td><td><p>0</p></td><td><p>0</p></td><td><p>0</p></td><td><p>14</p></td><td><p>941</p></td></tr><tr><td><p>iperf3</p></td><td><p>0.3</p></td><td><p>170</p></td><td><p>2</p></td><td><p>6</p></td><td><p>1000</p></td><td><p>128</p></td><td><p>1.33</p></td><td><p>0</p></td><td><p>0</p></td><td><p>0</p></td><td><p>6</p></td><td><p>941</p></td></tr><tr><td><p>iperf3</p></td><td><p>0.3</p></td><td><p>146</p></td><td><p>3</p></td><td><p>6</p></td><td><p>1000</p></td><td><p>128</p></td><td><p>1.14</p></td><td><p>0</p></td><td><p>0</p></td><td><p>0</p></td><td><p>7</p></td><td><p>941</p></td></tr><tr><td><p>iperf3</p></td><td><p>0.3</p></td><td><p>137</p></td><td><p>4</p></td><td><p>6</p></td><td><p>1000</p></td><td><p>128</p></td><td><p>1.07</p></td><td><p>0</p></td><td><p>0</p></td><td><p>0</p></td><td><p>6</p></td><td><p>941</p></td></tr></table><p>All tests are successful.</p>\n    <div class=\"flex anchor relative\">\n      <h2 id=\"setting-tcp_collapse_max_bytes\">Setting tcp_collapse_max_bytes</h2>\n      <a href=\"#setting-tcp_collapse_max_bytes\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>In order to determine this setting, we need to understand what the biggest queue we <i>can</i> collapse without incurring unacceptable latency.</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/6PfP9U02g39dedYXSqz0H2/8b61121e3ffe7f102fea0d682d902c56/image8-12.png\" alt=\"\" class=\"kg-image\" width=\"813\" height=\"474\" loading=\"lazy\"/>\n            \n            </figure>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/14e9NHQT0aciu7bJvpLyIM/5df55a86e2a24c7c7501c2c80a38e847/image7-13.png\" alt=\"\" class=\"kg-image\" width=\"820\" height=\"477\" loading=\"lazy\"/>\n            \n            </figure><p>Using 6 MiB should result in a maximum latency of no more than 2 ms.</p>\n    <div class=\"flex anchor relative\">\n      <h2 id=\"cloudflare-production-network-results\">Cloudflare production network results</h2>\n      <a href=\"#cloudflare-production-network-results\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    \n    <div class=\"flex anchor relative\">\n      <h3 id=\"current-production-settings-old\">Current production settings (“Old”)</h3>\n      <a href=\"#current-production-settings-old\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    \n            <pre class=\"language-bash\"><code class=\"language-bash\">net.ipv4.tcp_rmem = 8192 2097152 16777216\nnet.ipv4.tcp_wmem = 4096 16384 33554432\nnet.ipv4.tcp_adv_win_scale = -2\nnet.ipv4.tcp_collapse_max_bytes = 0\nnet.ipv4.tcp_notsent_lowat = 4294967295</pre></code>\n            <p>tcp_collapse_max_bytes of 0 means that the custom feature is disabled and that the vanilla kernel logic is used for TCP collapse processing.</p>\n    <div class=\"flex anchor relative\">\n      <h3 id=\"new-settings-under-test-new\">New settings under test (“New”)</h3>\n      <a href=\"#new-settings-under-test-new\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    \n            <pre class=\"language-bash\"><code class=\"language-bash\">net.ipv4.tcp_rmem = 8192 262144 536870912\nnet.ipv4.tcp_wmem = 4096 16384 536870912\nnet.ipv4.tcp_adv_win_scale = -2\nnet.ipv4.tcp_collapse_max_bytes = 6291456\nnet.ipv4.tcp_notsent_lowat = 131072</pre></code>\n            <p>The tcp_notsent_lowat setting is discussed in the last section of this post.</p><p>The middle value of tcp_rmem was changed as a result of separate work that found that Linux autotuning was setting receive buffers too high for localhost sessions. This updated setting reduces TCP memory usage for those sessions, but does not change anything about the type of TCP sessions that is the focus of this post.</p><p>For the following benchmarks, we used non-Cloudflare host machines in Iowa, US, and Melbourne, Australia performing data transfers to the Cloudflare data center in Marseille, France. In Marseille, we have some hosts configured with the existing production settings, and others with the system settings described in this post. Software used is perf3 version 3.9, kernel 5.15.32.</p>\n    <div class=\"flex anchor relative\">\n      <h3 id=\"throughput-results\">Throughput results</h3>\n      <a href=\"#throughput-results\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    \n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/3GkgLi4X0vuXMl7rOEWvZK/e64146a2a197174ab5e563b34b8ba9e8/image3-36.png\" alt=\"\" class=\"kg-image\" width=\"757\" height=\"449\" loading=\"lazy\"/>\n            \n            </figure><table><tr><td><p>\n</p></td><td><p>RTT</p><p>(ms)</p></td><td><p>Throughput with Current Settings</p><p>(mbps)</p></td><td><p>Throughput with</p><p>New Settings</p><p>(mbps)</p></td><td><p>Increase</p><p>Factor</p></td></tr><tr><td><p>Iowa to</p><p>Marseille</p></td><td><p>121 </p></td><td><p>276</p></td><td><p>6600</p></td><td><p>24x</p></td></tr><tr><td><p>Melbourne to Marseille</p></td><td><p>282</p></td><td><p>120</p></td><td><p>3800</p></td><td><p>32x</p></td></tr></table><p><b>Iowa-Marseille throughput</b></p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/2WvQmKxKD8buJU17GU15Kw/798330bea25ad3dedfe5d1beb8578363/image6-16.png\" alt=\"\" class=\"kg-image\" width=\"958\" height=\"512\" loading=\"lazy\"/>\n            \n            </figure><p><b>Iowa-Marseille receive window and bytes-in-flight</b></p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/2V2iFmp0s1h8vH4SyrzFQS/5b1118af5ff1ee2116aacfe7c9d60927/image2-51.png\" alt=\"\" class=\"kg-image\" width=\"1023\" height=\"507\" loading=\"lazy\"/>\n            \n            </figure><p><b>Melbourne-Marseille throughput</b></p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/PnqeHPMJbmCuyGvt9UjtA/037328ab56686611fce568409e0c8336/image9-10.png\" alt=\"\" class=\"kg-image\" width=\"965\" height=\"514\" loading=\"lazy\"/>\n            \n            </figure><p><b>Melbourne-Marseille receive window and bytes-in-flight</b></p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/4H5cZs3oyG7KiTKL9m4b7R/ff9563e9d1a69da0adb653587dd1f336/image5-21.png\" alt=\"\" class=\"kg-image\" width=\"1030\" height=\"504\" loading=\"lazy\"/>\n            \n            </figure><p>Even with the new settings in place, the Melbourne to Marseille performance is limited by the receive window on the Cloudflare host. This means that further adjustments to these settings yield even higher throughput.</p>\n    <div class=\"flex anchor relative\">\n      <h3 id=\"latency-results\">Latency results</h3>\n      <a href=\"#latency-results\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>The Y-axis on these charts are the 99th percentile time for TCP collapse in seconds.</p><p><b>Cloudflare hosts in Marseille running the current production settings</b></p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/1jxkFikBYH12eNxfHcHwXR/c8c3e183a28b84a4f93bc725309d5eff/image11-4.png\" alt=\"\" class=\"kg-image\" width=\"765\" height=\"372\" loading=\"lazy\"/>\n            \n            </figure><p><b>Cloudflare hosts in Marseille running the new settings</b></p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/6dqk8mtvEpghINGK0bfKUz/cec115db36d4a927ca4764bfe022cad8/image1-59.png\" alt=\"\" class=\"kg-image\" width=\"764\" height=\"368\" loading=\"lazy\"/>\n            \n            </figure><p>The takeaway in looking at these graphs is that maximum TCP collapse time for the new settings is no worse than with the current production settings. This is the desired result.</p>\n    <div class=\"flex anchor relative\">\n      <h3 id=\"send-buffers\">Send Buffers</h3>\n      <a href=\"#send-buffers\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>What we have shown so far is that the receiver side seems to be working well, but what about the sender side?</p><p>As part of this work, we are setting tcp_wmem max to 512 MiB. For oscillating reader flows, this can cause the send buffer to become quite large. This represents bufferbloat and wasted kernel memory, both things that nobody likes or wants.</p><p>Fortunately, there is already a solution: <b>tcp_notsent_lowat</b>. This setting limits the size of unsent bytes in the write queue. More details can be found at <a href=\"https://lwn.net/Articles/560082/\">https://lwn.net/Articles/560082</a>.</p><p>The results are significant:</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/3yXWQ4mWrhKWV8Tzv0bUCb/75339710d08b943a8cb884a68377d4ec/image4-29.png\" alt=\"\" class=\"kg-image\" width=\"1199\" height=\"639\" loading=\"lazy\"/>\n            \n            </figure><p>The RTT for these tests was 466ms. Throughput is not negatively affected. Throughput is at full wire speed in all cases (1 Gbps). Memory usage is as reported by /proc/net/sockstat, TCP mem.</p><p>Our web servers already set tcp_notsent_lowat to 131072 for its sockets. All other senders are using 4 GiB, the default value. We are changing the sysctl so that 131072 is in effect for all senders running on the server.</p>\n    <div class=\"flex anchor relative\">\n      <h2 id=\"conclusion\">Conclusion</h2>\n      <a href=\"#conclusion\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>The goal of this work is to open the throughput floodgates for high BDP connections while simultaneously ensuring very low HTTP request latency.</p><p>We have accomplished that goal.</p><p>...<i>We protect </i><a href=\"https://www.cloudflare.com/network-services/\"><i>entire corporate networks</i></a><i>, help customers build </i><a href=\"https://workers.cloudflare.com/\"><i>Internet-scale applications efficiently</i></a><i>, accelerate any </i><a href=\"https://www.cloudflare.com/performance/accelerate-internet-applications/\"><i>website or Internet application</i></a><i>, ward off </i><a href=\"https://www.cloudflare.com/ddos/\"><i>DDoS attacks</i></a><i>, keep </i><a href=\"https://www.cloudflare.com/application-security/\"><i>hackers at bay</i></a><i>, and can help you on </i><a href=\"https://www.cloudflare.com/products/zero-trust/\"><i>your journey to Zero Trust</i></a><i>.</i></p><p><i>Visit </i><a href=\"https://1.1.1.1/\"><i>1.1.1.1</i></a><i> from any device to get started with our free app that makes your Internet faster and safer.To learn more about our mission to help build a better Internet, start </i><a href=\"https://www.cloudflare.com/learning/what-is-cloudflare/\"><i>here</i></a><i>. If you’re looking for a new career direction, check out </i><a href=\"http://cloudflare.com/careers\"><i>our open positions</i></a><i>.</i></p>",
		"id": "BcOgofewzZGwenQrFsVMq",
		"localeList": {
			"name": "Optimizing TCP for high WAN throughput while preserving low latency Config",
			"enUS": "English for Locale",
			"zhCN": "No Page for Locale",
			"zhHansCN": "No Page for Locale",
			"zhTW": "No Page for Locale",
			"frFR": "No Page for Locale",
			"deDE": "No Page for Locale",
			"itIT": "No Page for Locale",
			"jaJP": "No Page for Locale",
			"koKR": "No Page for Locale",
			"ptBR": "No Page for Locale",
			"esLA": "No Page for Locale",
			"esES": "No Page for Locale",
			"enAU": "No Page for Locale",
			"enCA": "No Page for Locale",
			"enIN": "No Page for Locale",
			"enGB": "No Page for Locale",
			"idID": "No Page for Locale",
			"ruRU": "No Page for Locale",
			"svSE": "No Page for Locale",
			"viVN": "No Page for Locale",
			"plPL": "No Page for Locale",
			"arAR": "No Page for Locale",
			"nlNL": "No Page for Locale",
			"thTH": "No Page for Locale",
			"trTR": "No Page for Locale",
			"heIL": "No Page for Locale",
			"lvLV": "No Page for Locale",
			"etEE": "No Page for Locale",
			"ltLT": "No Page for Locale"
		},
		"meta_description": "Tuning TCP servers for both low latency and high WAN throughput usually involves making tradeoffs. Due to the breadth of products and variety of traffic patterns at Cloudflare, we need both. In this post, we describe how we modified the Linux kernel to optimize for both low latency and high throughput concurrently.",
		"metadata": {
			"title": "Optimizing TCP for high WAN throughput while preserving low latency",
			"description": "Tuning TCP servers for both low latency and high WAN throughput usually involves making tradeoffs. Due to the breadth of products and variety of traffic patterns at Cloudflare, we need both. In this post, we describe how we modified the Linux kernel to optimize for both low latency and high throughput concurrently.",
			"imgPreview": "https://cf-assets.www.cloudflare.com/zkvhlag99gkb/6M3gtHcZ6gzaPAXhRRbenD/f2c1c1738315d1433dfbcf0ed27c5f41/optimizing-tcp-for-high-throughput-and-low-latency-UeGazu.png"
		},
		"primary_author": {},
		"publicly_index": true,
		"published_at": "2022-07-01T14:00:01.000+01:00",
		"slug": "optimizing-tcp-for-high-throughput-and-low-latency",
		"tags": [
			{
				"id": "2UVIYusJwlvsmPYl2AvSuR",
				"name": "Deep Dive",
				"slug": "deep-dive"
			},
			{
				"id": "5NpgoTJYJjhgjSLaY7Gt3p",
				"name": "TCP",
				"slug": "tcp"
			},
			{
				"id": "3VzPc3tH2H1sIl54PuVJbX",
				"name": "Latency",
				"slug": "latency"
			},
			{
				"id": "2kPXcZlQ7I9S1hKI5tRYxl",
				"name": "Optimization",
				"slug": "optimization"
			}
		],
		"title": "Optimizing TCP for high WAN throughput while preserving low latency",
		"updated_at": "2025-10-03T17:46:05.494Z",
		"url": "https://blog.cloudflare.com/optimizing-tcp-for-high-throughput-and-low-latency"
	},
	"translations": {
		"posts.by": "By",
		"footer.gdpr": "GDPR",
		"lang_blurb1": "This post is also available in {lang1}.",
		"lang_blurb2": "This post is also available in {lang1} and {lang2}.",
		"lang_blurb3": "This post is also available in {lang1}, {lang2} and {lang3}.",
		"footer.press": "Press",
		"header.title": "The Cloudflare Blog",
		"search.clear": "Clear",
		"search.filter": "Filter",
		"search.source": "Source",
		"footer.careers": "Careers",
		"footer.company": "Company",
		"footer.support": "Support",
		"footer.the_net": "theNet",
		"search.filters": "Filters",
		"footer.our_team": "Our team",
		"footer.webinars": "Webinars",
		"page.more_posts": "More posts",
		"posts.time_read": "{time} min read",
		"search.language": "Language",
		"footer.community": "Community",
		"footer.resources": "Resources",
		"footer.solutions": "Solutions",
		"footer.trademark": "Trademark",
		"header.subscribe": "Subscribe",
		"footer.compliance": "Compliance",
		"footer.free_plans": "Free plans",
		"footer.impact_ESG": "Impact/ESG",
		"posts.follow_on_X": "Follow on X",
		"footer.help_center": "Help center",
		"footer.network_map": "Network Map",
		"header.please_wait": "Please Wait",
		"page.related_posts": "Related posts",
		"search.result_stat": "Results <strong>{search_range}</strong> of <strong>{search_total}</strong> for <strong>{search_keyword}</strong>",
		"footer.case_studies": "Case Studies",
		"footer.connect_2024": "Connect 2024",
		"footer.terms_of_use": "Terms of Use",
		"footer.white_papers": "White Papers",
		"footer.cloudflare_tv": "Cloudflare TV",
		"footer.community_hub": "Community Hub",
		"footer.compare_plans": "Compare plans",
		"footer.contact_sales": "Contact Sales",
		"header.contact_sales": "Contact Sales",
		"header.email_address": "Email Address",
		"page.error.not_found": "Page not found",
		"footer.developer_docs": "Developer docs",
		"footer.privacy_policy": "Privacy Policy",
		"footer.request_a_demo": "Request a demo",
		"page.continue_reading": "Continue reading",
		"footer.analysts_report": "Analyst reports",
		"footer.for_enterprises": "For enterprises",
		"footer.getting_started": "Getting Started",
		"footer.learning_center": "Learning Center",
		"footer.project_galileo": "Project Galileo",
		"pagination.newer_posts": "Newer Posts",
		"pagination.older_posts": "Older Posts",
		"posts.social_buttons.x": "Discuss on X",
		"search.icon_aria_label": "Search",
		"search.source_location": "Source/Location",
		"footer.about_cloudflare": "About Cloudflare",
		"footer.athenian_project": "Athenian Project",
		"footer.become_a_partner": "Become a partner",
		"footer.cloudflare_radar": "Cloudflare Radar",
		"footer.network_services": "Network services",
		"footer.trust_and_safety": "Trust & Safety",
		"header.get_started_free": "Get Started Free",
		"page.search.placeholder": "Search Cloudflare",
		"footer.cloudflare_status": "Cloudflare Status",
		"footer.cookie_preference": "Cookie Preferences",
		"header.valid_email_error": "Must be valid email.",
		"search.result_stat_empty": "Results <strong>{search_range}</strong> of <strong>{search_total}</strong>",
		"footer.connectivity_cloud": "Connectivity cloud",
		"footer.developer_services": "Developer services",
		"footer.investor_relations": "Investor relations",
		"page.not_found.error_code": "Error Code: 404",
		"search.autocomplete_title": "Insert a query. Press enter to send",
		"footer.logos_and_press_kit": "Logos & press kit",
		"footer.application_services": "Application services",
		"footer.get_a_recommendation": "Get a recommendation",
		"posts.social_buttons.reddit": "Discuss on Reddit",
		"footer.sse_and_sase_services": "SSE and SASE services",
		"page.not_found.outdated_link": "You may have used an outdated link, or you may have typed the address incorrectly.",
		"footer.report_security_issues": "Report Security Issues",
		"page.error.error_message_page": "Sorry, we can't find the page you are looking for.",
		"header.subscribe_notifications": "Subscribe to receive notifications of new posts:",
		"footer.cloudflare_for_campaigns": "Cloudflare for Campaigns",
		"header.subscription_confimation": "Subscription confirmed. Thank you for subscribing!",
		"posts.social_buttons.hackernews": "Discuss on Hacker News",
		"footer.diversity_equity_inclusion": "Diversity, equity & inclusion",
		"footer.critical_infrastructure_defense_project": "Critical Infrastructure Defense Project"
	}
}