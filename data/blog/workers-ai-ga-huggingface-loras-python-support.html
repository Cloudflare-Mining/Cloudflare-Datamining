<div class="mb2 gray5">8 min read</div>
<div class="mt4">This post is also available in <a href="https://blog.cloudflare.com/zh-cn/workers-ai-ga-huggingface-loras-python-support">简体中文</a>, <a href="https://blog.cloudflare.com/fr-fr/workers-ai-ga-huggingface-loras-python-support">Français</a>, <a href="https://blog.cloudflare.com/de-de/workers-ai-ga-huggingface-loras-python-support">Deutsch</a>, <a href="https://blog.cloudflare.com/ja-jp/workers-ai-ga-huggingface-loras-python-support">日本語</a>, <a href="https://blog.cloudflare.com/ko-kr/workers-ai-ga-huggingface-loras-python-support">한국어</a>, <a href="https://blog.cloudflare.com/es-es/workers-ai-ga-huggingface-loras-python-support">Español</a> and <a href="https://blog.cloudflare.com/zh-tw/workers-ai-ga-huggingface-loras-python-support">繁體中文</a>.</div>
<div class="post-content lh-copy gray1">
	<p></p>
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/1YNXJ4s4e47U7MvddTlpz8/33fcaa7b05febdc7247be327d35f0e2f/Cities-with-GPUs-momentum-update.png" alt="Leveling up Workers AI: general availability and more new capabilities" class="kg-image" width="1600" height="900" loading="lazy">

	</figure>
	<p>Welcome to Tuesday – our AI day of Developer Week 2024! In this blog post, we’re excited to share an overview of our new AI announcements and vision, including news about Workers AI officially going GA with improved pricing, a GPU hardware momentum update, an expansion of our Hugging Face partnership, Bring Your Own LoRA fine-tuned inference, Python support in Workers, more providers in AI Gateway, and Vectorize metadata filtering.</p>
	<h3>Workers AI GA</h3>
	<p>Today, we’re excited to announce that our Workers AI inference platform is now Generally Available. After months of being in open beta, we’ve improved our service with greater reliability and performance, unveiled pricing, and added many more models to our catalog.</p>
	<h4>Improved performance &amp; reliability</h4>
	<p>With Workers AI, our goal is to make AI inference as reliable and easy to use as the rest of Cloudflare’s network. Under the hood, we’ve upgraded the load balancing that is built into Workers AI. Requests can now be routed to more GPUs in more cities, and each city is aware of the total available capacity for AI inference. If the request would have to wait in a queue in the current city, it can instead be routed to another location, getting results back to you faster when traffic is high. With this, we’ve increased rate limits across all our models – most LLMs now have a of 300 requests per minute, up from 50 requests per minute during our beta phase. Smaller models have a limit of 1500-3000 requests per minute. Check out our <a href="https://developers.cloudflare.com/workers-ai/platform/limits">Developer Docs for the rate limits</a> of individual models.</p>
	<h4>Lowering costs on popular models</h4>
	<p>Alongside our GA of Workers AI, we published a <a href="https://ai.cloudflare.com/#pricing-calculator">pricing calculator</a> for our 10 non-beta models earlier this month. We want Workers AI to be one of the most affordable and accessible solutions to run <a href="https://www.cloudflare.com/learning/ai/inference-vs-training">inference</a>, so we added a few optimizations to our models to make them more affordable. Now, Llama 2 is over 7x cheaper and Mistral 7B is over 14x cheaper to run than we had initially <a href="https://developers.cloudflare.com/workers-ai/platform/pricing">published</a> on March 1. We want to continue to be the best platform for AI inference and will continue to roll out optimizations to our customers when we can.</p>
	<p>As a reminder, our billing for Workers AI started on April 1st for our non-beta models, while beta models remain free and unlimited. We offer 10,000 <a href="https://blog.cloudflare.com/workers-ai#:~:text=may%20be%20wondering%20%E2%80%94-,what%E2%80%99s%20a%20neuron">neurons</a> per day for free to all customers. Workers Free customers will encounter a hard rate limit after 10,000 neurons in 24 hours while Workers Paid customers will incur usage at $0.011 per 1000 additional neurons. &nbsp;Read our <a href="https://developers.cloudflare.com/workers-ai/platform/pricing">Workers AI Pricing Developer Docs</a> for the most up-to-date information on pricing.</p>
	<h4>New dashboard and playground</h4>
	<p>Lastly, we’ve revamped our <a href="https://dash.cloudflare.com/?to=%2F%3Aaccount%2Fai%2Fworkers-ai">Workers AI dashboard</a> and <a href="https://playground.ai.cloudflare.com">AI playground</a>. The Workers AI page in the Cloudflare dashboard now shows analytics for usage across models, including neuron calculations to help you better predict pricing. The AI playground lets you quickly test and compare different models and configure prompts and parameters. We hope these new tools help developers start building on Workers AI seamlessly – go try them out!</p>
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/3uSiHo9pV21DreFiLpUfPX/b7b27270b8aebdb428ee358b2387f693/image3-3.png" alt="" class="kg-image" width="1999" height="1233" loading="lazy">

	</figure>
	<h3>Run inference on GPUs in over 150 cities around the world</h3>
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/qyZ48xVu70u80swQKPUau/d103db6f1eb1c013797b59652ef4e745/image5-2.png" alt="" class="kg-image" width="1999" height="500" loading="lazy">

	</figure>
	<p>When we announced Workers AI back in September 2023, we set out to deploy GPUs to our data centers around the world. We plan to deliver on that promise and deploy inference-tuned GPUs almost everywhere by the end of 2024, making us the most widely distributed cloud-AI inference platform. We have over 150 cities with GPUs today and will continue to roll out more throughout the year.</p>
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/451IgQCfz2j10xM8kXhZy0/71cae8bfa706bded21addc7661089000/image7-1.png" alt="" class="kg-image" width="1800" height="1098" loading="lazy">

	</figure>
	<p>We also have our next generation of compute servers with GPUs launching in Q2 2024, which means better performance, power efficiency, and improved reliability over previous generations. We provided a preview of our Gen 12 Compute servers design in a <a href="https://blog.cloudflare.com/cloudflare-gen-12-server-bigger-better-cooler-in-a-2u1n-form-factor">December 2023 blog post</a>, with more details to come. With Gen 12 and future planned hardware launches, the next step is to support larger machine learning models and offer fine-tuning on our platform. This will allow us to achieve higher inference throughput, lower latency and greater availability for production workloads, as well as expanding support to new categories of workloads such as fine-tuning.</p>
	<h3>Hugging Face Partnership</h3>
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/7ATIA1mxaeqHYE31cztdjg/57bd84ce696acf416306f3609eadcad2/image2-2.png" alt="" class="kg-image" width="1999" height="551" loading="lazy">

	</figure>
	<p>We’re also excited to continue our partnership with Hugging Face in the spirit of bringing the best of open-source to our customers. Now, you can visit some of the most popular models on Hugging Face and easily click to run the model on Workers AI if it is available on our platform.</p>
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/3geQJnlHZhMktG1eoEWYKE/3fc5ece16caa2c5410e04ca1c83df9fe/image6-1.png" alt="" class="kg-image" width="1999" height="726" loading="lazy">

	</figure>
	<p>We’re happy to announce that we’ve added 4 more models to our platform in conjunction with Hugging Face. You can now access the new <a href="https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2">Mistral 7B v0.2</a> model with improved context windows, <a href="https://huggingface.co/NousResearch/Hermes-2-Pro-Mistral-7B">Nous Research’s Hermes 2 Pro</a> fine-tuned version of Mistral 7B, <a href="https://huggingface.co/google/gemma-7b-it">Google’s Gemma 7B</a>, and <a href="https://huggingface.co/Nexusflow/Starling-LM-7B-beta">Starling-LM-7B-beta</a> fine-tuned from OpenChat. There are currently 14 models that we’ve curated with Hugging Face to be available for serverless GPU inference powered by Cloudflare’s Workers AI platform, with more coming soon. These models are all served using Hugging Face’s technology with a <a href="https://github.com/huggingface/text-generation-inference">TGI</a> backend, and we work closely with the Hugging Face team to curate, optimize, and deploy these models.</p>
	<blockquote>
		<p><i>“We are excited to work with Cloudflare to make AI more accessible to developers. Offering the most popular open models with a serverless API, powered by a global fleet of GPUs is an amazing proposition for the Hugging Face community, and I can’t wait to see what they build with it.”</i>- <b>Julien Chaumond</b>, Co-founder and CTO, Hugging Face</p>
	</blockquote>
	<p>You can find all of the open models supported in Workers AI in this <a href="https://huggingface.co/collections/Cloudflare/hf-curated-models-available-on-workers-ai-66036e7ad5064318b3e45db6">Hugging Face Collection</a>, and the “Deploy to Cloudflare Workers AI” button is at the top of each model card. To learn more, read Hugging Face’s <a href="http://huggingface.co/blog/cloudflare-workers-ai">blog post</a> and take a look at our <a href="https://developers.cloudflare.com/workers-ai/models">Developer Docs</a> to get started. Have a model you want to see on Workers AI? Send us a message on <a href="https://discord.cloudflare.com">Discord</a> with your request.</p>
	<h3>Supporting fine-tuned inference - BYO LoRAs</h3>
	<p>Fine-tuned inference is one of our most requested features for Workers AI, and we’re one step closer now with Bring Your Own (BYO) LoRAs. Using the popular <a href="https://www.cloudflare.com/learning/ai/what-is-lora">Low-Rank Adaptation</a> method, researchers have figured out how to take a model and adapt <i>some</i> model parameters to the task at hand, rather than rewriting <i>all</i> model parameters like you would for a fully fine-tuned model. This means that you can get fine-tuned model outputs without the computational expense of fully fine-tuning a model.</p>
	<p>We now support bringing trained LoRAs to Workers AI, where we apply the LoRA adapter to a base model at runtime to give you fine-tuned inference, at a fraction of the cost, size, and speed of a fully fine-tuned model. In the future, we want to be able to support fine-tuning jobs and fully fine-tuned models directly on our platform, but we’re excited to be one step closer today with LoRAs.</p>
	<pre class="language-js"><code class="language-js">const response = await ai.run(
  "@cf/mistralai/mistral-7b-instruct-v0.2-lora", //the model supporting LoRAs
  {
      messages: [{"role": "user", "content": "Hello world"],
      raw: true, //skip applying the default chat template
      lora: "00000000-0000-0000-0000-000000000", //the finetune id OR name 
  }
);</code></pre>
	<p>BYO LoRAs is in open beta as of today for Gemma 2B and 7B, Llama 2 7B and Mistral 7B models with LoRA adapters up to 100MB in size and max rank of 8, and up to 30 total LoRAs per account. As always, we expect you to use Workers AI and our new BYO LoRA feature with our <a href="https://www.cloudflare.com/service-specific-terms-developer-platform/#developer-platform-terms">Terms of Service</a> in mind, including any model-specific restrictions on use contained in the models’ license terms.</p>
	<p>Read the technical deep dive blog post on <a href="https://blog.cloudflare.com/fine-tuned-inference-with-loras">fine-tuning with LoRA</a> and <a href="https://developers.cloudflare.com/workers-ai/fine-tunes">developer docs</a> to get started.</p>
	<h3>Write Workers in Python</h3>
	<p>Python is the second most popular programming language in the world (after JavaScript) and the language of choice for building AI applications. And starting today, in open beta, you can now <a href="https://ggu-python.cloudflare-docs-7ou.pages.dev/workers/languages/python">write Cloudflare Workers in Python</a>. Python Workers support all <a href="https://developers.cloudflare.com/workers/configuration/bindings">bindings</a> to resources on Cloudflare, including <a href="https://developers.cloudflare.com/vectorize">Vectorize</a>, <a href="https://developers.cloudflare.com/d1">D1</a>, <a href="https://developers.cloudflare.com/kv">KV</a>, <a href="https://developers.cloudflare.com/r2">R2</a> and more.</p>
	<p><a href="https://ggu-python.cloudflare-docs-7ou.pages.dev/workers/languages/python/packages/langchain">LangChain</a> is the most popular framework for building LLM‑powered applications, and like how <a href="https://blog.cloudflare.com/langchain-and-cloudflare">Workers AI works with langchain-js</a>, the <a href="https://python.langchain.com/docs/get_started/introduction">Python LangChain library</a> works on Python Workers, as do <a href="https://ggu-python.cloudflare-docs-7ou.pages.dev/workers/languages/python/packages">other Python packages</a> like FastAPI.</p>
	<p>Workers written in Python are just as simple as Workers written in JavaScript:</p>
	<pre class="language-js"><code class="language-js">from js import Response

async def on_fetch(request, env):
    return Response.new("Hello world!")</code></pre>
	<p>…and are configured by simply pointing at a .py file in your <code>wrangler.toml</code>:</p>
	<pre class="language-js"><code class="language-js">name = "hello-world-python-worker"
main = "src/entry.py"
compatibility_date = "2024-03-18"
compatibility_flags = ["python_workers"]</code></pre>
	<p>There are no extra toolchain or precompilation steps needed. The <a href="https://pyodide.org/en/stable">Pyodide</a> Python execution environment is provided for you, directly by the Workers runtime, mirroring how Workers written in JavaScript already work.</p>
	<p>There’s lots more to dive into — take a look at the <a href="https://ggu-python.cloudflare-docs-7ou.pages.dev/workers/languages/python">docs</a>, and check out our <a href="https://blog.cloudflare.com/python-workers">companion blog post</a> for details about how Python Workers work behind the scenes.</p>
	<h2>AI Gateway now supports Anthropic, Azure, AWS Bedrock, Google Vertex, and Perplexity</h2>
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/6YaNMed9Aw4YjhbZk75SGI/f7c94cd952cbe1da1b780c5f83aec094/image4-2.png" alt="" class="kg-image" width="1999" height="653" loading="lazy">

	</figure>
	<p>Our <a href="https://blog.cloudflare.com/announcing-ai-gateway">AI Gateway</a> product helps developers better control and observe their AI applications, with analytics, caching, rate limiting, and more. We are continuing to add more providers to the product, including Anthropic, Google Vertex, and Perplexity, which we’re excited to announce today. We quietly rolled out Azure and Amazon Bedrock support in December 2023, which means that the most popular providers are now supported via AI Gateway, including Workers AI itself.</p>
	<p>Take a look at our <a href="https://developers.cloudflare.com/ai-gateway">Developer Docs</a> to get started with AI Gateway.</p>
	<h4>Coming soon: Persistent Logs</h4>
	<p>In Q2 of 2024, we will be adding persistent logs so that you can push your logs (including prompts and responses) to object storage, custom metadata so that you can tag requests with user IDs or other identifiers, and secrets management so that you can securely manage your application’s API keys.</p>
	<p>We want AI Gateway to be the control plane for your AI applications, allowing developers to dynamically evaluate and route requests to different models and providers. With our persistent logs feature, we want to enable developers to use their logged data to fine-tune models in one click, eventually running the fine-tune job and the fine-tuned model directly on our Workers AI platform. AI Gateway is just one product in our AI toolkit, but we’re excited about the workflows and use cases it can unlock for developers building on our platform, and we hope you’re excited about it too.</p>
	<h3>Vectorize metadata filtering and future GA of million vector indexes</h3>
	<p>Vectorize is another component of our toolkit for AI applications. In open beta since September 2023, Vectorize allows developers to persist embeddings (vectors), like those generated from Workers AI <a href="https://developers.cloudflare.com/workers-ai/models/#text-embeddings">text embedding</a> models, and query for the closest match to support use cases like similarity search or recommendations. Without a vector database, model output is forgotten and can’t be recalled without extra costs to re-run a model.</p>
	<p>Since Vectorize’s open beta, we’ve added <a href="https://developers.cloudflare.com/vectorize/reference/metadata-filtering">metadata filtering</a>. Metadata filtering lets developers combine vector search with filtering for arbitrary metadata, supporting the query complexity in AI applications. We’re laser-focused on getting Vectorize ready for general availability, with an target launch date of June 2024, which will include support for multi-million vector indexes.</p>
	<pre class="language-js"><code class="language-js">// Insert vectors with metadata
const vectors: Array&lt;VectorizeVector&gt; = [
  {
    id: "1",
    values: [32.4, 74.1, 3.2],
    metadata: { url: "/products/sku/13913913", streaming_platform: "netflix" }
  },
  {
    id: "2",
    values: [15.1, 19.2, 15.8],
    metadata: { url: "/products/sku/10148191", streaming_platform: "hbo" }
  },
...
];
let upserted = await env.YOUR_INDEX.upsert(vectors);

// Query with metadata filtering
let metadataMatches = await env.YOUR_INDEX.query(&lt;queryVector&gt;, { filter: { streaming_platform: "netflix" }} )</code></pre>
	<h3>The most comprehensive Developer Platform to build AI applications</h3>
	<p>On Cloudflare’s Developer Platform, we believe that all developers should be able to quickly build and ship full-stack applications &nbsp;– and that includes AI experiences as well. With our GA of Workers AI, announcements for Python support in Workers, AI Gateway, and Vectorize, and our partnership with Hugging Face, we’ve expanded the world of possibilities for what you can build with AI on our platform. We hope you are as excited as we are – take a look at all our <a href="https://developers.cloudflare.com">Developer Docs</a> to get started, and <a href="https://discord.cloudflare.com">let us know</a> what you build.</p>
</div>