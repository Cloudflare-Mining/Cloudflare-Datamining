<div class="mb2 gray5">8 min read</div><img class="mr2" src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/4vpFmGVqolJDm5PbgMWd2z/6f664899bc3f26019deee5805008612d/Feature_Image.png" alt="">
<div class="post-content lh-copy gray1">
	<p>Today, weâ€™re launching the open beta of Pipelines, our streaming ingestion product. Pipelines allows you to ingest high volumes of structured, real-time data, and load it into our <a href="https://www.cloudflare.com/developer-platform/products/r2"><u>object storage service, R2</u></a>. You donâ€™t have to manage any of the underlying infrastructure, worry about scaling shards or metadata services, and you pay for the data processed (and not by the hour). Anyone on a Workers paid plan can start using it to ingest and batch data â€” at tens of thousands of requests per second (RPS) â€” directly into R2.</p>
	<p>But this is just the tip of the iceberg: you often want to transform the data youâ€™re ingesting, hydrate it on-the-fly from other sources, and write it to an open table format (such as Apache Iceberg), so that you can efficiently query that data once youâ€™ve landed it in object storage.</p>
	<p>The good news is that weâ€™ve thought about that too, and weâ€™re excited to announce that weâ€™ve acquired <a href="https://www.arroyo.dev"><u>Arroyo</u></a>, a cloud-native, distributed stream processing engine, to make that happen.</p>
	<p>With Arroyo <i>and </i>our just announced <a href="https://blog.cloudflare.com/r2-data-catalog-public-beta">R2 Data Catalog</a>, weâ€™re getting increasingly serious about building a data platform that allows you to ingest data across the planet, store it at scale, and <i>run compute over it</i>.&nbsp;</p>
	<p>To get started, you can dive into the <a href="https://developers.cloudflare.com/pipelines"><u>Pipelines developer docs</u></a> or just run this <a href="https://developers.cloudflare.com/workers/wrangler"><u>Wrangler</u></a> command to create your first pipeline:</p>
	<pre class="language-Rust"><code class="language-Rust">$ npx wrangler@latest pipelines create my-clickstream-pipeline --r2-bucket my-bucket

...
âœ… Successfully created Pipeline my-clickstream-pipeline with ID 0e00c5ff09b34d018152af98d06f5a1xv</code></pre>
	<p>â€¦ and then write your first record(s):</p>
	<pre class="language-Rust"><code class="language-Rust">$ curl -d '[{"payload": [],"id":"abc-def"}]' 
"https://0e00c5ff09b34d018152af98d06f5a1xvc.pipelines.cloudflarestorage.com/"</code></pre>
	<p>However, the true power comes from the processing of data streams between ingestion and when theyâ€™re written to sinks like R2. Being able to write SQL that acts on windows of data <i>as itâ€™s being ingested</i>, that can transform &amp; aggregate it, and even extract insights from the data in real-time, turns out to be extremely powerful.</p>
	<p>This is where Arroyo comes in, and weâ€™re going to be bringing the best parts of Arroyo into Pipelines and deeply integrate it with Workers, R2, and the rest of our Developer Platform.</p>
	<div class="flex anchor relative">
		<h2 id="the-arroyo-origin-story">The Arroyo origin story&nbsp;</h2>
		<a href="https://blog.cloudflare.com/#the-arroyo-origin-story" aria-hidden="true" class="relative sm:absolute sm:-start-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p><i>(By Micah Wylde, founder of Arroyo)</i></p>
	<p>We started Arroyo in 2023 to bring real-time (<i>stream</i>) processing to everyone who works with data. Modern companies rely on data pipelines to power their applications and businesses â€” from user customization, recommendations, and anti-fraud, to the emerging world of AI agents.</p>
	<p>But today, most of these pipelines operate in batch, running once per hour, day, or even month. After spending many years working on stream processing at companies like Lyft and Splunk, it was no mystery why: it was just too hard for developers and data scientists to build correct, performant, and reliable pipelines. Large tech companies hire streaming experts to build and operate these systems, but everyone else is stuck waiting for batches to arrive.&nbsp;</p>
	<p>When we started, the dominant solution for streaming pipelines â€” and what we ran at Lyft and Splunk â€” was Apache Flink. Flink was the first system that successfully combined a fault-tolerant (able to recover consistently from failures), distributed (across multiple machines), stateful (and remember data about past events) dataflow with a graph-construction API. This combination of features meant that we could finally build powerful real-time data applications, with capabilities like windows, aggregations, and joins. But while Flink had the necessary power, in practice the API proved too hard and low-level for non-expert users, and the stateful nature of the resulting services required endless operations.</p>
	<p>We realized we would need to build a new streaming engine â€” one with the power of Flink, but designed for product engineers and data scientists and to run on modern cloud infrastructure. We started with SQL as our API because itâ€™s easy to use, widely known, and declarative. We built it in Rust for speed and operational simplicity (no JVM tuning required!). We constructed an object-storage-native state backend, simplifying the challenge of running stateful pipelines â€” which each are like a weird, specialized database. And then in the summer of 2023, we open-sourced it. Today, dozens of companies are running Arroyo pipelines with use cases including data ingestion, anti-fraud, IoT observability, and financial trading.&nbsp;</p>
	<p>But we always knew that the engine was just one piece of the puzzle. To make streaming as easy as batch, users need to be able to develop and test query logic, backfill on historical data, and deploy serverlessly without having to worry about cluster sizing or ongoing operations. Democratizing streaming ultimately meant building a complete data platform. And when we started talking with Cloudflare, we realized they already had all of the pieces in place: R2 provides object storage for state and data at rest, Cloudflare <a href="https://developers.cloudflare.com/queues"><u>Queues</u></a> for data in transit, and Workers to safely and efficiently run user code. And Cloudflare, uniquely, allows us to push these systems all the way to the edge, enabling a new paradigm of local stream processing that will be key for a future of data sovereignty and AI.</p>
	<p>Thatâ€™s why weâ€™re incredibly excited to join with the Cloudflare team to make this vision a reality.</p>
	<div class="flex anchor relative">
		<h2 id="ingestion-at-scale">Ingestion at scale</h2>
		<a href="https://blog.cloudflare.com/#ingestion-at-scale" aria-hidden="true" class="relative sm:absolute sm:-start-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>While transformations and a streaming SQL API are on the way for Pipelines, it already solves two critical parts of the data journey: globally distributed, high-throughput ingestion and efficient loading into object storage.&nbsp;</p>
	<p>Creating a pipeline is as simple as running one command:&nbsp;</p>
	<pre class="language-Rust"><code class="language-Rust">$ npx wrangler@latest pipelines create my-clickstream-pipeline --r2-bucket my-bucket

ðŸŒ€ Creating pipeline named "my-clickstream-pipeline"
âœ… Successfully created pipeline my-clickstream-pipeline with ID 
0e00c5ff09b34d018152af98d06f5a1xvc

Id:    0e00c5ff09b34d018152af98d06f5a1xvc
Name:  my-clickstream-pipeline
Sources:
  HTTP:
    Endpoint:        https://0e00c5ff09b34d018152af98d06f5a1xvc.pipelines.cloudflare.com/
    Authentication:  off
    Format:          JSON
  Worker:
    Format:  JSON
Destination:
  Type:         R2
  Bucket:       my-bucket
  Format:       newline-delimited JSON
  Compression:  GZIP
Batch hints:
  Max bytes:     100 MB
  Max duration:  300 seconds
  Max records:   100,000

ðŸŽ‰ You can now send data to your pipeline!

Send data to your pipeline's HTTP endpoint:
curl "https://0e00c5ff09b34d018152af98d06f5a1xvc.pipelines.cloudflare.com/" -d '[{ ...JSON_DATA... }]'</code></pre>
	<p>By default, a pipeline can ingest data from two sources â€“ Workers and an HTTP endpoint â€“ and load batched events into an R2 bucket. This gives you an out-of-the-box solution for streaming raw event data into object storage. If the defaults donâ€™t work, you can configure pipelines during creation or anytime after. Options include: adding authentication to the HTTP endpoint, configuring CORS to allow browsers to make cross-origin requests, and specifying output file compression and batch settings.</p>
	<p>Weâ€™ve built Pipelines for high ingestion volumes from day 1. Each pipeline can scale to ~100,000 records per second (and weâ€™re just getting started here). Once records are written to a Pipeline, they are then durably stored, batched, and written out as files in an R2 bucket. Batching is critical here: if youâ€™re going to act on and query that data, you donâ€™t want your query engine querying millions (or tens of millions) of tiny files. Itâ€™s slow (per-file &amp; request overheads), inefficient (more files to read), and costly (more operations). Instead, you want to find the right balance between batch size for your query engine and latency (not waiting too long for a batch): Pipelines allows you to configure this.</p>
	<p>To further optimize queries, output files are partitioned by date and time, using the standard Hive partitioning scheme. This can optimize queries even further, because your query engine can just skip data that is irrelevant to the query youâ€™re running. The output in your R2 bucket might look like this:</p>
	<figure class="kg-card kg-image-card">
		<img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/7q63u2kRoYBAZJtgfcF874/2a7341e1cba6e371e0eed311e89fec6a/image1.png" alt="" class="kg-image" width="1999" height="650" loading="lazy">
	</figure>
	<p><sup><i>Hive-partioned files from Pipelines in an R2 bucket</i></sup></p>
	<p>Output files are stored as new-line delimited JSON (NDJSON) â€” which makes it easy to materialize a stream from these files (hint: in the future youâ€™ll be able to use R2 as a pipeline source too). Finally, the file names are <a href="https://github.com/ulid/spec"><u>ULIDs</u></a> - so theyâ€™re sorted by time by default.</p>
	<div class="flex anchor relative">
		<h2 id="first-you-shard-then-you-shard-some-more">First you shard, then you shard some more</h2>
		<a href="https://blog.cloudflare.com/#first-you-shard-then-you-shard-some-more" aria-hidden="true" class="relative sm:absolute sm:-start-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>What makes Pipelines so horizontally scalable <i>and</i> able to acknowledge writes quickly is how we built it: we use Durable Objects and the <a href="https://blog.cloudflare.com/sqlite-in-durable-objects"><u>embedded, zero-latency SQLite</u></a> storage within each Durable Object to immediately persist data as itâ€™s written, before then processing it and writing it to R2.</p>
	<p>For example: imagine youâ€™re an e-commerce or SaaS site and need to ingest website usage data (known as <i>clickstream data</i>), and make it available to your data science team to query. The infrastructure which handles this workload has to be resilient to several failure scenarios. The ingestion service needs to maintain high availability in the face of bursts in traffic. Once ingested, the data needs to be buffered, to minimize downstream invocations and thus downstream cost. Finally, the buffered data needs to be delivered to a sink, with appropriate retry &amp; failure handling if the sink is unavailable. Each step of this process needs to signal backpressure upstream when overloaded. It also needs to scale: up during major sales or events, and down during the quieter periods of the day.</p>
	<p>Data engineers reading this post might be familiar with the status quo of using Kafka and the associated ecosystem to handle this. But if youâ€™re an application engineer: you use Pipelines to build an ingestion service <i>without </i>learning about Kafka, Zookeeper, and Kafka streams.</p>
	<figure class="kg-card kg-image-card">
		<img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/eRIUocbyvY2oHwEK34pzE/e2ef72b2858c02e890446cfd34accb45/image3.png" alt="" class="kg-image" width="1999" height="860" loading="lazy">
	</figure>
	<p><sup><i>Pipelines horizontal sharding</i></sup></p>
	<p>The diagram above shows how Pipelines splits the control plane, which is responsible for accounting, tracking shards, and Pipelines lifecycle events, and the data path, which is a scalable group of Durable Objects shards.</p>
	<p>When a record (or batch of records) is written to Pipelines:</p>
	<ol>
		<li>
			<p>The Pipelines Worker receives the records either through the fetch handler or worker binding.</p>
		</li>
		<li>
			<p>Contacts the Coordinator, based upon the <code>pipeline_id</code> to get the execution plan: subsequent reads are cached to reduce pressure on the coordinator.</p>
		</li>
		<li>
			<p>Executes the plan, which first shards to a set of Executors, while are primarily serving to scale read request handling</p>
		</li>
		<li>
			<p>These then re-shard to another set of executors that are actually handling the writes, beginning with persisting to Durable Object storage, which will be replicated for durability and availability by the <a href="https://blog.cloudflare.com/sqlite-in-durable-objects/#under-the-hood-storage-relay-service"><u>Storage Relay Service</u></a> (SRS).&nbsp;</p>
		</li>
		<li>
			<p>After SRS, we pass to any configured Transform Workers to customize the data.</p>
		</li>
		<li>
			<p>The data is batched, written to output files, and compressed (if applicable).</p>
		</li>
		<li>
			<p>The files are compressed, data is packaged into the final batches, and written to the configured R2 bucket.</p>
		</li>
	</ol>
	<p>Each step of this pipeline can signal backpressure upstream. We do this by leveraging <a href="https://developer.mozilla.org/en-US/docs/Web/API/ReadableStream"><u>ReadableStreams</u></a> and responding with <a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Reference/Status/429"><u>429s</u></a> when the total number of bytes awaiting write exceeds a threshold. Each ReadableStream is able to cross Durable Object boundaries by using <a href="https://developers.cloudflare.com/workers/runtime-apis/rpc"><u>JSRPC</u></a> calls between Durable Objects. To improve performance, we use RPC stubs for connection reuse between Durable Objects. Each step is also able to retry operations, to handle any temporary unavailability in the Durable Objects or R2.</p>
	<p>We also guarantee delivery even while updating an existing pipeline. When you update an existing pipeline, we create a new deployment, including all the shards and Durable Objects described above. Requests are gracefully re-routed to the new pipeline. The old pipeline continues to write data into R2, until all the Durable Object storage is drained. We spin down the old pipeline only after all the data has been written out. This way, you wonâ€™t lose data even while updating a pipeline.</p>
	<p>Youâ€™ll notice thereâ€™s one interesting part in here â€” the Transform Workers â€” which we havenâ€™t yet exposed. As we work to integrate Arroyoâ€™s streaming engine with Pipelines, this will be a key part of how we hand over data for Arroyo to process.</p>
	<div class="flex anchor relative">
		<h2 id="so-whats-it-cost">So, whatâ€™s it cost?</h2>
		<a href="https://blog.cloudflare.com/#so-whats-it-cost" aria-hidden="true" class="relative sm:absolute sm:-start-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>During the first phase of the open beta, there will be no additional charges beyond standard R2 storage and operation costs incurred when loading and accessing data. And as always, egress directly from R2 buckets is free, so you can process and query your data from any cloud or region without worrying about data transfer costs adding up.</p>
	<p>In the future, we plan to introduce pricing based on volume of data ingested into Pipelines and delivered from Pipelines:</p>
	<table>
		<tbody>
			<tr>
				<td>
					<p>
					</p>
				</td>
				<td>
					<p><b>Workers Paid ($5 / month)</b></p>
				</td>
			</tr>
			<tr>
				<td>
					<p><b>Ingestion</b></p>
				</td>
				<td>
					<p>First 50 GB per month included</p>
					<p>\$0.02 per additional GB</p>
				</td>
			</tr>
			<tr>
				<td>
					<p><b>Delivery to R2</b></p>
				</td>
				<td>
					<p>First 50 GB per month included</p>
					<p>\$0.02 per additional GB</p>
				</td>
			</tr>
		</tbody>
	</table>
	<p>Weâ€™re also planning to make Pipelines available on the Workers Free plan as the beta progresses.</p>
	<p>Weâ€™ll be sharing more as we bring transformations and additional sinks to Pipelines. Weâ€™ll provide at least 30 days notice before we make any changes or start charging for usage, which we expect to do by September 15, 2025.</p>
	<div class="flex anchor relative">
		<h2 id="whats-next">Whatâ€™s next?</h2>
		<a href="https://blog.cloudflare.com/#whats-next" aria-hidden="true" class="relative sm:absolute sm:-start-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>Thereâ€™s a lot to build here, and weâ€™re keen to build on a lot of the powerful components that Arroyo has built: integrating Workers as UDFs (User-Defined Functions), adding new sources like Kafka clients, and extending Pipelines with new sinks (beyond R2).</p>
	<p>Weâ€™ll also be integrating Pipelines with our just-launched <a href="https://blog.cloudflare.com/r2-data-catalog-public-beta">R2 Data Catalog</a>: enabling you ingest streams of data directly into Iceberg tables and immediately query them, without needing to rely on other systems.</p>
	<p>In the meantime, you can:</p>
	<ul>
		<li>
			<p>Get started and <a href="https://developers.cloudflare.com/pipelines/getting-started"><u>create your first Pipeline</u></a></p>
		</li>
		<li>
			<p><a href="https://developers.cloudflare.com/pipelines"><u>Read the docs</u></a></p>
		</li>
		<li>
			<p>Join the <code>#pipelines-beta</code> channel on <a href="https://discord.cloudflare.com"><u>our Developer Discord</u></a></p>
		</li>
	</ul>
	<p>â€¦ or deploy the example project directly:&nbsp;</p>
	<pre class="language-Rust"><code class="language-Rust">$ npm create cloudflare@latest -- pipelines-starter 
--template="cloudflare/pipelines-starter"</code></pre>
	<p></p>
</div>