<div class="mb2 gray5">3 min read</div><img class="mr2" src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/22EHOfwq4HTxAYc28d7SFV/974b5d12c4b6745f06815f001739b1b1/logs-from-the-edge.jpg" alt="">
<div class="post-content lh-copy gray1">
	<p>With <a href="https://blog.cloudflare.com/introducing-cloudflare-workers">Cloudflare Workers</a>, our JavaScript environment at the edge, it is possible to send traffic logs to arbitrary locations. In this post we are going to discuss an example Worker implementation on how to achieve this. So if you are building or maintaining your own traffic logging/analytics environment, read on.</p>
	<p>To build the underlying script we are going to leverage sub requests. Sub requests, which can be spawned from the initial HTTP/S request, can be used to aggregate and compose a response from several back end services, or, like in the example discussed here, to post data to a specific endpoint. Sub requests can be made asynchronously and after the initial request has been fully served to avoid adding unnecessary latency to the main request.</p>
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/5Ky2AOpjSfW9fZY5cLt9iy/6e71e0b52a3a3e42188f2cc18493dccf/logs.jpg" alt="logs" class="kg-image" width="3072" height="2048" loading="lazy">

	</figure>
	<div class="flex anchor relative">
		<h3 id="the-worker-code">The Worker Code</h3>
		<a href="https://blog.cloudflare.com/#the-worker-code" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>In this example we assume an <a href="https://www.elastic.co/products">Elastic stack</a> has been set up at <code>elk.example.com</code> and has been configured to receive via HTTP/S <code>PUT</code> requests a number of fields for each log line. The full script that we are going to look at can be found below:</p>
	<pre class="language-javascript"><code class="language-javascript">addEventListener('fetch', event =&gt; {
  event.respondWith(fetchAndLog(event));
})

async function fetchAndLog(event) {
  const response = await fetch(event.request);
  event.waitUntil(logToElk(event.request, response));
  return response;
}

async function logToElk(request, response) {
  var ray  = request.headers.get('cf-ray') || '';
  var id   = ray.slice(0, -4);
  var data = {
    'timestamp':  Date.now(),
    'url':        request.url,
    'referer':    request.referrer,
    'method':     request.method,
    'ray':        ray,
    'ip':         request.headers.get('cf-connecting-ip') || '',
    'host':       request.headers.get('host') || '',
    'ua':         request.headers.get('user-agent') || '',
    'cc':         request.headers.get('Cf-Ipcountry') || '',
    'colo':       request.cf.colo,
    'tlsVersion': request.cf.tlsVersion || '',
    'tlsCipher':  request.cf.tlsCipher || '',
    'status':     response.status,
  };
  
  var url = "https://elk.example.com/weblogs/logs/" + id + "?pipeline=weblogs&amp;pretty"
  await fetch(url, {
    method: 'PUT',
    body: JSON.stringify(data),
    headers: new Headers({
      'Content-Type': 'application/json',
    })
  })
}</code></pre>
	<p>Let's look at the script in a little more detail:</p>
	<pre class="language-javascript"><code class="language-javascript">addEventListener('fetch', event =&gt; {
  event.respondWith(fetchAndLog(event));
})

async function fetchAndLog(event) {
  const response = await fetch(event.request);
  event.waitUntil(logToElk(event.request, response));
  return response;
}</code></pre>
	<p>At the start of the script we are listening to all request events, and on each request, we are calling the <code>fetchAndLog</code> function. This function will proxy the request as is and asynchronously call the <code>logToElk</code> function that will post a log line to our ELK stack. As this function is executed asynchronously, delays while logging to ELK will not affect the original request.</p>
	<p>Let's take a deeper look at the <code>logToElk</code> function.</p>
	<pre class="language-javascript"><code class="language-javascript">var ray  = request.headers.get('cf-ray') || '';
var id   = ray.slice(0, -4);
var data = {
  'timestamp':  Date.now(),
  'url':        request.url,
  'referer':    request.referrer,
  'method':     request.method,
  'ray':        ray,
  'ip':         request.headers.get('cf-connecting-ip') || '',
  'host':       request.headers.get('host') || '',
  'ua':         request.headers.get('user-agent') || '',
  'cc':         request.headers.get('Cf-Ipcountry') || '',
  'colo':       request.cf.colo,
  'tlsVersion': request.cf.tlsVersion || '',
  'tlsCipher':  request.cf.tlsCipher || '',
  'status':     response.status,
};</code></pre>
	<p>The first part is collecting the data we wish to log. Some of these fields are part of the standard fields of any HTTP request (e.g. the URL, the HTTP method etc.), however, we are also adding fields specific to Cloudflare such as the ray ID (a unique identifier of any HTTP request proxying via Cloudflare), the country code as provided by Cloudflare IP to country logic, and the PoP/colo ID that the request is hitting.</p>
	<pre class="language-javascript"><code class="language-javascript">var url = "https://elk.example.com/weblogs/logs/" + id + " pipeline=weblogs&amp;pretty"
await fetch(url, {
  method: 'PUT',
  body: JSON.stringify(data),
  headers: new Headers({
    'Content-Type': 'application/json',
  })
})</code></pre>
	<p>Once we have all the fields we wish to log saved in our <code>data</code> variable, we need to perform a sub request to <code>PUT</code> the log line to our backend ELK stack. By calling the <code>fetch</code> function we are initiating a new HTTP request for which we are specifying the request method, the body and additional headers as expected by the ELK stack (in this case we are <code>PUT</code>ing JSON content).</p>
	<p>And that is it, with a small Worker script you can import Cloudflare traffic logs into your ELK stack in real time. If you found this useful we also talked about <a href="https://blog.cloudflare.com/dogfooding-edge-workers">logging events and alerts to Sentry via workers</a> in the past, and community members have also shared <a href="https://community.cloudflare.com/t/simple-log-collector-worker/40954">similar methods for other logging tools such as logdna</a>.</p>
	<h4>Making it better with Argo Tunnel and Access</h4>
	<p>As an additional improvement it is worth noting that an Elastic Stack uses <a href="https://www.elastic.co/webinars/getting-started-kibana">Kibana</a> as a front end visualisation interface available over HTTP/S. The Kibana endpoint (let's assume <code>kibana.example.com</code>) can also be proxied via Cloudflare, but it is normally used only internally within an organization. The origin therefore needs to be protected and made accessible only to colleagues.</p>
	<p>We can use two Cloudflare features to improve the Kibana deployment:</p>
	<ul>
		<li>
			<p><a href="https://www.cloudflare.com/products/argo-tunnel">Argo Tunnel</a> allows the Kibana origin to reach out directly to Cloudflare, avoiding the need of a publicly accessible IP address or hostname;</p>
		</li>
		<li>
			<p><a href="https://www.cloudflare.com/products/cloudflare-access">Cloudflare Access</a> allows you to integrate Cloudflare with your Identity and Access Management tool, and define rules that specify which users or groups have access to the Kibana instance;</p>
		</li>
	</ul>
	<div class="flex anchor relative">
		<h3 id="logs-for-everyone">Logs for Everyone</h3>
		<a href="https://blog.cloudflare.com/#logs-for-everyone" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>Before we launched Workers, retrieving raw logs from Cloudflare was only available to enterprise customers. With a little technical effort you can now start receiving logs from the edge by leveraging Workers regardless of which plan you are using. Workers are available starting on a pay per usage model and the ELK stack is open source.</p>
	<p>Enterprise customers can still retrieve raw traffic logs with our Enterprise Log Share (ELS) feature. If ELS is turned on, all requests for the chosen application will be logged and stored on Cloudflare infrastructure. The logs can be downloaded when required via a <a href="https://support.cloudflare.com/hc/en-us/articles/216672448-Enterprise-Log-Share-Logpull-REST-API">RESTful API</a> or pushed directly to Amazon S3 or Google Cloud (soon also Microsoft Azure) for further processing in your favorite log analysis tool. Logs with ELS are available within 10 minutes of the request being processed at the edge regardless of location, and they will have a number of additional fields that are not yet available to the Worker environment. ELS also guarantees log delivery and retention periods and allows you not to worry about load and storage.</p>
</div>