<div class="post-content lh-copy gray1">
	<!--kg-card-begin: markdown-->
	<p><em><small>This post is also available in <a href="https://blog.cloudflare.com/zh-cn/when-the-window-is-not-fully-open-your-tcp-stack-is-doing-more-than-you-think-zh-cn/">简体中文</a> and <a href="https://blog.cloudflare.com/zh-tw/when-the-window-is-not-fully-open-your-tcp-stack-is-doing-more-than-you-think-zh-tw/">繁體中文</a>.</small></em></p>
	<!--kg-card-end: markdown-->
	<p>Over the years I've been lurking around the Linux kernel and have investigated the TCP code many times. But when recently we were working on <a href="https://blog.cloudflare.com/optimizing-tcp-for-high-throughput-and-low-latency/">Optimizing TCP for high WAN throughput while preserving low latency</a>, I realized I have gaps in my knowledge about how Linux manages TCP receive buffers and windows. As I dug deeper I found the subject complex and certainly non-obvious.</p>
	<p>In this blog post I'll share my journey deep into the Linux networking stack, trying to understand the memory and window management of the receiving side of a TCP connection. Specifically, looking for answers to seemingly trivial questions:</p>
	<ul>
		<li>How much data can be stored in the TCP receive buffer? (it's not what you think)</li>
		<li>How fast can it be filled? (it's not what you think either!)</li>
	</ul>
	<p>Our exploration focuses on the receiving side of the TCP connection. We'll try to understand how to tune it for the best speed, without wasting precious memory.</p>
	<h3 id="a-case-of-a-rapid-upload">A case of a rapid upload</h3>
	<p>To best illustrate the receive side buffer management we need pretty charts! But to grasp all the numbers, we need a bit of theory.</p>
	<p>We'll draw charts from a receive side of a TCP flow, running a pretty straightforward scenario:</p>
	<ul>
		<li>The client opens a TCP connection.</li>
		<li>The client does <code>send()</code>, and pushes as much data as possible.</li>
		<li>The server doesn't <code>recv()</code> any data. We expect all the data to stay and wait in the receive queue.</li>
		<li>We fix the SO_RCVBUF for better illustration.</li>
	</ul>
	<p>Simplified pseudocode might look like (<a href="https://github.com/cloudflare/cloudflare-blog/blob/master/2022-07-rmem-a/window.py" target="_blank">full code if you dare</a>):</p>
	<!--kg-card-begin: markdown-->
	<pre><code>sd = socket.socket(AF_INET, SOCK_STREAM, 0)
sd.bind(('127.0.0.3', 1234))
sd.listen(32)

cd = socket.socket(AF_INET, SOCK_STREAM, 0)
cd.setsockopt(SOL_SOCKET, SO_RCVBUF, 32*1024)
cd.connect(('127.0.0.3', 1234))

ssd, _ = sd.accept()

while true:
    cd.send(b'a'*128*1024)
</code></pre>
	<!--kg-card-end: markdown-->
	<p>We're interested in basic questions:</p>
	<ul>
		<li>How much data can fit in the server’s receive buffer? It turns out it's not exactly the same as the default read buffer size on Linux; we'll get there.</li>
		<li>Assuming infinite bandwidth, what is the minimal time &nbsp;- measured in RTT - for the client to fill the receive buffer?</li>
	</ul>
	<h3 id="a-bit-of-theory">A bit of theory</h3>
	<p>Let's start by establishing some common nomenclature. I'll follow the wording used by the <a href="https://man7.org/linux/man-pages/man8/ss.8.html" target="_blank"><code>ss</code> Linux tool from the <code>iproute2</code> package</a>.</p>
	<p>First, there is the buffer budget limit. <a href="https://man7.org/linux/man-pages/man8/ss.8.html" target="_blank"><code>ss</code> manpage</a> calls it <strong>skmem_rb</strong>, in the kernel it's named <strong>sk_rcvbuf</strong>. This value is most often controlled by the Linux autotune mechanism using the <code>net.ipv4.tcp_rmem</code> setting:</p>
	<!--kg-card-begin: markdown-->
	<pre><code>$ sysctl net.ipv4.tcp_rmem
net.ipv4.tcp_rmem = 4096 131072 6291456
</code></pre>
	<!--kg-card-end: markdown-->
	<p>Alternatively it can be manually set with <code>setsockopt(SO_RCVBUF)</code> on a socket. Note that the kernel doubles the value given to this setsockopt. For example SO_RCVBUF=16384 will result in skmem_rb=32768. The max value allowed to this setsockopt is limited to meager 208KiB by default:</p>
	<!--kg-card-begin: markdown-->
	<pre><code>$ sysctl net.core.rmem_max net.core.wmem_max
net.core.rmem_max = 212992
net.core.wmem_max = 212992
</code></pre>
	<!--kg-card-end: markdown-->
	<p><a href="https://blog.cloudflare.com/optimizing-tcp-for-high-throughput-and-low-latency/">The aforementioned blog post</a> discusses why manual buffer size management is problematic - relying on autotuning is generally preferable.</p>
	<p>Here’s a diagram showing how <strong>skmem_rb</strong> budget is being divided:</p>
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2022/07/image2-17.png" class="kg-image"></figure>
	<p>In any given moment, we can think of the budget as being divided into four parts:</p>
	<ul>
		<li><strong>Recv-q</strong>: part of the buffer budget occupied by actual application bytes awaiting <code>read()</code>.</li>
		<li>Another part of is consumed by metadata handling - the cost of <strong>struct sk_buff </strong>and such.</li>
		<li>Those two parts together are reported by <code>ss</code> as <strong>skmem_r</strong> - kernel name is <strong>sk_rmem_alloc</strong>.</li>
		<li>What remains is "free", that is: it's not actively used yet.</li>
		<li>However, a portion of this "free" region is an advertised window - it may become occupied with application data soon.</li>
		<li>The remainder will be used for future metadata handling, or might be divided into the advertised window further in the future.</li>
	</ul>
	<p>The upper limit for the window is configured by <code>tcp_adv_win_scale</code> setting. By default, the window is set to at most 50% of the "free" space. The value can be clamped further by the TCP_WINDOW_CLAMP option or an internal <code>rcv_ssthresh</code> variable.</p>
	<h3 id="how-much-data-can-a-server-receive">How much data can a server receive?</h3>
	<p>Our first question was "How much data can a server receive?". A naive reader might think it's simple: if the server has a receive buffer set to say 64KiB, then the client will surely be able to deliver 64KiB of data!</p>
	<p>But this is totally not how it works. To illustrate this, allow me to temporarily set sysctl <code>tcp_adv_win_scale=0</code>. This is not a default and, as we'll learn, it's the wrong thing to do. With this setting the server will indeed set 100% of the receive buffer as an advertised window.</p>
	<p>Here's our setup:</p>
	<ul>
		<li>The client tries to send as fast as possible.</li>
		<li>Since we are interested in the receiving side, we can cheat a bit and speed up the sender arbitrarily. The client has transmission congestion control disabled: we set initcwnd=10000 as the route option.</li>
		<li>The server has a fixed <strong>skmem_rb</strong> set at 64KiB.</li>
		<li>The server has <strong><code>tcp_adv_win_scale=0</code></strong>.</li>
	</ul>
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2022/07/image6-10.png" class="kg-image"></figure>
	<p>There are so many things here! Let's try to digest it. First, the X axis is an ingress packet number (we saw about 65). The Y axis shows the buffer sizes as seen on the receive path for every packet.</p>
	<ul>
		<li>First, the purple line is a buffer size limit in bytes - <strong>skmem_rb</strong>. In our experiment we called <code>setsockopt(SO_RCVBUF)=32K</code> and skmem_rb is double that value. Notice, by calling SO_RCVBUF we disabled the Linux autotune mechanism.</li>
		<li>Green <strong>recv-q</strong> line is how many application bytes are available in the receive socket. This grows linearly with each received packet.</li>
		<li>Then there is the blue <strong>skmem_r</strong>, the used data + metadata cost in the receive socket. It grows just like <strong>recv-q</strong> but a bit faster, since it accounts for the cost of the metadata kernel needs to deal with.</li>
		<li>The orange <strong>rcv_win</strong> is an advertised window. We start with 64KiB (100% of skmem_rb) and go down as the data arrives.</li>
		<li>Finally, the dotted line shows <strong>rcv_ssthresh</strong>, which is not important yet, we'll get there.</li>
	</ul>
	<h3 id="running-over-the-budget-is-bad">Running over the budget is bad</h3>
	<p>It's super important to notice that we finished with <strong>skmem_r </strong>higher than <strong>skmem_rb</strong>! This is rather unexpected, and undesired. The whole point of the <strong>skmem_rb</strong> memory budget is, well, not to exceed it. Here's how <code>ss</code> shows it:</p>
	<!--kg-card-begin: markdown-->
	<pre><code>$ ss -m
Netid  State  Recv-Q  Send-Q  Local Address:Port  Peer Address:Port   
tcp    ESTAB  62464   0       127.0.0.3:1234      127.0.0.2:1235
     skmem:(r73984,rb65536,...)
</code></pre>
	<!--kg-card-end: markdown-->
	<p>As you can see, skmem_rb is 65536 and skmem_r is 73984, which is 8448 bytes over! When this happens we have an even bigger issue on our hands. At around the 62nd packet we have an advertised window of 3072 bytes, but while packets are being sent, the receiver is unable to process them! This is easily verifiable by inspecting an nstat TcpExtTCPRcvQDrop counter:</p>
	<!--kg-card-begin: markdown-->
	<pre><code>$ nstat -az TcpExtTCPRcvQDrop
TcpExtTCPRcvQDrop    13    0.0
</code></pre>
	<!--kg-card-end: markdown-->
	<p>In our run 13 packets were dropped. This variable counts a number of packets dropped due to either system-wide or per-socket memory pressure - we know we hit the latter. In our case, soon after the socket memory limit was crossed, new packets were prevented from being enqueued to the socket. This happened even though the TCP advertised window was still open.</p>
	<p>This results in an interesting situation. The receiver's window is open which might indicate it has resources to handle the data. But that's not always the case, like in our example when it runs out of the memory budget.</p>
	<p>The sender will think it hit a network congestion packet loss and will run the usual retry mechanisms including exponential backoff. This behavior can be looked at as desired or undesired, depending on how you look at it. On one hand no data will be lost, the sender can eventually deliver all the bytes reliably. On the other hand the exponential backoff logic might stall the sender for a long time, causing a noticeable delay.</p>
	<p>The root of the problem is straightforward - Linux kernel <strong>skmem_rb </strong>sets a memory budget for both the <strong>data</strong> and <strong>metadata </strong>which reside on the socket. In a pessimistic case each packet might incur a cost of a <strong>struct sk_buff</strong> + <strong>struct skb_shared_info</strong>, which on my system is 576 bytes, above the actual payload size, plus memory waste due to network card buffer alignment:</p>
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2022/07/image7-10.png" class="kg-image"></figure>
	<p>We now understand that Linux can't just advertise 100% of the memory budget as an advertised window. Some budget must be reserved for metadata and such. The upper limit of window size is expressed as a fraction of the "free" socket budget. It is controlled by <code>tcp_adv_win_scale</code>, with the following values:</p>
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2022/07/image9-5.png" class="kg-image"></figure>
	<p>By default, Linux sets the advertised window at most at 50% of the remaining buffer space.</p>
	<p>Even with 50% of space "reserved" for metadata, the kernel is very smart and tries hard to reduce the metadata memory footprint. It has two mechanisms for this:</p>
	<ul>
		<li><strong>TCP Coalesce</strong> - on the happy path, Linux is able to throw away <strong>struct sk_buff</strong>. It can do so, by just linking the data to the previously enqueued packet. You can think about it as if it was <a href="https://www.spinics.net/lists/netdev/msg755359.html" target="_blank">extending the last packet on the socket</a>.</li>
		<li><strong>TCP Collapse</strong> - when the memory budget is hit, Linux runs "collapse" code. Collapse rewrites and defragments the receive buffer from many small skb's into a few very long segments - therefore reducing the metadata cost.</li>
	</ul>
	<p>Here's an extension to our previous chart showing these mechanisms in action:</p>
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2022/07/image3-10.png" class="kg-image"></figure>
	<p><strong>TCP Coalesce</strong> is a very effective measure and works behind the scenes at all times. In the bottom chart, the packets where the coalesce was engaged are shown with a pink line. You can see - the <strong>skmem_r</strong> bumps (blue line) are clearly correlated with a <strong>lack</strong> of coalesce (pink line)! The nstat TcpExtTCPRcvCoalesce counter might be helpful in debugging coalesce issues.</p>
	<p>The <strong>TCP Collapse</strong> is a bigger gun. <a href="https://blog.cloudflare.com/optimizing-tcp-for-high-throughput-and-low-latency/">Mike wrote about it extensively</a>, and <a href="https://blog.cloudflare.com/the-story-of-one-latency-spike/">I wrote a blog post years ago, when the latency of TCP collapse hit us hard</a>. In the chart above, the collapse is shown as a red circle. We clearly see it being engaged after the socket memory budget is reached - from packet number 63. The nstat TcpExtTCPRcvCollapsed counter is relevant here. This value growing is a bad sign and might indicate bad latency spikes - especially when dealing with larger buffers. Normally collapse is supposed to be run very sporadically. A <a href="https://lore.kernel.org/lkml/20120510173135.615265392@linuxfoundation.org/" target="_blank">prominent kernel developer describes</a> this pessimistic situation:</p>
	<blockquote>This also means tcp advertises a too optimistic window for a given allocated rcvspace: When receiving frames, <code>sk_rmem_alloc</code> can hit <code>sk_rcvbuf</code> limit and we call <code>tcp_collapse()</code> too often, especially when application is slow to drain its receive queue [...] This is a major latency source.</blockquote>
	<p>If the memory budget remains exhausted after the collapse, Linux will drop ingress packets. In our chart it's marked as a red "X". The nstat TcpExtTCPRcvQDrop counter shows the count of dropped packets.</p>
	<h3 id="rcv_ssthresh-predicts-the-metadata-cost">rcv_ssthresh predicts the metadata cost</h3>
	<p>Perhaps counter-intuitively, the memory cost of a packet can be much larger than the amount of actual application data contained in it. It depends on number of things:</p>
	<ul>
		<li><strong>Network card</strong>: some network cards always allocate a full page (4096, or even 16KiB) per packet, no matter how small or large the payload.</li>
		<li><strong>Payload size</strong>: shorter packets, will have worse metadata to content ratio since <strong>struct skb</strong> will be comparably larger.</li>
		<li>Whether XDP is being used.</li>
		<li>L2 header size: things like ethernet, vlan tags, and tunneling can add up.</li>
		<li>Cache line size: many kernel structs are cache line aligned. On systems with larger cache lines, they will use more memory (see P4 or S390X architectures).</li>
	</ul>
	<p>The first two factors are the most important. Here's a run when the sender was specially configured to make the metadata cost bad and the coalesce ineffective (the <a href="https://github.com/cloudflare/cloudflare-blog/blob/master/2022-07-rmem-a/window.py#L90" target="_blank">details of the setup are messy</a>):</p>
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2022/07/image1-10.png" class="kg-image"></figure>
	<p>You can see the kernel hitting TCP collapse multiple times, which is totally undesired. Each time a collapse kernel is likely to rewrite the full receive buffer. This whole kernel machinery, from reserving some space for metadata with tcp_adv_win_scale, via using coalesce to reduce the memory cost of each packet, up to the rcv_ssthresh limit, exists to avoid this very case of hitting collapse too often.</p>
	<p>The kernel machinery most often works fine, and TCP collapse is rare in practice. However, we noticed that's not the case for certain types of traffic. One example is <a href="https://lore.kernel.org/lkml/CA+wXwBSGsBjovTqvoPQEe012yEF2eYbnC5_0W==EAvWH1zbOAg@mail.gmail.com/" target="_blank">websocket traffic with loads of tiny packets</a> and a slow reader. One <a href="https://elixir.bootlin.com/linux/latest/source/net/ipv4/tcp_input.c#L452" target="_blank">kernel comment talks about</a> such a case:</p>
	<!--kg-card-begin: markdown-->
	<pre><code>* The scheme does not work when sender sends good segments opening
* window and then starts to feed us spaghetti. But it should work
* in common situations. Otherwise, we have to rely on queue collapsing.
</code></pre>
	<!--kg-card-end: markdown-->
	<p>Notice that the <strong>rcv_ssthresh</strong> line dropped down on the TCP collapse. This variable is an internal limit to the advertised window. By dropping it the kernel effectively says: hold on, I mispredicted the packet cost, next time I'm given an opportunity I'm going to open a smaller window. Kernel will advertise a smaller window and be more careful - all of this dance is done to avoid the collapse.</p>
	<h3 id="normal-run-continuously-updated-window">Normal run - continuously updated window</h3>
	<p>Finally, here's a chart from a normal run of a connection. Here, we use the default <code>tcp_adv_win_wcale=1 (50%)</code>:</p>
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2022/07/image5-13.png" class="kg-image"></figure>
	<p>Early in the connection you can see <strong>rcv_win</strong> being continuously updated with each received packet. This makes sense: while the <strong>rcv_ssthresh</strong> and <strong>tcp_adv_win_scale</strong> restrict the advertised window to never exceed 32KiB, the window is sliding nicely as long as there is enough space. At packet 18 the receiver stops updating the window and waits a bit. At packet 32 the receiver decides there still is some space and updates the window again, and so on. At the end of the flow the socket has 56KiB of data. This 56KiB of data was received over a sliding window reaching at most 32KiB .</p>
	<p>The saw blade pattern of rcv_win is enabled by delayed ACK (aka QUICKACK). You can see the "<strong>acked</strong>" bytes in red dashed line. Since the ACK's might be delayed, the receiver waits a bit before updating the window. If you want a smooth line, you can use <code>quickack 1</code> per-route parameter, but this is not recommended since it will result in many small ACK packets flying over the wire.</p>
	<p>In normal connection we expect the majority of packets to be coalesced and the collapse/drop code paths never to be hit.</p>
	<h3 id="large-receive-windows-rcv_ssthresh">Large receive windows - rcv_ssthresh</h3>
	<p>For large bandwidth transfers over big latency links - big BDP case - it's beneficial to have a very wide advertised window. However, Linux takes a while to fully open large receive windows:</p>
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2022/07/image8-4.png" class="kg-image"></figure>
	<p>In this run, the <strong>skmem_rb</strong> is set to 2MiB. As opposed to previous runs, the buffer budget is large and the receive window doesn't start with 50% of the skmem_rb! Instead it starts from 64KiB and grows linearly. It takes a while for Linux to ramp up the receive window to full size - ~800KiB in this case. The window is clamped by <strong>rcv_ssthresh</strong>. This variable starts at 64KiB and then grows at a rate of two full-MSS packets per each packet which has a "good" ratio of total size (truesize) to payload size.</p>
	<p><a href="https://lore.kernel.org/lkml/CANn89i+mhqGaM2tuhgEmEPbbNu_59GGMhBMha4jnnzFE=UBNYg@mail.gmail.com/" target="_blank">Eric Dumazet writes</a> about this behavior:</p>
	<blockquote>Stack is conservative about RWIN increase, it wants to receive packets to have an idea of the skb-&gt;len/skb-&gt;truesize ratio to convert a memory budget to &nbsp;RWIN.<br>Some drivers have to allocate 16K buffers (or even 32K buffers) just to hold one segment (of less than 1500 bytes of payload), while others are able to pack memory more efficiently.</blockquote>
	<p>This behavior of slow window opening is fixed, and not configurable in vanilla kernel.<a href="https://lore.kernel.org/netdev/20220721151041.1215017-1-marek@cloudflare.com/#r" target="_blank"> We prepared a kernel patch that allows to start up with higher rcv_ssthresh</a> based on per-route option <code>initrwnd</code>:</p>
	<!--kg-card-begin: markdown-->
	<pre><code>$ ip route change local 127.0.0.0/8 dev lo initrwnd 1000
</code></pre>
	<!--kg-card-end: markdown-->
	<p>With the patch and the route change deployed, this is how the buffers look:</p>
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2022/07/image4-12.png" class="kg-image"></figure>
	<p>The advertised window is limited to 64KiB during the TCP handshake, but with our kernel patch enabled it's quickly bumped up to 1MiB in the first ACK packet afterwards. In both runs it took ~1800 packets to fill the receive buffer, however it took different time. In the first run the sender could push only 64KiB onto the wire in the second RTT. In the second run it could immediately push full 1MiB of data.</p>
	<p>This trick of aggressive window opening is not really necessary for most users. It's only helpful when:</p>
	<ul>
		<li>You have high-bandwidth TCP transfers over big-latency links.</li>
		<li>The metadata + buffer alignment cost of your NIC is sensible and predictable.</li>
		<li>Immediately after the flow starts your application is ready to send a lot of data.</li>
		<li>The sender has configured large <code>initcwnd</code>.</li>
	</ul>
	<h3 id="you-care-about-shaving-off-every-possible-rtt-">You care about shaving off every possible RTT.</h3>
	<p>On our systems we do have such flows, but arguably it might not be a common scenario. In the real world most of your TCP connections go to the nearest CDN point of presence, which is very close.</p>
	<h3 id="getting-it-all-together">Getting it all together</h3>
	<p>In this blog post, we discussed a seemingly simple case of a TCP sender filling up the receive socket. We tried to address two questions: with our isolated setup, how much data can be sent, and how quickly?</p>
	<p>With the default settings of net.ipv4.tcp_rmem, Linux initially sets a memory budget of 128KiB for the receive data and metadata. On my system, given full-sized packets, it's able to eventually accept around 113KiB of application data.</p>
	<p>Then, we showed that the receive window is not fully opened immediately. Linux keeps the receive window small, as it tries to predict the metadata cost and avoid overshooting the memory budget, therefore hitting TCP collapse. By default, with the net.ipv4.tcp_adv_win_scale=1, the upper limit for the advertised window is 50% of "free" memory. rcv_ssthresh starts up with 64KiB and grows linearly up to that limit.</p>
	<p>On my system it took five window updates - six RTTs in total - to fill the 128KiB receive buffer. In the first batch the sender sent ~64KiB of data (remember we hacked the <code>initcwnd</code> limit), and then the sender topped it up with smaller and smaller batches until the receive window fully closed.</p>
	<p>I hope this blog post is helpful and explains well the relationship between the buffer size and advertised window on Linux. Also, it describes the often misunderstood rcv_ssthresh which limits the advertised window in order to manage the memory budget and predict the unpredictable cost of metadata.</p>
	<p>In case you wonder, similar mechanisms are in play in QUIC. The QUIC/H3 libraries though are still pretty young and don't have so many complex and mysterious toggles.... yet.</p>
	<p>As always, <a href="https://github.com/cloudflare/cloudflare-blog/tree/master/2022-07-rmem-a" target="_blank">the code and instructions on how to reproduce the charts are available at our GitHub</a>.</p>
</div>