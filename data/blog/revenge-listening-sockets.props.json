{
	"footerBlurb": "Cloudflare's connectivity cloud protects <a target='_blank' href='https://www.cloudflare.com/network-services/' rel='noreferrer'>entire corporate networks</a>, helps customers build <a target='_blank' href='https://workers.cloudflare.com/' rel='noreferrer'>Internet-scale applications efficiently</a>, accelerates any <a target='_blank' href='https://www.cloudflare.com/performance/accelerate-internet-applications/' rel='noreferrer'>website or Internet application</a>, <a target='_blank' href='https://www.cloudflare.com/ddos/' rel='noreferrer'>wards off DDoS attacks</a>, keeps <a target='_blank' href='https://www.cloudflare.com/application-security/' rel='noreferrer'>hackers at bay</a>, and can help you on <a target='_blank' href='https://www.cloudflare.com/products/zero-trust/' rel='noreferrer'>your journey to Zero Trust</a>.<br/><br/>Visit <a target='_blank' href='https://one.one.one.one/' rel='noreferrer'>1.1.1.1</a> from any device to get started with our free app that makes your Internet faster and safer.<br/><br/>To learn more about our mission to help build a better Internet, <a target='_blank' href='https://www.cloudflare.com/learning/what-is-cloudflare/' rel='noreferrer'>start here</a>. If you&apos;re looking for a new career direction, check out <a target='_blank' href='http://www.cloudflare.com/careers' rel='noreferrer'>our open positions</a>.",
	"initialReadingTime": "5",
	"locale": "en-us",
	"localesAvailable": [],
	"post": {
		"authors": [
			{
				"name": "Marek Majkowski",
				"slug": "marek-majkowski",
				"bio": null,
				"profile_image": "https://cf-assets.www.cloudflare.com/zkvhlag99gkb/1JuU5qavgwVeqR8BAUrd6U/3a0d0445d41c9a3c42011046efe9c37b/marek-majkowski.jpeg",
				"location": null,
				"website": null,
				"twitter": "@majek04",
				"facebook": null,
				"publiclyIndex": true
			}
		],
		"excerpt": "Back in November we wrote a blog post about one latency spike. Today I'd like to share a continuation of that story. As it turns out, the misconfigured rmem setting wasn't the only source of added latency. It looked like Mr Wolf hadn't finished his job.",
		"feature_image": "https://cf-assets.www.cloudflare.com/zkvhlag99gkb/1ME74WmN0LL35y8Ic7DShc/33423919a893126ad9de5147bd00a2ee/revenge-listening-sockets.png",
		"featured": false,
		"html": "<p>Back in November we wrote <a href=\"/the-story-of-one-latency-spike/\">a blog post about one latency spike</a>. Today I&#39;d like to share a continuation of that story. As it turns out, the misconfigured <code>rmem</code> setting wasn&#39;t the only source of added latency.</p><p>It looked like Mr. Wolf hadn&#39;t finished his job.</p><p>After adjusting the previously discussed <code>rmem</code> sysctl we continued monitoring our systems&#39; latency. Among other things we measured <code>ping</code> times to our edge servers. While the worst case improved and we didn&#39;t see 1000ms+ pings anymore, the line still wasn&#39;t flat. Here&#39;s a graph of ping latency between an idling internal machine and a production server. The test was done within the datacenter, the packets never went to the public internet. The Y axis of the chart shows <code>ping</code> times in milliseconds, the X axis is the time of the measurement. Measurements were taken every second for over 6 hours:</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/1XHIKdZc7gzLfYQHJOqlvn/61047e9477c5923c552b01aa49750200/ping-48m4-csv-before.png\" alt=\"\" class=\"kg-image\" width=\"1000\" height=\"400\" loading=\"lazy\"/>\n            \n            </figure><p>As you can see most pings finished below 1ms. But out of 21,600 measurements about 20 had high latency of up to 100ms. Not ideal, is it?</p>\n    <div class=\"flex anchor relative\">\n      <h2 id=\"system-tap\">System tap</h2>\n      <a href=\"#system-tap\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>The latency occurred within our datacenter and the packets weren&#39;t lost. This suggested a kernel issue again. Linux responds to ICMP pings from its soft interrupt handling code. A delay in handling <code>ping</code> indicates a delay in Soft IRQ handling which is really bad and can affect all packets delivered to a machine. Using the <a href=\"https://github.com/cloudflare/cloudflare-blog/blob/master/2016-04-bind-to-star/histogram-kernel.stp\">system tap script</a> we were able to measure the time distribution of the main soft IRQ function <code>net_rx_action</code>:</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/6A2YakORYW8kp1LE15b8nk/1eabe5e66f5c30b60cf32506f9fa70bd/a-net.png\" alt=\"\" class=\"kg-image\" width=\"1069\" height=\"494\" loading=\"lazy\"/>\n            \n            </figure><p>This distribution was pretty awful. While most of the calls to <code>net_rx_action</code> were handled in under 81us (average), the slow outliers were really bad. Three calls took a whopping 32ms! No wonder the <code>ping</code> times were off.</p>\n    <div class=\"flex anchor relative\">\n      <h2 id=\"the-inet_lookup-is-slow\">The <code>inet_lookup</code> is slow</h2>\n      <a href=\"#the-inet_lookup-is-slow\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>With some back and forth with flame graphs and the <a href=\"https://github.com/cloudflare/cloudflare-blog/blob/master/2016-04-bind-to-star/histogram-kernel.stp\"><code>histogram-kernel.stp</code> script</a> we went deeper to look for the culprit. We found that <a href=\"http://lxr.free-electrons.com/source/net/ipv4/tcp_ipv4.c?v=3.18#L1585\"><code>tcp_v4_rcv</code></a> had a similarly poor latency distribution. More specifically the problem lies between lines 1637 and 1642 in the <code>tcp_v4_rcv</code> function in the <code>tcp_ipv4.c</code> file. We wrote <a href=\"https://github.com/cloudflare/cloudflare-blog/blob/master/2016-04-bind-to-star/histogram-kernel2.stp\">another script to show</a> just that:</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/2YByqoyG6F8kapoqFISNcG/26eb861a2d9e07747d70c4b13e2fadf5/a-tcp.png\" alt=\"\" class=\"kg-image\" width=\"1069\" height=\"494\" loading=\"lazy\"/>\n            \n            </figure><p>The latency is created at this specific line in <code>tcp_v4_rcv</code> function:</p>\n            <pre class=\"language-.c\"><code class=\"language-.c\">sk = __inet_lookup_skb(&amp;tcp_hashinfo, skb, th-&gt;source, th-&gt;dest);</pre></code>\n            <p>The numbers shown above indicate that the function usually terminated quickly, in under 2us, but sometimes it hit a slow path and took 1-2ms to finish.</p><p>The <a href=\"http://lxr.free-electrons.com/source/include/net/inet_hashtables.h?v=3.18#L343\"><code>__inet_lookup_skb</code></a> is inlined which makes it tricky to accurately measure. Fortunately the function is simple - all it does is to call <code>__inet_lookup_established</code> and <code>__inet_lookup_listener</code>. It&#39;s the latter function that was causing the trouble:</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/4IcyrSCQ0rO6dGDDMJIYAw/f86a763d330e26fde206d51de3ed1ad6/a-tcp-copy.png\" alt=\"\" class=\"kg-image\" width=\"1069\" height=\"447\" loading=\"lazy\"/>\n            \n            </figure><p>Let&#39;s discuss how <code>__inet_lookup</code> works. This function tries to find an appropriate connection <code>sock struct</code> structure for a packet. This is done in the <code>__inet_lookup_established</code> call. If that fails, the <code>__inet_lookup</code> will attempt to find a bound socket in listening state that could potentially handle the packet. For example, if the packet is SYN and the listening socket exists we should respond with SYN+ACK. If there is no bound listening socket we should send an RST instead. The <code>__inet_lookup_listener</code> function finds the bound socket in the <code>LHTABLE</code> hash table. It does so by using the destination port as a hash and picks an appropriate bucket in the hash table. Then it iterates over it linearly to find the matching listening socket.</p><p>To understand the problem we traced the slow packets, with <a href=\"https://github.com/cloudflare/cloudflare-blog/blob/master/2016-04-bind-to-star/duration-inet-lookup2.stp\">another crafted system tap script</a>. It hooks onto <code>__inet_lookup_listener</code> and prints out the details of only the slow packets:</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/3OBYuGrWgdW7nOI1bp4ZIi/4cb7a50dbe9e2a2af3fc44be6e54790c/a-inet.png\" alt=\"\" class=\"kg-image\" width=\"1069\" height=\"313\" loading=\"lazy\"/>\n            \n            </figure><p>With this data we went deeper and matched these log lines to specific packets captured with <code>tcpdump</code>. I&#39;ll spare you the details, but these are inbound SYN and RST packets which destination port modulo 32 is equal to 21. Check it out:</p><ul><li><p>16725 % 32 = 21</p></li><li><p>53 % 32 = 21</p></li><li><p>63925 % 32 = 21</p></li></ul><p>Now, where does this magic number come from?</p>\n    <div class=\"flex anchor relative\">\n      <h2 id=\"the-listening-hash-table\">The listening hash table</h2>\n      <a href=\"#the-listening-hash-table\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>As mentioned above, Linux maintains a listening hash table containing the listening TCP sockets - the <code>LHTABLE</code>. It has a fixed size of <a href=\"http://lxr.free-electrons.com/source/include/net/inet_hashtables.h?v=3.18#L117\">32 buckets</a>:</p>\n            <pre class=\"language-.txt\"><code class=\"language-.txt\">/* Yes, really, this is all you need. */\n#define INET_LHTABLE_SIZE       32</pre></code>\n            <p>To recap:</p><ul><li><p>All the SYN and RST packets trigger a lookup in LHTABLE. Since the connection entry doesn&#39;t exist the <code>__inet_lookup_established</code> call fails and <code>__inet_lookup_listener</code> will be called.</p></li><li><p>LHTABLE is small - it has only 32 buckets.</p></li><li><p>LHTABLE is hashed by destination port only.</p></li></ul><p>It&#39;s time for a quick diversion. Let&#39;s speak about CloudFlare&#39;s DNS server.</p>\n    <div class=\"flex anchor relative\">\n      <h2 id=\"lets-speak-about-dns\">Let&#39;s speak about DNS</h2>\n      <a href=\"#lets-speak-about-dns\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>At CloudFlare we are using a custom DNS server called <code>rrdns</code>. Among many other requirements, the server is designed to withstand DDoS attacks.</p><p>Even though our server is pretty fast, when a large attack kicks in it might be unable to cope with the attack load. If that happens we must contain the damage - even if one IP address is under a heavy attack the server must still handle legitimate traffic on other IP addresses. In fact, our DNS architecture is designed to spread the load among 16k IP addresses.</p><p>When <a href=\"https://www.cloudflare.com/ddos/under-attack/\">an IP address is under attack</a>, and the server is not keeping up with incoming packets, the kernel receive queue on a UDP socket will overflow. We monitor that by looking at the <code>netstat</code> counters:</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">$ netstat -s --udp\nUdp:\n    43957575 packet receive errors</pre></code>\n            <p>With that number increasing we can see the affected IP addresses by listing the UDP sockets with non-empty receive queues:</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">$ netstat -ep4ln --udp|grep 53 |egrep -v &quot;^udp *0&quot;\nudp   524293  0 173.245.1.1:53  0.0.0.0:*   0\nudp   524288  0 173.245.2.3:53  0.0.0.0:*   0</pre></code>\n            <p>In this case two IP addresses received heavy UDP traffic. It was more than the DNS server could handle, the receive queues built up and eventually overflowed. Fortunately, because we are binding to specific IP addresses, overflowing some UDP receive queues won&#39;t affect any other IP addresses.</p><p>Binding to specific IP addresses is critical to keep our DNS infrastructure online. With this setup even if other mitigation techniques fail and the DNS server is left exposed to the packet flood, we are certain the attack will not affect handling DNS on other IP addresses.</p><p>But what does that have to do with the <code>LHTABLE</code>? Well, in our setup we bound to specific IP addresses for both UDP <i>and</i> TCP. While having 16k listening <a href=\"http://lxr.free-electrons.com/source/net/ipv4/udp.c?v=3.18#L476\">sockets in UDP is okay</a>, it turns out it is not fine for TCP.</p>\n    <div class=\"flex anchor relative\">\n      <h2 id=\"what-happened\">What happened</h2>\n      <a href=\"#what-happened\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>Due to our DNS setup we had 16k TCP sockets bound to different IP addresses on port 53. Since the port number is fixed, all these sockets ended in exactly one <code>LHTABLE</code> bucket. This particular bucket was number 21 (53 % 32 = 21). When an RST or SYN packet hit it, the <code>__inet_lookup_listener</code> call had to traverse all 16k socket entries. This wasn&#39;t fast, in fact it took 2ms to finish.</p><p>To solve the problem we deployed two changes:</p><ul><li><p>For TCP connections our DNS server now binds to ANY_IP address (aka: 0.0.0.0:53, *:53). We call this &quot;bind to star&quot;. While binding to specific IP addresses is still necessary for UDP, there is little benefit in doing that for the TCP traffic. For TCP we can bind to star safely, without compromising our DDoS defenses.</p></li><li><p>We increased the <code>LHTABLE</code> size in our kernels. We are not the first to do that: Bill Sommerfeld from Google <a href=\"http://patchwork.ozlabs.org/patch/79014/\">suggested that back in 2011</a>.</p></li></ul><p>With these changes deployed the <code>ping</code> times within our datacenter are finally flat, as they should always have been:</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/2SDuQY3mNlpBZoynVywAMr/25131e60e13762142e59408be69e51d6/ping-48m3-csv-after.png\" alt=\"\" class=\"kg-image\" width=\"1000\" height=\"400\" loading=\"lazy\"/>\n            \n            </figure>\n    <div class=\"flex anchor relative\">\n      <h2 id=\"final-words\">Final words</h2>\n      <a href=\"#final-words\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>You can&#39;t have a very large number of bound TCP sockets and we learned that the hard way. We learned a bit about the Linux networking stack: the fact that <code>LHTABLE</code> is fixed size and is hashed by destination port only. Once again we showed a couple of <a href=\"https://github.com/cloudflare/cloudflare-blog/blob/master/2016-04-bind-to-star\">powerful of System Tap scripts</a>.</p><p>With the fixes deployed maximum latency numbers have dropped significantly. We are confident that soft interrupt handling in <code>net_rx_action</code> is behaving well.</p><p>Mr Wolf has finally finished his assignment.</p><p>If this sounds interesting, <a href=\"https://www.cloudflare.com/join-our-team/\">consider joining us</a>. We have teams in Singapore, San Francisco and London.</p>",
		"id": "5iVF3h1IUwOBj50KAE7chO",
		"localeList": {
			"name": "The revenge of the listening sockets Config",
			"enUS": "English for Locale",
			"zhCN": "No Page for Locale",
			"zhHansCN": "No Page for Locale",
			"zhTW": "No Page for Locale",
			"frFR": "No Page for Locale",
			"deDE": "No Page for Locale",
			"itIT": "No Page for Locale",
			"jaJP": "No Page for Locale",
			"koKR": "No Page for Locale",
			"ptBR": "No Page for Locale",
			"esLA": "No Page for Locale",
			"esES": "No Page for Locale",
			"enAU": "No Page for Locale",
			"enCA": "No Page for Locale",
			"enIN": "No Page for Locale",
			"enGB": "No Page for Locale",
			"idID": "No Page for Locale",
			"ruRU": "No Page for Locale",
			"svSE": "No Page for Locale",
			"viVN": "No Page for Locale",
			"plPL": "No Page for Locale",
			"arAR": "No Page for Locale",
			"nlNL": "No Page for Locale",
			"thTH": "No Page for Locale",
			"trTR": "No Page for Locale",
			"heIL": "No Page for Locale",
			"lvLV": "No Page for Locale",
			"etEE": "No Page for Locale",
			"ltLT": "No Page for Locale"
		},
		"meta_description": null,
		"metadata": {
			"imgPreview": ""
		},
		"primary_author": {},
		"publicly_index": true,
		"published_at": "2016-04-05T13:05:01.000+01:00",
		"slug": "revenge-listening-sockets",
		"tags": [
			{
				"id": "6lhzEBz2B56RKa4nUEAGYJ",
				"name": "Programming",
				"slug": "programming"
			},
			{
				"id": "5fZHv2k9HnJ7phOPmYexHw",
				"name": "DNS",
				"slug": "dns"
			},
			{
				"id": "6QVJOBzgKXUO9xAPEpqxvK",
				"name": "Reliability",
				"slug": "reliability"
			},
			{
				"id": "383iv0UQ6Lp0GZwOAxGq2p",
				"name": "Linux",
				"slug": "linux"
			}
		],
		"title": "The revenge of the listening sockets",
		"updated_at": "2024-10-10T00:34:42.795Z",
		"url": "https://blog.cloudflare.com/revenge-listening-sockets"
	},
	"translations": {
		"posts.by": "By",
		"footer.gdpr": "GDPR",
		"lang_blurb1": "This post is also available in {lang1}.",
		"lang_blurb2": "This post is also available in {lang1} and {lang2}.",
		"lang_blurb3": "This post is also available in {lang1}, {lang2} and {lang3}.",
		"footer.press": "Press",
		"header.title": "The Cloudflare Blog",
		"search.clear": "Clear",
		"search.filter": "Filter",
		"search.source": "Source",
		"footer.careers": "Careers",
		"footer.company": "Company",
		"footer.support": "Support",
		"footer.the_net": "theNet",
		"search.filters": "Filters",
		"footer.our_team": "Our team",
		"footer.webinars": "Webinars",
		"page.more_posts": "More posts",
		"posts.time_read": "{time} min read",
		"search.language": "Language",
		"footer.community": "Community",
		"footer.resources": "Resources",
		"footer.solutions": "Solutions",
		"footer.trademark": "Trademark",
		"header.subscribe": "Subscribe",
		"footer.compliance": "Compliance",
		"footer.free_plans": "Free plans",
		"footer.impact_ESG": "Impact/ESG",
		"posts.follow_on_X": "Follow on X",
		"footer.help_center": "Help center",
		"footer.network_map": "Network Map",
		"header.please_wait": "Please Wait",
		"page.related_posts": "Related posts",
		"search.result_stat": "Results <strong>{search_range}</strong> of <strong>{search_total}</strong> for <strong>{search_keyword}</strong>",
		"footer.case_studies": "Case Studies",
		"footer.connect_2024": "Connect 2024",
		"footer.terms_of_use": "Terms of Use",
		"footer.white_papers": "White Papers",
		"footer.cloudflare_tv": "Cloudflare TV",
		"footer.community_hub": "Community Hub",
		"footer.compare_plans": "Compare plans",
		"footer.contact_sales": "Contact Sales",
		"header.contact_sales": "Contact Sales",
		"header.email_address": "Email Address",
		"page.error.not_found": "Page not found",
		"footer.developer_docs": "Developer docs",
		"footer.privacy_policy": "Privacy Policy",
		"footer.request_a_demo": "Request a demo",
		"page.continue_reading": "Continue reading",
		"footer.analysts_report": "Analyst reports",
		"footer.for_enterprises": "For enterprises",
		"footer.getting_started": "Getting Started",
		"footer.learning_center": "Learning Center",
		"footer.project_galileo": "Project Galileo",
		"pagination.newer_posts": "Newer Posts",
		"pagination.older_posts": "Older Posts",
		"posts.social_buttons.x": "Discuss on X",
		"search.icon_aria_label": "Search",
		"search.source_location": "Source/Location",
		"footer.about_cloudflare": "About Cloudflare",
		"footer.athenian_project": "Athenian Project",
		"footer.become_a_partner": "Become a partner",
		"footer.cloudflare_radar": "Cloudflare Radar",
		"footer.network_services": "Network services",
		"footer.trust_and_safety": "Trust & Safety",
		"header.get_started_free": "Get Started Free",
		"page.search.placeholder": "Search Cloudflare",
		"footer.cloudflare_status": "Cloudflare Status",
		"footer.cookie_preference": "Cookie Preferences",
		"header.valid_email_error": "Must be valid email.",
		"search.result_stat_empty": "Results <strong>{search_range}</strong> of <strong>{search_total}</strong>",
		"footer.connectivity_cloud": "Connectivity cloud",
		"footer.developer_services": "Developer services",
		"footer.investor_relations": "Investor relations",
		"page.not_found.error_code": "Error Code: 404",
		"search.autocomplete_title": "Insert a query. Press enter to send",
		"footer.logos_and_press_kit": "Logos & press kit",
		"footer.application_services": "Application services",
		"footer.get_a_recommendation": "Get a recommendation",
		"posts.social_buttons.reddit": "Discuss on Reddit",
		"footer.sse_and_sase_services": "SSE and SASE services",
		"page.not_found.outdated_link": "You may have used an outdated link, or you may have typed the address incorrectly.",
		"footer.report_security_issues": "Report Security Issues",
		"page.error.error_message_page": "Sorry, we can't find the page you are looking for.",
		"header.subscribe_notifications": "Subscribe to receive notifications of new posts:",
		"footer.cloudflare_for_campaigns": "Cloudflare for Campaigns",
		"header.subscription_confimation": "Subscription confirmed. Thank you for subscribing!",
		"posts.social_buttons.hackernews": "Discuss on Hacker News",
		"footer.diversity_equity_inclusion": "Diversity, equity & inclusion",
		"footer.critical_infrastructure_defense_project": "Critical Infrastructure Defense Project"
	}
}