<div class="post-content lh-copy gray1">
	<!--kg-card-begin: markdown-->
	<p>Here at Cloudflare, we have a lot of experience of operating servers on the wild Internet. But we are always improving our mastery of this black art. On this very blog we have touched on multiple dark corners of the Internet protocols: like <a href="https://blog.cloudflare.com/this-is-strictly-a-violation-of-the-tcp-specification/">understanding FIN-WAIT-2</a> or <a href="https://blog.cloudflare.com/the-story-of-one-latency-spike/">receive buffer tuning</a>.</p>
	<p><img src="https://blog.cloudflare.com/content/images/2018/01/1471936772_652043cb6c_b.jpg" alt="1471936772_652043cb6c_b"><br>
		<small><a href="https://creativecommons.org/licenses/by/2.0/" target="_blank">CC BY 2.0</a> <a href="https://www.flickr.com/photos/isaimoreno/1471936772/in/photolist-3f54wd-6mweJG-maUn5T-2tgqad-6YuCuM-pZ7r8T-Sa3LQ9-adTFS2-qSLQzk-sJ66Lq-71cJPS-oFU9rf-mbom12-23fVpJW-71ciCN-718DHR-j4VCQQ-71chKo-5DMBr4-5DLQFK-71cG4s-qQFjhZ-2RMBP6-718KWR-71cAFA-fAr8Ri-pe5zev-8TtDbQ-b6p5gk-qAdMqQ-qSBvUZ-qyg7oz-o5yof6-adTGvc-718xp4-5XQgJZ-bgGiwk-kf7aMc-qAjY14-718uti-smXfxF-8Kdnpx-nVVy8a-cmMJGb-puizaG-qP18i9-71cu1E-nYNfjq-718CjH-qyQM72" target="_blank">image</a> by <a href="https://www.flickr.com/photos/isaimoreno/" target="_blank">Isaí Moreno</a></small>
	</p>
	<p>One subject hasn't had enough attention though - SYN floods. We use Linux and it turns out that SYN packet handling in Linux is truly complex. In this post we'll shine some light on this subject.</p>
	<h3 id="thetaleoftwoqueues">The tale of two queues</h3>
	<p><img src="https://blog.cloudflare.com/content/images/2018/01/all-1.jpeg" alt="all-1"></p>
	<p>First we must understand that each bound socket, in the "LISTENING" TCP state has two separate queues:</p>
	<ul>
		<li>The SYN Queue</li>
		<li>The Accept Queue</li>
	</ul>
	<p>In the literature these queues are often given other names such as "reqsk_queue", "ACK backlog", "listen backlog" or even "TCP backlog", but I'll stick to the names above to avoid confusion.</p>
	<h3 id="synqueue">SYN Queue</h3>
	<p>The SYN Queue stores inbound SYN packets<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup> (specifically: <a href="https://elixir.free-electrons.com/linux/v4.14.12/source/include/net/inet_sock.h#L73" target="_blank"><code>struct inet_request_sock</code></a>). It's responsible for sending out SYN+ACK packets and retrying them on timeout. On Linux the number of retries is configured with:</p>
	<pre><code>$ sysctl net.ipv4.tcp_synack_retries
net.ipv4.tcp_synack_retries = 5
</code></pre>
	<p>The <a href="https://www.kernel.org/doc/Documentation/networking/ip-sysctl.txt" target="_blank">docs describe this toggle</a>:</p>
	<pre><code>tcp_synack_retries - INTEGER

	Number of times SYNACKs for a passive TCP connection attempt
	will be retransmitted. Should not be higher than 255. Default
	value is 5, which corresponds to 31 seconds till the last
	retransmission with the current initial RTO of 1second. With
	this the final timeout for a passive TCP connection will
	happen after 63 seconds.

</code></pre>
	<p>After transmitting the SYN+ACK, the SYN Queue waits for an ACK packet from the client - the last packet in the three-way-handshake. All received ACK packets must first be matched against the fully established connection table, and only then against data in the relevant SYN Queue. On SYN Queue match, the kernel removes the item from the SYN Queue, happily creates a fully fledged connection (specifically: <a href="https://elixir.free-electrons.com/linux/v4.14.12/source/include/net/inet_sock.h#L183" target="_blank"><code>struct inet_sock</code></a>), and adds it to the Accept Queue.</p>
	<h3 id="acceptqueue">Accept Queue</h3>
	<p>The Accept Queue contains fully established connections: ready to be picked up by the application. When a process calls <code>accept()</code>, the sockets are de-queued and passed to the application.</p>
	<p>This is a rather simplified view of SYN packet handling on Linux. With socket toggles like <code>TCP_DEFER_ACCEPT</code><sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup> and <code>TCP_FASTOPEN</code> things work slightly differently.</p>
	<h3 id="queuesizelimits">Queue size limits</h3>
	<p>The maximum allowed length of both the Accept and SYN Queues is taken from the <code>backlog</code> parameter passed to the <code>listen(2)</code> syscall by the application. For example, this sets the Accept and SYN Queue sizes to 1,024:</p>
	<pre><code>listen(sfd, 1024)
</code></pre>
	<p>Note: In kernels before 4.3 the <a href="https://github.com/torvalds/linux/commit/ef547f2ac16bd9d77a780a0e7c70857e69e8f23f#diff-56ecfd3cd70d57cde321f395f0d8d743L43" target="_blank">SYN Queue length was counted differently</a>.</p>
	<p>This SYN Queue cap used to be configured by the <code>net.ipv4.tcp_max_syn_backlog</code> toggle, but this isn't the case anymore. Nowadays <code>net.core.somaxconn</code> caps both queue sizes. On our servers we set it to 16k:</p>
	<pre><code>$ sysctl net.core.somaxconn
net.core.somaxconn = 16384
</code></pre>
	<h3 id="perfectbacklogvalue">Perfect backlog value</h3>
	<p>Knowing all that, we might ask the question - what is the ideal <code>backlog</code> parameter value?</p>
	<p>The answer is: it depends. For the majority of trivial TCP Servers it doesn't really matter. For example, before version 1.11 <a href="https://github.com/golang/go/issues/6079" target="_blank">Golang famously didn't support customizing backlog value</a>. There are valid reasons to increase this value though:</p>
	<ul>
		<li>When the rate of incoming connections is really large, even with a performant application, the inbound SYN Queue may need a larger number of slots.</li>
		<li>The <code>backlog</code> value controls the SYN Queue size. This effectively can be read as "ACK packets in flight". The larger the average round trip time to the client, the more slots are going to be used. In the case of many clients far away from the server, hundreds of milliseconds away, it makes sense to increase the backlog value.</li>
		<li>The <code>TCP_DEFER_ACCEPT</code> option causes sockets to remain in the SYN-RECV state longer and contribute to the queue limits.</li>
	</ul>
	<p>Overshooting the <code>backlog</code> is bad as well:</p>
	<ul>
		<li>Each slot in SYN Queue uses some memory. During a SYN Flood it makes no sense to waste resources on storing attack packets. Each <code>struct inet_request_sock</code> entry in SYN Queue takes 256 bytes of memory on kernel 4.14.</li>
	</ul>
	<p>To peek into the SYN Queue on Linux we can use the <code>ss</code> command and look for <code>SYN-RECV</code> sockets. For example, on one of Cloudflare's servers we can see 119 slots used in tcp/80 SYN Queue and 78 on tcp/443.</p>
	<pre><code>$ ss -n state syn-recv sport = :80 | wc -l
119
$ ss -n state syn-recv sport = :443 | wc -l
78
</code></pre>
	<p>Similar data can be shown with our <a href="https://github.com/cloudflare/cloudflare-blog/blob/master/2018-01-syn-floods/resq.stp" target="_blank">overenginered SystemTap script: <code>resq.stp</code></a>.</p>
	<h3 id="slowapplication">Slow application</h3>
	<p><img src="https://blog.cloudflare.com/content/images/2018/01/full-accept-1.jpeg" alt="full-accept-1"></p>
	<p>What happens if the application can't keep up with calling <code>accept()</code> fast enough?</p>
	<p>This is when the magic happens! When the Accept Queue gets full (is of a size of <code>backlog</code>+1) then:</p>
	<ul>
		<li>Inbound SYN packets to the SYN Queue are dropped.</li>
		<li>Inbound ACK packets to the SYN Queue are dropped.</li>
		<li>The TcpExtListenOverflows / <code>LINUX_MIB_LISTENOVERFLOWS</code> counter is incremented.</li>
		<li>The TcpExtListenDrops / <code>LINUX_MIB_LISTENDROPS</code> counter is incremented.</li>
	</ul>
	<p>There is a strong rationale for dropping <em>inbound</em> packets: it's a push-back mechanism. The other party will sooner or later resend the SYN or ACK packets by which point, the hope is, the slow application will have recovered.</p>
	<p>This is a desirable behavior for almost all servers. For completeness: it can be adjusted with the global <code>net.ipv4.tcp_abort_on_overflow</code> toggle, but better not touch it.</p>
	<p>If your server needs to handle a large number of inbound connections and is struggling with <code>accept()</code> throughput, consider reading our <a href="https://blog.cloudflare.com/the-sad-state-of-linux-socket-balancing/">Nginx tuning / Epoll work distribution</a> post and a <a href="https://blog.cloudflare.com/perfect-locality-and-three-epic-systemtap-scripts/">follow up showing useful SystemTap scripts</a>.</p>
	<p>You can trace the Accept Queue overflow stats by looking at <code>nstat</code> counters:</p>
	<pre><code>$ nstat -az TcpExtListenDrops
TcpExtListenDrops     49199     0.0
</code></pre>
	<p>This is a global counter. It's not ideal - sometimes we saw it increasing, while all applications looked healthy! The first step should always be to print the Accept Queue sizes with <code>ss</code>:</p>
	<pre><code>$ ss -plnt sport = :6443|cat
State   Recv-Q Send-Q  Local Address:Port  Peer Address:Port
LISTEN  0      1024                *:6443             *:*
</code></pre>
	<p>The column <code>Recv-Q</code> shows the number of sockets in the Accept Queue, and <code>Send-Q</code> shows the backlog parameter. In this case we see there are no outstanding sockets to be <code>accept()</code>ed, but we still saw the ListenDrops counter increasing.</p>
	<p>It turns out our application was stuck for fraction of a second. This was sufficient to let the Accept Queue overflow for a very brief period of time. Moments later it would recover. Cases like this are hard to debug with <code>ss</code>, so we wrote <a href="https://github.com/cloudflare/cloudflare-blog/blob/master/2018-01-syn-floods/acceptq.stp" target="_blank">an <code>acceptq.stp</code> SystemTap script</a> to help us. It hooks into kernel and prints the SYN packets which are being dropped:</p>
	<pre><code>$ sudo stap -v acceptq.stp
time (us)        acceptq qmax  local addr    remote_addr
1495634198449075  1025   1024  0.0.0.0:6443  10.0.1.92:28585
1495634198449253  1025   1024  0.0.0.0:6443  10.0.1.92:50500
1495634198450062  1025   1024  0.0.0.0:6443  10.0.1.92:65434
...
</code></pre>
	<p>Here you can see precisely which SYN packets were affected by the ListenDrops. With this script it's trivial to understand which application is dropping connections.</p>
	<p><img src="https://blog.cloudflare.com/content/images/2018/01/3713965419_20388fb368_b.jpg" alt="3713965419_20388fb368_b"><br>
		<small><a href="https://creativecommons.org/licenses/by/2.0/" target="_blank">CC BY 2.0</a> <a href="https://www.flickr.com/photos/16339684@N00/3713965419/in/photolist-6Ec3wx-5jhnwn-bfyTRX-5jhnCa-phYcey-dxZ95n-egkTN-kwT1YH-k22LWZ-5jBUiy-bzvDWx-5jBV31-5jhnr8-5jBTkq-5jxzHk-4K3cbP-9EePyg-4e5XNt-4e5XNn-dxZ8Tn-dy5A89-dxZ6GH-cztXcJ-gF7oY-dxZ9jv-dxZ7qM-ZvSPCv-dxZ6YV-5jBTqs-5jxzaP-MvuyK-nmVwP1-5jBRhY-dxZ7YF-5jxAc2-5jBU9U-5jBTEy-ejbWe6-5jxBc6-99ENZW-99KUsi-9bWScw-5jBRow-5jxzmx-5jBTfw-r6HcW-dy5zXE-5jxzg4-5jxBYR-5jxA2B" target="_blank">image</a> by <a href="https://www.flickr.com/photos/16339684@N00/" target="_blank">internets_dairy</a></small>
	</p>
	<h3 id="synflood">SYN Flood</h3>
	<p><img src="https://blog.cloudflare.com/content/images/2018/01/full-syn-1.jpeg" alt="full-syn-1"></p>
	<p>If it's possible to overflow the Accept Queue, it must be possible to overflow the SYN Queue as well. What happens in that case?</p>
	<p>This is what <a href="https://en.wikipedia.org/wiki/SYN_flood" target="_blank">SYN Flood attacks</a> are all about. In the past flooding the SYN Queue with bogus spoofed SYN packets was a real problem. Before 1996 it was possible to successfully deny the service of almost any TCP server with very little bandwidth, just by filling the SYN Queues.</p>
	<p>The solution is <a href="https://lwn.net/Articles/277146/" target="_blank">SYN Cookies</a>. SYN Cookies are a construct that allows the SYN+ACK to be generated statelessly, without actually saving the inbound SYN and wasting system memory. SYN Cookies don't break legitimate traffic. When the other party is real, it will respond with a valid ACK packet including the reflected sequence number, which can be cryptographically verified.</p>
	<p>By default SYN Cookies are enabled when needed - for sockets with a filled up SYN Queue. Linux updates a couple of counters on SYN Cookies. When a SYN cookie is being sent out:</p>
	<ul>
		<li>TcpExtTCPReqQFullDoCookies / <code>LINUX_MIB_TCPREQQFULLDOCOOKIES</code> is incremented.</li>
		<li>TcpExtSyncookiesSent / <code>LINUX_MIB_SYNCOOKIESSENT</code> is incremented.</li>
		<li>Linux used to increment <code>TcpExtListenDrops</code> but <a href="https://github.com/torvalds/linux/commit/9caad864151e525929d323de96cad382da49c3b2" target="_blank">doesn't from kernel 4.7</a>.</li>
	</ul>
	<p>When an inbound ACK is heading into the SYN Queue with SYN cookies engaged:</p>
	<ul>
		<li>TcpExtSyncookiesRecv / <code>LINUX_MIB_SYNCOOKIESRECV</code> is incremented when crypto validation succeeds.</li>
		<li>TcpExtSyncookiesFailed / <code>LINUX_MIB_SYNCOOKIESFAILED</code> is incremented when crypto fails.</li>
	</ul>
	<p>A sysctl <code>net.ipv4.tcp_syncookies</code> can disable SYN Cookies or force-enable them. Default is good, don't change it.</p>
	<h3 id="syncookiesandtcptimestamps">SYN Cookies and TCP Timestamps</h3>
	<p>The SYN Cookies magic works, but isn't without disadvantages. The main problem is that there is very little data that can be saved in a SYN Cookie. Specifically, only 32 bits of the sequence number are returned in the ACK. These bits are used as follows:</p>
	<pre><code>+----------+--------+-------------------+
|  6 bits  | 2 bits |     24 bits       |
| t mod 32 |  MSS   | hash(ip, port, t) |
+----------+--------+-------------------+
</code></pre>
	<p>With the MSS setting <a href="https://github.com/torvalds/linux/blob/5bbcc0f595fadb4cac0eddc4401035ec0bd95b09/net/ipv4/syncookies.c#L142" target="_blank">truncated to only 4 distinct values</a>, Linux doesn't know any optional TCP parameters of the other party. Information about Timestamps, ECN, Selective ACK, or Window Scaling is lost, and can lead to degraded TCP session performance.</p>
	<p>Fortunately Linux has a work around. If TCP Timestamps are enabled, the kernel can reuse another slot of 32 bits in the Timestamp field. It contains:</p>
	<pre><code>+-----------+-------+-------+--------+
|  26 bits  | 1 bit | 1 bit | 4 bits |
| Timestamp |  ECN  | SACK  | WScale |
+-----------+-------+-------+--------+
</code></pre>
	<p>TCP Timestamps should be enabled by default, to verify see the sysctl:</p>
	<pre><code>$ sysctl net.ipv4.tcp_timestamps
net.ipv4.tcp_timestamps = 1
</code></pre>
	<p>Historically there was plenty of discussion about the usefulness of TCP Timestamps.</p>
	<ul>
		<li>In the past timestamps leaked server uptime (whether that matters is another discussion). This <a href="https://github.com/torvalds/linux/commit/95a22caee396cef0bb2ca8fafdd82966a49367bb" target="_blank">was fixed 8 months ago</a>.</li>
		<li>TCP Timestamps use <a href="http://highscalability.com/blog/2015/10/14/save-some-bandwidth-by-turning-off-tcp-timestamps.html" target="_blank">non-trivial amount of of bandwidth</a> - 12 bytes on each packet.</li>
		<li>They can add additional randomness to packet checksums which <a href="https://www.snellman.net/blog/archive/2017-07-20-s3-mystery/" target="_blank">can help with certain broken hardware</a>.</li>
		<li>As mentioned above TCP Timestamps can boost the performance of TCP connections if SYN Cookies are engaged.</li>
	</ul>
	<p>Currently at Cloudflare, we have TCP Timestamps disabled.</p>
	<p>Finally, with SYN Cookies engaged some cool features won't work - things like <a href="https://lwn.net/Articles/645128/" target="_blank"><code>TCP_SAVED_SYN</code></a>, <code>TCP_DEFER_ACCEPT</code> or <code>TCP_FAST_OPEN</code>.</p>
	<h3 id="synfloodsatcloudflarescale">SYN Floods at Cloudflare scale</h3>
	<p><img src="https://blog.cloudflare.com/content/images/2018/01/Screen-Shot-2016-12-02-at-10.53.27-1.png" alt="Screen-Shot-2016-12-02-at-10.53.27-1"></p>
	<p>SYN Cookies are a great invention and solve the problem of smaller SYN Floods. At Cloudflare though, we try to avoid them if possible. While sending out a couple of thousand of cryptographically verifiable SYN+ACK packets per second is okay, we see <a href="https://blog.cloudflare.com/the-daily-ddos-ten-days-of-massive-attacks/">attacks of more than 200 Million packets per second</a>. At this scale, our SYN+ACK responses would just litter the internet, bringing absolutely no benefit.</p>
	<p>Instead, we attempt to drop the malicious SYN packets on the firewall layer. We use the <code>p0f</code> SYN fingerprints compiled to BPF. Read more in this blog post <a href="https://blog.cloudflare.com/introducing-the-p0f-bpf-compiler/">Introducing the p0f BPF compiler</a>. To detect and deploy the mitigations we developed an automation system we call "Gatebot". We described that here <a href="https://blog.cloudflare.com/meet-gatebot-a-bot-that-allows-us-to-sleep/">Meet Gatebot - the bot that allows us to sleep</a></p>
	<h3 id="evolvinglandscape">Evolving landscape</h3>
	<p>For more - slightly outdated - data on the subject read on <a href="https://veithen.github.io/2014/01/01/how-tcp-backlog-works-in-linux.html" target="_blank">an excellent explanation by Andreas Veithen from 2015</a> and <a href="https://www.giac.org/paper/gsec/2013/syn-cookies-exploration/103486" target="_blank">a comprehensive paper by Gerald W. Gordon from 2013</a>.</p>
	<p>The Linux SYN packet handling landscape is constantly evolving. Until recently SYN Cookies were slow, due to an old fashioned lock in the kernel. This was fixed in 4.4 and now you can rely on the kernel to be able to send millions of SYN Cookies per second, practically solving the SYN Flood problem for most users. With proper tuning it's possible to mitigate even the most annoying SYN Floods without affecting the performance of legitimate connections.</p>
	<p>Application performance is also getting significant attention. Recent ideas like <code>SO_ATTACH_REUSEPORT_EBPF</code> introduce a whole new layer of programmability into the network stack.</p>
	<p>It's great to see innovations and fresh thinking funneled into the networking stack, in the otherwise stagnant world of operating systems.</p>
	<p><em>Thanks to Binh Le for helping with this post.</em></p>
	<hr>
	<p><em>Dealing with the internals of Linux and NGINX sound interesting? Join our <a href="https://boards.greenhouse.io/cloudflare/jobs/589572" target="_blank">world famous team</a> in London, Austin, San Francisco and our elite office in Warsaw, Poland.</em></p>
	<hr class="footnotes-sep">
	<section class="footnotes">
		<ol class="footnotes-list">
			<li id="fn1" class="footnote-item">
				<p>I'm simplifying, technically speaking the SYN Queue stores not yet ESTABLISHED connections, not SYN packets themselves. With <code>TCP_SAVE_SYN</code> it gets close enough though. <a href="#fnref1" class="footnote-backref">↩︎</a></p>
			</li>
			<li id="fn2" class="footnote-item">
				<p>If <a href="http://man7.org/linux/man-pages/man7/tcp.7.html" target="_blank"><code>TCP_DEFER_ACCEPT</code></a> is new to you, definitely check FreeBSD's version of it - <a href="https://www.freebsd.org/cgi/man.cgi?query=accf_http&amp;sektion=9" target="_blank">accept filters</a>. <a href="#fnref2" class="footnote-backref">↩︎</a></p>
			</li>
		</ol>
	</section>
	<!--kg-card-end: markdown-->
</div>