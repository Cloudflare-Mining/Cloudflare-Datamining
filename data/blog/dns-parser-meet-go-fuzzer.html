<div class="post-content lh-copy gray1">
	<!--kg-card-begin: markdown-->
	<p>Here at CloudFlare we are heavy users of the <a href="https://github.com/miekg/dns" target="_blank"><code>github.com/miekg/dns</code></a> Go DNS library and we make sure to contribute to its development as much as possible. Therefore when <a href="https://github.com/dvyukov" target="_blank">Dmitry Vyukov</a> published go-fuzz and started to uncover tens of bugs in the Go standard library, our task was clear.</p>
	<h3 id="hotfuzz">Hot Fuzz</h3>
	<p>Fuzzing is the technique of <em>testing software by continuously feeding it inputs that are automatically mutated</em>. For C/C++, the wildly successful <a href="http://lcamtuf.coredump.cx/afl/" target="_blank">afl-fuzz</a> tool by Michał Zalewski uses instrumented source coverage to judge which mutations pushed the program into new paths, <em>eventually hitting many rarely-tested branches</em>.</p>
	<p><em><a href="https://github.com/dvyukov/go-fuzz" target="_blank">go-fuzz</a> applies the same technique to Go programs</em>, instrumenting the source by rewriting it (<a href="https://blog.cloudflare.com/go-has-a-debugger-and-its-awesome/">like godebug does</a>). An interesting difference between afl-fuzz and go-fuzz is that the former normally operates on file inputs to unmodified programs, while the latter asks you to <em>write a Go function and passes inputs to that</em>. The former usually forks a new process for each input, the latter keeps calling the function without restarting often.</p>
	<p>There is no strong technical reason for this difference (and indeed afl recently gained the ability to behave like go-fuzz), but it's likely due to the <em>different ecosystems</em> in which they operate: Go programs often expose <em>well-documented, well-behaved APIs</em> which enable the tester to write a good wrapper that doesn't contaminate state across calls. Also, Go programs are often easier to dive into and <em>more predictable</em>, thanks obviously to GC and memory management, but also to the general community repulsion towards unexpected global states and side effects. On the other hand many legacy C code bases are so intractable that the easy and stable file input interface is worth the performance tradeoff.</p>
	<p>Back to our DNS library. RRDNS, our in-house DNS server, uses <code>github.com/miekgs/dns</code> for all its parsing needs, and it has proved to be up to the task. However, it's a bit fragile on the edge cases and has a track record of panicking on malformed packets. Thankfully, this is Go, not <a href="https://blog.cloudflare.com/a-deep-look-at-cve-2015-5477-and-how-cloudflare-virtual-dns-customers-are-protected/"><s>BIND</s></a> C, and we can afford to <code>recover()</code> panics without worrying about ending up with insane memory states. Here's what we are doing</p>
	<pre><code class="language-go">func ParseDNSPacketSafely(buf []byte, msg *old.Msg) (err error) {
	defer func() {
		panicked := recover()

		if panicked != nil {
			err = errors.New("ParseError")
		}
	}()

	err = msg.Unpack(buf)

	return
}
</code></pre>
	<p>We saw an opportunity to make the library more robust so we wrote this initial simple fuzzing function:</p>
	<pre><code class="language-go">func Fuzz(rawMsg []byte) int {
    msg := &amp;dns.Msg{}

    if unpackErr := msg.Unpack(rawMsg); unpackErr != nil {
        return 0
    }

    if _, packErr = msg.Pack(); packErr != nil {
        println("failed to pack back a message")
        spew.Dump(msg)
        panic(packErr)
    }

    return 1
}
</code></pre>
	<p>To create a corpus of initial inputs we took our stress and regression test suites and used <code>github.com/miekg/pcap</code> to write a file per packet.</p>
	<pre><code>package main

import (
	"crypto/rand"
	"encoding/hex"
	"log"
	"os"
	"strconv"

	"github.com/miekg/pcap"
)

func fatalIfErr(err error) {
	if err != nil {
		log.Fatal(err)
	}
}

func main() {
	handle, err := pcap.OpenOffline(os.Args[1])
	fatalIfErr(err)

	b := make([]byte, 4)
	_, err = rand.Read(b)
	fatalIfErr(err)
	prefix := hex.EncodeToString(b)

	i := 0
	for pkt := handle.Next(); pkt != nil; pkt = handle.Next() {
		pkt.Decode()

		f, err := os.Create("p_" + prefix + "_" + strconv.Itoa(i))
		fatalIfErr(err)
		_, err = f.Write(pkt.Payload)
		fatalIfErr(err)
		fatalIfErr(f.Close())

		i++
	}
}
</code></pre>
	<p><img src="https://blog.cloudflare.com/content/images/2015/08/11597106396_a1927f8c71_z.jpg" alt=""><small><a href="https://creativecommons.org/licenses/by/2.0/" target="_blank">CC BY 2.0</a> <a href="https://www.flickr.com/photos/jdhancock/11597106396/in/photolist-iENcGh-5GuU8g-4G3SzJ-cybzvf-ej9ytf-5PT2gy-2wCHkp-oTNLKN-4T5TVk-pikg74-64fbtb-64fbny-6iPZrk-6WSbWA-gTwR9P-6JEbMJ-uS5Qoe-p3LoLt-8rTRPb-gzJBbc-6u4Ko7-4uXbz8-bX4rtL-6HoBT8-cybFb7-pDtnkY-doskG2-a9tSqx-3NX4E-978gS2-4iW5fs-4VhKK2-7EpKqc-7EtB6Y-7EtAiN-7EpH54-7EpGgX-poTg8-55WEef-qfzP-dt83Gq-naDJvs-aCKhDG-drR492-aTSFS6-aTSEER-aTSC2t-aTSAUR-qqFXz2-ftsXnd" target="_blank">image</a> by <a href="https://www.flickr.com/photos/jdhancock/" target="_blank">JD Hancock</a></small></p>
	<p>We then compiled our <code>Fuzz</code> function with go-fuzz, and launched the fuzzer on a lab server. The first thing go-fuzz does is minimize the corpus by throwing away packets that trigger the same code paths, then it starts mutating the inputs and passing them to <code>Fuzz()</code> in a loop. The mutations that don't fail (<code>return 1</code>) and <em>expand code coverage</em> are kept and iterated over. When the program panics, a small report (input and output) is saved and the program restarted. If you want to learn more about go-fuzz watch <a href="https://www.youtube.com/watch?v=a9xrxRsIbSU" target="_blank">the author's GopherCon talk</a> or read <a href="https://github.com/dvyukov/go-fuzz" target="_blank">the README</a>.</p>
	<p><em>Crashes, mostly "index out of bounds", started to surface.</em> go-fuzz becomes pretty slow and ineffective when the program crashes often, so while the CPUs burned I started fixing the bugs.</p>
	<p>In some cases I just decided to change some parser patterns, for example <a href="https://github.com/miekg/dns/commit/b5133fead4c0571c20eea405a917778f011dde02" target="_blank">reslicing and using <code>len()</code> instead of keeping offsets</a>. However these can be potentially disrupting changes—I'm far from perfect—so I adapted the Fuzz function to keep an eye on the differences between the old and new, fixed parser, and crash if the new parser started refusing good packets or changed its behavior:</p>
	<pre><code>func Fuzz(rawMsg []byte) int {
    var (
        msg, msgOld = &amp;dns.Msg{}, &amp;old.Msg{}
        buf, bufOld = make([]byte, 100000), make([]byte, 100000)
        res, resOld []byte

        unpackErr, unpackErrOld error
        packErr, packErrOld     error
    )

    unpackErr = msg.Unpack(rawMsg)
    unpackErrOld = ParseDNSPacketSafely(rawMsg, msgOld)

    if unpackErr != nil &amp;&amp; unpackErrOld != nil {
        return 0
    }

    if unpackErr != nil &amp;&amp; unpackErr.Error() == "dns: out of order NSEC block" {
        // 97b0a31 - rewrite NSEC bitmap [un]packing to account for out-of-order
        return 0
    }

    if unpackErr != nil &amp;&amp; unpackErr.Error() == "dns: bad rdlength" {
        // 3157620 - unpackStructValue: drop rdlen, reslice msg instead
        return 0
    }

    if unpackErr != nil &amp;&amp; unpackErr.Error() == "dns: bad address family" {
        // f37c7ea - Reject a bad EDNS0_SUBNET family on unpack (not only on pack)
        return 0
    }

    if unpackErr != nil &amp;&amp; unpackErr.Error() == "dns: bad netmask" {
        // 6d5de0a - EDNS0_SUBNET: refactor netmask handling
        return 0
    }

    if unpackErr != nil &amp;&amp; unpackErrOld == nil {
        println("new code fails to unpack valid packets")
        panic(unpackErr)
    }

    res, packErr = msg.PackBuffer(buf)

    if packErr != nil {
        println("failed to pack back a message")
        spew.Dump(msg)
        panic(packErr)
    }

    if unpackErrOld == nil {

        resOld, packErrOld = msgOld.PackBuffer(bufOld)

        if packErrOld == nil &amp;&amp; !bytes.Equal(res, resOld) {
            println("new code changed behavior of valid packets:")
            println()
            println(hex.Dump(res))
            println(hex.Dump(resOld))
            os.Exit(1)
        }

    }

    return 1
}
</code></pre>
	<p>I was pretty happy about the robustness gain, but since we used the <code>ParseDNSPacketSafely</code> wrapper in RRDNS I didn't expect to find security vulnerabilities. I was wrong!</p>
	<p>DNS names are made of labels, usually shown separated by dots. In a space saving effort, labels can be replaced by pointers to other names, so that if we know we encoded <code>example.com</code> at offset 15, <code>www.example.com</code> can be packed as <code>www.</code> + <em>PTR(15)</em>. What we found is <a href="https://github.com/FiloSottile/dns/commit/b364f94" target="_blank">a bug in handling of pointers to empty names</a>: when encountering the end of a name (<code>0x00</code>), if no label were read, <code>"."</code> (the empty name) was returned as a special case. Problem is that this special case was unaware of pointers, and it would instruct the parser to resume reading from the end of the pointed-to empty name instead of the end of the original name.</p>
	<p>For example if the parser encountered at offset 60 a pointer to offset 15, and <code>msg[15] == 0x00</code>, parsing would then resume from offset 16 instead of 61, causing a infinite loop. This is a potential Denial of Service vulnerability.</p>
	<pre><code>A) Parse up to position 60, where a DNS name is found

| ... |  15  |  16  |  17  | ... |  58  |  59  |  60  |  61  |
| ... | 0x00 |      |      | ... |      |      | -&gt;15 |      |

-------------------------------------------------&gt;     

B) Follow the pointer to position 15

| ... |  15  |  16  |  17  | ... |  58  |  59  |  60  |  61  |
| ... | 0x00 |      |      | ... |      |      | -&gt;15 |      |

         ^                                        |
         ------------------------------------------      

C) Return a empty name ".", special case triggers

D) Erroneously resume from position 16 instead of 61

| ... |  15  |  16  |  17  | ... |  58  |  59  |  60  |  61  |
| ... | 0x00 |      |      | ... |      |      | -&gt;15 |      |

                 --------------------------------&gt;   

E) Rinse and repeat
</code></pre>
	<p>We sent the fixes privately to the library maintainer while we patched our servers and we <a href="https://github.com/miekg/dns/pull/237" target="_blank">opened a PR</a> once done. (Two bugs were independently found and fixed by Miek while we released our RRDNS updates, as it happens.)</p>
	<h3 id="notjustcrashesandhangs">Not just crashes and hangs</h3>
	<p>Thanks to its flexible fuzzing API, go-fuzz lends itself nicely not only to the mere search of crashing inputs, but <em>can be used to explore all scenarios where edge cases are troublesome</em>.</p>
	<p>Useful applications range from checking output validation by adding crashing assertions to your <code>Fuzz()</code> function, to comparing the two ends of a unpack-pack chain and even comparing the behavior of two different versions or implementations of the same functionality.</p>
	<p>For example, while preparing our <a href="https://blog.cloudflare.com/tag/dnssec/">DNSSEC</a> engine for launch, I faced a weird bug that would happen only on production or under stress tests: <em>NSEC records that were supposed to only have a couple bits set in their types bitmap would sometimes look like this</em></p>
	<pre><code>deleg.filippo.io.  IN  NSEC    3600    \000.deleg.filippo.io. NS WKS HINFO TXT AAAA LOC SRV CERT SSHFP RRSIG NSEC TLSA HIP TYPE60 TYPE61 SPF
</code></pre>
	<p>The catch was that our "pack and send" code <em>pools <code>[]byte</code> buffers to reduce GC and allocation churn</em>, so buffers passed to <code>dns.msg.PackBuffer(buf []byte)</code> can be "dirty" from previous uses.</p>
	<pre><code class="language-go">var bufpool = sync.Pool{
    New: func() interface{} {
        return make([]byte, 0, 2048)
    },
}

[...]

    data := bufpool.Get().([]byte)
    defer bufpool.Put(data)

    if data, err = r.Response.PackBuffer(data); err != nil {
</code></pre>
	<p>However, <code>buf</code> not being an array of zeroes was not handled by some <code>github.com/miekgs/dns</code> packers, including the NSEC rdata one, that would <em>just OR present bits, without clearing ones that are supposed to be absent</em>.</p>
	<pre><code class="language-go">case `dns:"nsec"`:
    lastwindow := uint16(0)
    length := uint16(0)
    for j := 0; j &lt; val.Field(i).Len(); j++ {
        t := uint16((fv.Index(j).Uint()))
        window := uint16(t / 256)
        if lastwindow != window {
            off += int(length) + 3
        }
        length = (t - window*256) / 8
        bit := t - (window * 256) - (length * 8)

        msg[off] = byte(window) // window #
        msg[off+1] = byte(length + 1) // octets length

        // Setting the bit value for the type in the right octet
---&gt;    msg[off+2+int(length)] |= byte(1 &lt;&lt; (7 - bit)) 

        lastwindow = window
    }
    off += 2 + int(length)
    off++
}
</code></pre>
	<p>The fix was clear and easy: we benchmarked a few different ways to zero a buffer and updated the code like this</p>
	<pre><code class="language-go">// zeroBuf is a big buffer of zero bytes, used to zero out the buffers passed
// to PackBuffer.
var zeroBuf = make([]byte, 65535)

var bufpool = sync.Pool{
    New: func() interface{} {
        return make([]byte, 0, 2048)
    },
}

[...]

    data := bufpool.Get().([]byte)
    defer bufpool.Put(data)
    copy(data[0:cap(data)], zeroBuf)

    if data, err = r.Response.PackBuffer(data); err != nil {
</code></pre>
	<p>Note: <a href="https://github.com/golang/go/commit/f03c9202c43e0abb130669852082117ca50aa9b1" target="_blank">a recent optimization</a> turns zeroing range loops into <code>memclr</code> calls, so once 1.5 lands that will be much faster than <code>copy()</code>.</p>
	<p>But this was a boring fix! Wouldn't it be nicer if we could trust our library to work with any buffer we pass it? Luckily, this is exactly what coverage based fuzzing is good for: <em>making sure all code paths behave in a certain way</em>.</p>
	<p>What I did then is write a <code>Fuzz()</code> function that would first parse a message, and then pack it to two different buffers: one filled with zeroes and one filled with <code>0xff</code>. <em>Any differences between the two results would signal cases where the underlying buffer is leaking into the output.</em></p>
	<pre><code>func Fuzz(rawMsg []byte) int {
    var (
        msg         = &amp;dns.Msg{}
        buf, bufOne = make([]byte, 100000), make([]byte, 100000)
        res, resOne []byte

        unpackErr, packErr error
    )

    if unpackErr = msg.Unpack(rawMsg); unpackErr != nil {
        return 0
    }

    if res, packErr = msg.PackBuffer(buf); packErr != nil {
        return 0
    }

    for i := range res {
        bufOne[i] = 1
    }

    resOne, packErr = msg.PackBuffer(bufOne)
    if packErr != nil {
        println("Pack failed only with a filled buffer")
        panic(packErr)
    }

    if !bytes.Equal(res, resOne) {
        println("buffer bits leaked into the packed message")
        println(hex.Dump(res))
        println(hex.Dump(resOne))
        os.Exit(1)
    }

    return 1
}
</code></pre>
	<p>I wish here, too, I could show a PR fixing all the bugs, but go-fuzz did its job even too well and we are still triaging and fixing what it finds.</p>
	<p>Anyway, once the fixes are done and go-fuzz falls silent, we will be free to drop the buffer zeroing step without worry, with no need to audit the whole codebase!</p>
	<p><em>Do you fancy fuzzing the libraries that serve 43 billion queries per day? We are <a href="https://www.cloudflare.com/join-our-team" target="_blank">hiring</a> in London, San Francisco and Singapore!</em></p>
	<!--kg-card-end: markdown-->
</div>