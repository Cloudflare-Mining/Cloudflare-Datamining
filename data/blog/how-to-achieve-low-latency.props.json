{
	"footerBlurb": "Cloudflare's connectivity cloud protects <a target='_blank' href='https://www.cloudflare.com/network-services/' rel='noreferrer'>entire corporate networks</a>, helps customers build <a target='_blank' href='https://workers.cloudflare.com/' rel='noreferrer'>Internet-scale applications efficiently</a>, accelerates any <a target='_blank' href='https://www.cloudflare.com/performance/accelerate-internet-applications/' rel='noreferrer'>website or Internet application</a>, <a target='_blank' href='https://www.cloudflare.com/ddos/' rel='noreferrer'>wards off DDoS attacks</a>, keeps <a target='_blank' href='https://www.cloudflare.com/application-security/' rel='noreferrer'>hackers at bay</a>, and can help you on <a target='_blank' href='https://www.cloudflare.com/products/zero-trust/' rel='noreferrer'>your journey to Zero Trust</a>.<br/><br/>Visit <a target='_blank' href='https://one.one.one.one/' rel='noreferrer'>1.1.1.1</a> from any device to get started with our free app that makes your Internet faster and safer.<br/><br/>To learn more about our mission to help build a better Internet, <a target='_blank' href='https://www.cloudflare.com/learning/what-is-cloudflare/' rel='noreferrer'>start here</a>. If you&apos;re looking for a new career direction, check out <a target='_blank' href='http://www.cloudflare.com/careers' rel='noreferrer'>our open positions</a>.",
	"initialReadingTime": "9",
	"locale": "en-us",
	"localesAvailable": [],
	"post": {
		"authors": [
			{
				"name": "Marek Majkowski",
				"slug": "marek-majkowski",
				"bio": null,
				"profile_image": "https://cf-assets.www.cloudflare.com/zkvhlag99gkb/1JuU5qavgwVeqR8BAUrd6U/3a0d0445d41c9a3c42011046efe9c37b/marek-majkowski.jpeg",
				"location": null,
				"website": null,
				"twitter": "@majek04",
				"facebook": null,
				"publiclyIndex": true
			}
		],
		"excerpt": "In a recent blog post we explained how to tweak a simple UDP application to maximize throughput. This time we are going to optimize our UDP application for latency. Fighting with latency is a great excuse to discuss modern features of multiqueue NICs. ",
		"feature_image": "https://cf-assets.www.cloudflare.com/zkvhlag99gkb/AXJDvFdC3bUiMT8BurbcL/2a7b6da20f7f3aac257f891d75ea8434/how-to-achieve-low-latency.jpg",
		"featured": false,
		"html": "<p>Good morning!</p><p>In <a href=\"/how-to-receive-a-million-packets/\">a recent blog post</a> we explained how to tweak a simple UDP application to maximize throughput. This time we are going to optimize our UDP application for <a href=\"https://www.cloudflare.com/learning/performance/glossary/what-is-latency/\">latency</a>. Fighting with latency is a great excuse to discuss modern features of multiqueue NICs. Some of the techniques covered here are also discussed in the <a href=\"https://www.kernel.org/doc/Documentation/networking/scaling.txt\">scaling.txt kernel document</a>.</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/6Yd58jloFHjTHG1fQjAkRB/3c087350b7ed34cb19ea1d27c02b5dca/queue.jpg\" alt=\"\" class=\"kg-image\" width=\"640\" height=\"426\" loading=\"lazy\"/>\n            \n            </figure><p>CC BY-SA 2.0 <a href=\"https://www.flickr.com/photos/hktang/4243300265/\">image</a> by Xiaojun Deng</p><p>Our experiment will be setup up as follows:</p><ul><li><p>We will have two physical Linux hosts: the &#39;client&#39; and the &#39;server&#39;. They communicate with a simple UDP echo protocol.</p></li><li><p>Client sends a small UDP frame (32 bytes of payload) and waits for the reply, measuring the <a href=\"https://www.cloudflare.com/learning/cdn/glossary/round-trip-time-rtt/\">round trip time (RTT)</a>. Server echoes back the packets immediately after they are received.</p></li><li><p>Both hosts have 2GHz Xeon CPU&#39;s, with two sockets of 6 cores and Hyper Threading (HT) enabled - so 24 CPUs per host.</p></li><li><p>The client has a Solarflare 10Gb NIC, the server has an Intel 82599 10Gb NIC. Both cards have fiber connected to a 10Gb switch.</p></li><li><p>We&#39;re going to measure the round trip time. Since the numbers are pretty small, there is a lot of jitter when counting the averages. Instead, it makes more sense to take a stable value - the lowest RTT from many runs done over one second.</p></li><li><p>As usual, code used here is available on GitHub: <a href=\"https://github.com/majek/dump/blob/master/how-to-receive-a-packet/udpclient.c\"><code>udpclient.c</code></a>, <a href=\"https://github.com/majek/dump/blob/master/how-to-receive-a-packet/udpserver.c\"><code>udpserver.c</code></a>.</p></li></ul>\n    <div class=\"flex anchor relative\">\n      <h3 id=\"prerequisites\">Prerequisites</h3>\n      <a href=\"#prerequisites\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>First, let&#39;s explicitly assign the IP addresses:</p>\n            <pre class=\"language-.bash\"><code class=\"language-.bash\">client$ ip addr add 192.168.254.1/24 dev eth2\nserver$ ip addr add 192.168.254.30/24 dev eth3</pre></code>\n            <p>Make sure <code>iptables</code> and <code>conntrack</code> don&#39;t interfere with our traffic:</p>\n            <pre class=\"language-.bash\"><code class=\"language-.bash\">client$ iptables -I INPUT 1 --src 192.168.254.0/24 -j ACCEPT\nclient$ iptables -t raw -I PREROUTING 1 --src 192.168.254.0/24 -j NOTRACK\nserver$ iptables -I INPUT 1 --src 192.168.254.0/24 -j ACCEPT\nserver$ iptables -t raw -I PREROUTING 1 --src 192.168.254.0/24 -j NOTRACK</pre></code>\n            <p>Finally, ensure the interrupts of multiqueue network cards are evenly distributed between CPUs. The <code>irqbalance</code> service is stopped and the interrupts are manually assigned. For simplicity let&#39;s pin the RX queue #0 to CPU #0, RX queue #1 to CPU #1 and so on.</p>\n            <pre class=\"language-.bash\"><code class=\"language-.bash\">client$ (let CPU=0; cd /sys/class/net/eth2/device/msi_irqs/;\n         for IRQ in *; do\n            echo $CPU &gt; /proc/irq/$IRQ/smp_affinity_list\n            let CPU+=1\n         done)\nserver$ (let CPU=0; cd /sys/class/net/eth3/device/msi_irqs/;\n         for IRQ in *; do\n            echo $CPU &gt; /proc/irq/$IRQ/smp_affinity_list\n            let CPU+=1\n         done)</pre></code>\n            <p>These scripts assign the interrupts fired by each RX queue to a selected CPU. Finally, some network cards have <a href=\"https://en.wikipedia.org/wiki/Ethernet_flow_control\">Ethernet flow control</a> enabled by default. In our experiment we won&#39;t push too many packets, so it shouldn&#39;t matter. In any case - it&#39;s unlikely the average user actually wants flow control - it can introduce unpredictable latency spikes during higher load:</p>\n            <pre class=\"language-.bash\"><code class=\"language-.bash\">client$ sudo ethtool -A eth2 autoneg off rx off tx off\nserver$ sudo ethtool -A eth3 autoneg off rx off tx off</pre></code>\n            \n    <div class=\"flex anchor relative\">\n      <h3 id=\"naive-round-trip\">Naive round trip</h3>\n      <a href=\"#naive-round-trip\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>Here&#39;s the sketch of our client code. Nothing too fancy: send a packet and measure the time until we hear the response.</p>\n            <pre class=\"language-.python\"><code class=\"language-.python\">fd = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\nfd.bind((&quot;0.0.0.0&quot;, 65400)) # pin source port to reduce nondeterminism\nfd.connect((&quot;192.168.254.30&quot;, 4321))\nwhile True:\n    t1 = time.time()\n    fd.sendmsg(&quot;\\x00&quot; * 32)\n    fd.readmsg()\n    t2 = time.time()\n    print &quot;rtt=%.3fus&quot; % ((t2-t1) * 1000000)</pre></code>\n            <p>The server is similarly uncomplicated. It waits for packets and echoes them back to the source:</p>\n            <pre class=\"language-.python\"><code class=\"language-.python\">fd = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\nfd.bind((&quot;0.0.0.0&quot;, 4321))\nwhile True:\n    data, client_addr = fd.recvmsg()\n    fd.sendmsg(data, client_addr)</pre></code>\n            <p>Let&#39;s run them:</p>\n            <pre class=\"language-.bash\"><code class=\"language-.bash\">server$ ./udpserver\nclient$ ./udpclient 192.168.254.30:4321\n[*] Sending to 192.168.254.30:4321, src_port=65500\npps= 16815 avg= 57.224us dev= 24.970us min=45.594us\npps= 15910 avg= 60.170us dev= 28.810us min=45.679us\npps= 15463 avg= 61.892us dev= 33.332us min=44.881us</pre></code>\n            <p>In the naive run we&#39;re able to do around 15k round trips a second, with an average RTT of 60 microseconds (us) and minimal 44us. The standard deviation is pretty scary and suggests high jitter.</p>\n    <div class=\"flex anchor relative\">\n      <h3 id=\"kernel-polling-so_busy_poll\">Kernel polling - SO_BUSY_POLL</h3>\n      <a href=\"#kernel-polling-so_busy_poll\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>Linux 3.11 added support for the <a href=\"http://www.intel.com/content/dam/www/public/us/en/documents/white-papers/open-source-kernel-enhancements-paper.pdf\"><code>SO_BUSY_POLL</code> socket option</a>. The idea is to ask the kernel to poll for incoming packets for <a href=\"http://man7.org/linux/man-pages/man7/socket.7.html\">a given amount of time</a>. This, of course, increases the CPU usage on the machine, but will reduce the latency. The benefit comes from avoiding the major context switch when a packet is received. In our experiment enabling <code>SO_BUSY_POLL</code> brings the min latency down by 7us:</p>\n            <pre class=\"language-.bash\"><code class=\"language-.bash\">server$ sudo ./udpserver  --busy-poll=50\nclient$ sudo ./udpclient 192.168.254.30:4321 --busy-poll=50\npps= 19440 avg= 49.886us dev= 16.405us min=36.588us\npps= 19316 avg= 50.224us dev= 15.764us min=37.283us\npps= 19024 avg= 50.960us dev= 18.570us min=37.116us</pre></code>\n            <p>While I&#39;m not thrilled by these numbers, there may be some valid use cases for <code>SO_BUSY_POLL</code>. To my understanding, as opposed to other approaches, it works well with interrupt coalescing (<code>rx-usecs</code>).</p>\n    <div class=\"flex anchor relative\">\n      <h3 id=\"userspace-busy-polling\">Userspace busy polling</h3>\n      <a href=\"#userspace-busy-polling\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>Instead of doing polling in the kernel with <code>SO_BUSY_POLL</code>, we can just do it in the application. Let&#39;s avoid blocking on <code>recvmsg</code> and run a non-blocking variant in a busy loop. The server pseudo code will look like:</p>\n            <pre class=\"language-.python\"><code class=\"language-.python\">while True:\n    while True:\n        data, client_addr = fd.recvmsg(MSG_DONTWAIT)\n        if data:\n            break\n    fd.sendmsg(data, client_addr)</pre></code>\n            <p>This approach is surprisingly effective:</p>\n            <pre class=\"language-.bash\"><code class=\"language-.bash\">server$ ./udpserver  --polling\nclient$ ./udpclient 192.168.254.30:4321 --polling\npps= 25812 avg= 37.426us dev= 11.865us min=31.837us\npps= 23877 avg= 40.399us dev= 14.665us min=31.832us\npps= 24746 avg= 39.086us dev= 14.041us min=32.540us</pre></code>\n            <p>Not only has the min time dropped by further 4us, but also the average and deviation look healthier.</p>\n    <div class=\"flex anchor relative\">\n      <h3 id=\"pin-processes\">Pin processes</h3>\n      <a href=\"#pin-processes\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>So far we allowed the Linux scheduler to allocate CPU for our busy polling applications. Some of the jitter came from the processes being moved around. Let&#39;s try pinning them to specific cores:</p>\n            <pre class=\"language-.bash\"><code class=\"language-.bash\">server$ taskset -c 3 ./udpserver --polling\nclient$ taskset -c 3 ./udpclient 192.168.254.30:4321 --polling\npps= 26824 avg= 35.879us dev= 11.450us min=30.060us\npps= 26424 avg= 36.464us dev= 12.463us min=30.090us\npps= 26604 avg= 36.149us dev= 11.321us min=30.421us</pre></code>\n            <p>This shaved off further 1us. Unfortunately running our applications on a &quot;bad&quot; CPU might actually degrade the numbers. To understand why we need to revisit how the packets are being dispatched across RX queues.</p>\n    <div class=\"flex anchor relative\">\n      <h3 id=\"rss-receive-side-scaling\">RSS - Receive Side Scaling</h3>\n      <a href=\"#rss-receive-side-scaling\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>In <a href=\"/how-to-receive-a-million-packets/\">the previous article</a> we mentioned that the NIC hashes packets in order to spread the load across many RX queues. This technique is called RSS - Receive Side Scaling. We can see it in action by observing the <code>/proc/net/softnet_stat</code> file with <a href=\"https://github.com/majek/dump/blob/master/how-to-receive-a-packet/softnet.sh\">the softnet.sh script</a>:</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/1fSRiGuv2ifzgyxdfm1yBi/1b2aa0c578a6851fcb50de600dffb8c2/softnet-rx.png\" alt=\"\" class=\"kg-image\" width=\"215\" height=\"213\" loading=\"lazy\"/>\n            \n            </figure><p>This makes sense - since we send only one flow (connection) from the <code>udpclient</code>, all the packets hit the same RX queue. In this case it is RX queue #1, which is bound to CPU #1. Indeed, when we run the client on that very CPU the latency goes up by around 2us:</p>\n            <pre class=\"language-.bash\"><code class=\"language-.bash\">client$ taskset -c 1 ./udpclient 192.168.254.30:4321  --polling\npps= 25517 avg= 37.615us dev= 12.551us min=31.709us\npps= 25425 avg= 37.787us dev= 12.090us min=32.119us\npps= 25279 avg= 38.041us dev= 12.565us min=32.235us</pre></code>\n            <p>Turns out that keeping the process on the same core as arriving interrupts slightly degrades the latency. But how can the process know if it&#39;s running on the offending CPU?</p><p>One way is for the application to query the kernel. Kernel 3.19 introduced <a href=\"https://lwn.net/Articles/619862/\"><code>SO_INCOMING_CPU</code></a> socket option. With that the process can figure out to which CPU the packet was originally delivered.</p><p>An alternative approach is to ensure the packets will be dispatched only to the CPUs we want. We can do that by adjusting NIC settings with <code>ethtool</code>.</p>\n    <div class=\"flex anchor relative\">\n      <h3 id=\"indirection-table\">Indirection Table</h3>\n      <a href=\"#indirection-table\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>RSS is intended to distribute the load across RX queues. In our case, a UDP flow, the RX queue is chosen by this formula:</p>\n            <pre class=\"language-.txt\"><code class=\"language-.txt\">RX_queue = INDIR[hash(src_ip, dst_ip) % 128]</pre></code>\n            <p>As discussed previously, the hash function for UDP flows is not configurable on Solarflare NICs. Fortunately the <code>INDIR</code> table is!</p><p>But what is <code>INDIR</code> anyway? Well, it&#39;s an indirection table that maps the least significant bits of a hash to an RX queue number. To view the indirection table run <code>ethtool -x</code>:</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/6aEJ2l39OFDhEOvDbK89md/58b46635f0132f5fc926ae82e500c0ad/indir-before.png\" alt=\"\" class=\"kg-image\" width=\"443\" height=\"294\" loading=\"lazy\"/>\n            \n            </figure><p>This reads as: packets with hash equal to, say, 72 will go to RX queue #6, hash of 126 will go to RX queue #5, and so on.</p><p>This table can be configured. For example, to make sure all traffic goes only to CPUs #0-#5 (the first NUMA node in our setup), we run:</p>\n            <pre class=\"language-.bash\"><code class=\"language-.bash\">client$ sudo ethtool -X eth2 weight 1 1 1 1 1 1 0 0 0 0 0\nserver$ sudo ethtool -X eth3 weight 1 1 1 1 1 1 0 0 0 0 0</pre></code>\n            <p>The adjusted indirection table:</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/60TeBiwuIiEE9xfAvQh4Fm/415f41806744c69dbe928fe997448964/indir-after.png\" alt=\"\" class=\"kg-image\" width=\"437\" height=\"292\" loading=\"lazy\"/>\n            \n            </figure><p>With that set up our latency numbers don&#39;t change much, but at least we are sure the packets will always hit only the CPUs on the first NUMA node. The lack of NUMA locality usually costs around 2us.</p><p>I should mention that Intel 82599 supports adjusting the RSS hash function for UDP flows, but the driver does not support indirection table manipulation. To get it running I <a href=\"https://github.com/majek/ixgbe/commit/336d556cbada2974700f4d809b54ce472d11709d\">used this patch</a>.</p>\n    <div class=\"flex anchor relative\">\n      <h3 id=\"flow-steering\">Flow steering</h3>\n      <a href=\"#flow-steering\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>There is also another way to ensure packets hit specific CPU: flow steering rules. Flow steering rules are used to specify exceptions on top of the indirection table. They tell the NIC to send specific flow to specific RX queue. For example, in our case, this is how we could pin our flows to RX queue #1 on both the server and the client:</p>\n            <pre class=\"language-.bash\"><code class=\"language-.bash\">client$ sudo ethtool -N eth2 flow-type udp4 dst-ip 192.168.254.1 dst-port 65500 action 1\nAdded rule with ID 12401\nserver$ sudo ethtool -N eth3 flow-type udp4 dst-port 4321 action 1\nAdded rule with ID 2045</pre></code>\n            <p>Flow steering can be used for more magical things. For example we can ask the NIC to drop selected traffic by specifying <code>action -1</code>. This is very useful during DDoS packet floods and is often a feasible alternative to dropping packets on a router firewall.</p><p>Some administrators also use flow steering to make sure things like <a href=\"https://www.cloudflare.com/learning/access-management/what-is-ssh/\">SSH</a> or BGP continue working even during <a href=\"https://en.wikipedia.org/wiki/Interrupt_storm\">high server load</a>. It can be achieved by manipulating the indirection table to move production traffic off, say, RX queue #0 and CPU #0. On top of that they explicitly forward packets going to destination port 22 or 179 to always hit only CPU #0 with flow steering rules. With that setup no matter how big the network load on the server, the SSH and <a href=\"https://www.cloudflare.com/learning/security/glossary/what-is-bgp/\">BGP</a> will keep on working since they hit a dedicated CPU and an RX queue.</p>\n    <div class=\"flex anchor relative\">\n      <h3 id=\"xfs-transmit-flow-steering\">XFS - Transmit flow steering</h3>\n      <a href=\"#xfs-transmit-flow-steering\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>Going back to our experiment, let&#39;s take a look at <code>/proc/interrupts</code>:</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/2sKCppzLcJoJYKesDSMU0L/f3b25006d9b3b90e886abbef75a931f3/interrupts-client.png\" alt=\"\" class=\"kg-image\" width=\"604\" height=\"201\" loading=\"lazy\"/>\n            \n            </figure><p>We can see the 27k interrupts corresponding to received packets on RX queue #1. But what about these 27k interrupts on RX queue #7? We surely disabled this RX queue with our RSS / indirection table setup.</p><p>It turns out these interrupts are caused by packet transmissions. As weird as it sounds, Linux has no way of figuring out what is the &quot;correct&quot; transmit queue to send packets on. To avoid packet reordering and to play safe by default, transmissions are distributed based on another flow hash. This pretty much guarantees that the transmissions will be done on a different CPU than the one running our application, therefore increasing the latency.</p><p>To ensure the transmissions are done on a local TX queue, Linux has a mechanism called <a href=\"http://patchwork.ozlabs.org/patch/62440/\">XFS</a>. To configure it we need to set a CPU mask for every TX queue. It&#39;s a bit more complex since we have 24 CPUs and only 11 TX queues:</p>\n            <pre class=\"language-.bash\"><code class=\"language-.bash\">XPS=(&quot;0 12&quot; &quot;1 13&quot; &quot;2 14&quot; &quot;3 15&quot; &quot;4 16&quot; &quot;5 17&quot; &quot;6 18&quot; &quot;7 19&quot; &quot;8 20&quot; &quot;9 21&quot; &quot;10 22 11 23&quot;);\n(let TX=0; for CPUS in &quot;$XPS[@]&quot;; do\n     let mask=0\n     for CPU in $CPUS; do let mask=$((mask | 1 &lt;&lt; $CPU)); done\n     printf %X $mask &gt; /sys/class/net/eth2/queues/tx-$TX/xps_cpus\n     let TX+=1\ndone)</pre></code>\n            <p>With XPS enabled and the CPU affinity carefully managed (to keep the process on different core than RX interrupts) I was able to squeeze 28us:</p>\n            <pre class=\"language-.txt\"><code class=\"language-.txt\">pps= 27613 avg= 34.724us dev= 12.430us min=28.536us\npps= 27845 avg= 34.435us dev= 12.050us min=28.379us\npps= 27046 avg= 35.365us dev= 12.577us min=29.234us</pre></code>\n            \n    <div class=\"flex anchor relative\">\n      <h3 id=\"rfs-receive-flow-steering\">RFS - Receive flow steering</h3>\n      <a href=\"#rfs-receive-flow-steering\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>While RSS solves the problem of balancing the load across many CPUs, it doesn&#39;t solve the problem of locality. In fact, the NIC has no understanding on which core the relevant application is waiting. This is where RFS kicks in.</p><p>RFS is a kernel technology that keeps the map of (flow_hash, CPU) for all ongoing flows. When a packet is received the kernel uses this map to quickly figure out to which CPU to direct the packet. Of course, if the application is rescheduled to another CPU the RFS will miss. If you use RFS, consider pinning the applications to specific cores.</p><p>To enable RFS on the client we run:</p>\n            <pre class=\"language-.bash\"><code class=\"language-.bash\">client$ echo 32768 &gt; /proc/sys/net/core/rps_sock_flow_entries\nclient$ for RX in `seq 0 10`; do\n            echo 2048 &gt; /sys/class/net/eth2/queues/rx-$RX/rps_flow_cnt\n        done</pre></code>\n            <p>To debug use sixth column <code>softnet.sh</code> script:</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/2u1j3D0bQeNf0yzBYFMOND/935835dd360dd100212ae42861da1312/softnet-rfs.png\" alt=\"\" class=\"kg-image\" width=\"510\" height=\"200\" loading=\"lazy\"/>\n            \n            </figure><p>I don&#39;t fully understand why, but enabling software RFS increases the latency by around 2us. On the other hand RFS is said to be effective in increasing throughput.</p>\n    <div class=\"flex anchor relative\">\n      <h3 id=\"rfs-on-solarflare-nics\">RFS on Solarflare NICs</h3>\n      <a href=\"#rfs-on-solarflare-nics\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>The software implementation of RFS is not perfect, since the kernel still needs to pass packets between CPUs. Fortunately, this can be improved with hardware support - it&#39;s called an Accelerated RFS. With network card drivers supporting ARFS, the kernel shares the (flow_hash, CPU) mapping with the NIC allowing it to correctly steer packets in hardware.</p><p>On Solarflare NICs <a href=\"https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Performance_Tuning_Guide/network-acc-rfs.html\">ARFS</a> is automatically enabled whenever RFS is enabled. This is visible in <code>/proc/interrupts</code>:</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/3TZoUo25i6N762Hrnku5ym/defa4d7988249069c6c4d3ff34f39ba4/interrupts-arfs.png\" alt=\"\" class=\"kg-image\" width=\"603\" height=\"150\" loading=\"lazy\"/>\n            \n            </figure><p>Here, I pinned the client process to CPU #2. As you see both receives and transmissions are visible on the same CPU. If the process was rescheduled to another CPU, the interrupts would &quot;follow&quot; it.</p><p>RFS works on any &quot;connected&quot; socket, whether TCP or UDP. In our case it will work on our <code>udpclient</code>, since it uses <code>connect()</code>, but it won&#39;t work on our <code>udpserver</code> since it sends messages directly on the <code>bind()</code> socket.</p>\n    <div class=\"flex anchor relative\">\n      <h3 id=\"rfs-on-intel-82599\">RFS on Intel 82599</h3>\n      <a href=\"#rfs-on-intel-82599\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>Intel drivers don&#39;t support ARFS, but they have their own branding of this technology called &quot;Flow Director&quot;. It&#39;s enabled by default, but the documentation is misleading and it won&#39;t work with <code>ntuple</code> routing enabled. To make it work run:</p>\n            <pre class=\"language-.bash\"><code class=\"language-.bash\">$ sudo ethtool -K eth3 ntuple off</pre></code>\n            <p>To debug its effectiveness look for <code>fdir_miss</code> and <code>fdir_match</code> in <code>ethtool</code> statistics:</p>\n            <pre class=\"language-.bash\"><code class=\"language-.bash\">$ sudo ethtool -S eth3|grep fdir\n     fdir_match: 7169438\n     fdir_miss: 128191066</pre></code>\n            <p>One more thing - Flow Director is implemented by the NIC driver, and is not integrated with the kernel. It has no way of truly knowing on which core the application actually lives. Instead, it takes a guess by looking at the transmitted packets every now and then. In fact it inspects every 20th (ATR setting) packet or <a href=\"https://github.com/majek/ixgbe/blob/master/src/ixgbe_main.c#L8568\">all packets with a SYN flag set</a>. Furthermore, it <a href=\"http://arxiv.org/pdf/1106.0443.pdf\">may introduce packet reordering</a> and <a href=\"http://sourceforge.net/p/e1000/mailman/message/27482089/\">doesn&#39;t support UDP</a>.</p>\n    <div class=\"flex anchor relative\">\n      <h3 id=\"low-level-tweaks\">Low level tweaks</h3>\n      <a href=\"#low-level-tweaks\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>Finally, our last resort in fighting the latency is to adjust a series of <a href=\"http://www.intel.com/content/dam/doc/application-note/82575-82576-82598-82599-ethernet-controllers-latency-appl-note.pdf\">low level network card</a> settings. <a href=\"https://en.wikipedia.org/wiki/Interrupt_coalescing\">Interrupt coalescing</a> is often used to reduce the number of interrupts fired by a network card, but as a result it adds some latency to the system. To disable interrupt coalescing:</p>\n            <pre class=\"language-.bash\"><code class=\"language-.bash\">client$ sudo ethtool -C eth2 rx-usecs 0\nserver$ sudo ethtool -C eth3 rx-usecs 0</pre></code>\n            <p>This reduces the latency by another couple of microseconds:</p>\n            <pre class=\"language-.txt\"><code class=\"language-.txt\">pps= 31096 avg= 30.703us dev=  8.635us min=25.681us\npps= 30809 avg= 30.991us dev=  9.065us min=25.833us\npps= 30179 avg= 31.659us dev=  9.978us min=25.735us</pre></code>\n            <p>Further tuning gets more complex. People recommend turning off advanced features like <a href=\"https://lwn.net/Articles/358910/\">GRO</a> or <a href=\"https://lwn.net/Articles/243949/\">LRO</a>, but I don&#39;t think they make much difference for our UDP application. Disabling them should improve the latency for TCP streams, but will harm the throughput.</p><p>An even more extreme option is to disable <a href=\"https://www.myricom.com/software/myri10ge/432-how-do-i-achieve-the-lowest-possible-latency-with-myri10ge.html\">C-sleep states</a> of the processor in BIOS.</p>\n    <div class=\"flex anchor relative\">\n      <h3 id=\"hardware-timestamps-with-so_timestampns\">Hardware timestamps with SO_TIMESTAMPNS</h3>\n      <a href=\"#hardware-timestamps-with-so_timestampns\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>On Linux we can retrieve a packet hardware timestamp with a <a href=\"https://www.kernel.org/doc/Documentation/networking/timestamping.txt\"><code>SO_TIMESTAMPNS</code> socket option</a>. Comparing that value with the wall clock allows us to measure the delay added by the kernel network stack:</p>\n            <pre class=\"language-.bash\"><code class=\"language-.bash\">client$ taskset -c 1 ./udpclient 192.168.254.30:4321 --polling --timestamp\npps=27564 avg=34.722us dev=14.836us min=26.828us packet=4.796us/1.622\npps=29385 avg=32.504us dev=10.670us min=26.897us packet=4.274us/1.415\npps=28679 avg=33.282us dev=12.249us min=26.106us packet=4.491us/1.440</pre></code>\n            <p>It takes the kernel between 4.3 and 5us to deliver a packet to our busy-polling application.</p>\n    <div class=\"flex anchor relative\">\n      <h3 id=\"openonload-for-the-extreme\">OpenOnload for the extreme</h3>\n      <a href=\"#openonload-for-the-extreme\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>Since we have a Solarflare network card handy, we can use the <a href=\"http://www.openonload.org/\">OpenOnload</a> kernel bypass technology to skip the kernel network stack all together:</p>\n            <pre class=\"language-.bash\"><code class=\"language-.bash\">client$ onload ./udpclient 192.168.254.30:4321 --polling --timestamp\npps=48881 avg=19.187us dev=1.401us min=17.660us packet=0.470us/0.457\npps=49733 avg=18.804us dev=1.306us min=17.702us packet=0.487us/0.390\npps=49735 avg=18.788us dev=1.220us min=17.654us packet=0.491us/0.466</pre></code>\n            <p>The latency reduction is one thing, but look at the deviation! OpenOnload reduces the time spent in packet delivery 10x, from 4.3us to 0.47us.</p>\n    <div class=\"flex anchor relative\">\n      <h3 id=\"final-notes\">Final notes</h3>\n      <a href=\"#final-notes\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>In this article we discussed what network latency to expect between two Linux hosts connected with 10Gb Ethernet. Without major tweaks it is possible to get 40us round trip time, with software tweaks like busy polling or CPU affinity, this can be trimmed to 30us. Reducing by further a 5us is harder, but can be done with some <code>ethtool</code> toggles.</p><p>Generally speaking it takes the kernel around 8us to deliver a packet to an idling application, and only ~4us to a busy-polling process. In our configuration it took around 4us to physically pass the packet between two network cards via a switch.</p><p>Low latency settings like low <code>rx-usecs</code> or disabled <code>LRO</code> may reduce throughput and increase the number of interrupts. This means tweaking the system for low latency makes it more vulnerable to denial of service problems.</p><p>While we haven&#39;t discussed TCP specifically, we&#39;ve covered ARFS and XFS, the techniques which increase data locality. Enabling them reduces cross-CPU chatter and is said to increase the throughput of TCP connections. In practice, for TCP the throughput is often way more important than latency.</p><p>For a general setup I would suggest having one TX queue for each CPU, enabling XFS and RFS/ARFS. To dispatch incoming packets effectively I recommend setting the RSS indirection table to skip CPU #0 and all the fake HT cores. To increase locality for RFS I recommend pinning applications to dedicated CPUs.</p><p><i>Interested in this sort of low-level, high-performance packet wrangling? CloudFlare is </i><a href=\"https://www.cloudflare.com/join-our-team\"><i>hiring</i></a><i> in London, San Francisco and Singapore</i>.</p>",
		"id": "61taurKfXvpnotezm1Uaeo",
		"localeList": {
			"name": "How to achieve low latency with 10Gbps Ethernet Config",
			"enUS": "English for Locale",
			"zhCN": "No Page for Locale",
			"zhHansCN": "No Page for Locale",
			"zhTW": "No Page for Locale",
			"frFR": "No Page for Locale",
			"deDE": "No Page for Locale",
			"itIT": "No Page for Locale",
			"jaJP": "No Page for Locale",
			"koKR": "No Page for Locale",
			"ptBR": "No Page for Locale",
			"esLA": "No Page for Locale",
			"esES": "No Page for Locale",
			"enAU": "No Page for Locale",
			"enCA": "No Page for Locale",
			"enIN": "No Page for Locale",
			"enGB": "No Page for Locale",
			"idID": "No Page for Locale",
			"ruRU": "No Page for Locale",
			"svSE": "No Page for Locale",
			"viVN": "No Page for Locale",
			"plPL": "No Page for Locale",
			"arAR": "No Page for Locale",
			"nlNL": "No Page for Locale",
			"thTH": "No Page for Locale",
			"trTR": "No Page for Locale",
			"heIL": "No Page for Locale",
			"lvLV": "No Page for Locale",
			"etEE": "No Page for Locale",
			"ltLT": "No Page for Locale"
		},
		"meta_description": null,
		"metadata": {
			"imgPreview": ""
		},
		"primary_author": {},
		"publicly_index": true,
		"published_at": "2015-06-30T12:38:28.000+01:00",
		"slug": "how-to-achieve-low-latency",
		"tags": [
			{
				"id": "6lhzEBz2B56RKa4nUEAGYJ",
				"name": "Programming",
				"slug": "programming"
			},
			{
				"id": "48r7QV00gLMWOIcM1CSDRy",
				"name": "Speed & Reliability",
				"slug": "speed-and-reliability"
			},
			{
				"id": "5NpgoTJYJjhgjSLaY7Gt3p",
				"name": "TCP",
				"slug": "tcp"
			}
		],
		"title": "How to achieve low latency with 10Gbps Ethernet",
		"updated_at": "2025-10-03T18:09:20.133Z",
		"url": "https://blog.cloudflare.com/how-to-achieve-low-latency"
	},
	"translations": {
		"posts.by": "By",
		"footer.gdpr": "GDPR",
		"lang_blurb1": "This post is also available in {lang1}.",
		"lang_blurb2": "This post is also available in {lang1} and {lang2}.",
		"lang_blurb3": "This post is also available in {lang1}, {lang2} and {lang3}.",
		"footer.press": "Press",
		"header.title": "The Cloudflare Blog",
		"search.clear": "Clear",
		"search.filter": "Filter",
		"search.source": "Source",
		"footer.careers": "Careers",
		"footer.company": "Company",
		"footer.support": "Support",
		"footer.the_net": "theNet",
		"search.filters": "Filters",
		"footer.our_team": "Our team",
		"footer.webinars": "Webinars",
		"page.more_posts": "More posts",
		"posts.time_read": "{time} min read",
		"search.language": "Language",
		"footer.community": "Community",
		"footer.resources": "Resources",
		"footer.solutions": "Solutions",
		"footer.trademark": "Trademark",
		"header.subscribe": "Subscribe",
		"footer.compliance": "Compliance",
		"footer.free_plans": "Free plans",
		"footer.impact_ESG": "Impact/ESG",
		"posts.follow_on_X": "Follow on X",
		"footer.help_center": "Help center",
		"footer.network_map": "Network Map",
		"header.please_wait": "Please Wait",
		"page.related_posts": "Related posts",
		"search.result_stat": "Results <strong>{search_range}</strong> of <strong>{search_total}</strong> for <strong>{search_keyword}</strong>",
		"footer.case_studies": "Case Studies",
		"footer.connect_2024": "Connect 2024",
		"footer.terms_of_use": "Terms of Use",
		"footer.white_papers": "White Papers",
		"footer.cloudflare_tv": "Cloudflare TV",
		"footer.community_hub": "Community Hub",
		"footer.compare_plans": "Compare plans",
		"footer.contact_sales": "Contact Sales",
		"header.contact_sales": "Contact Sales",
		"header.email_address": "Email Address",
		"page.error.not_found": "Page not found",
		"footer.developer_docs": "Developer docs",
		"footer.privacy_policy": "Privacy Policy",
		"footer.request_a_demo": "Request a demo",
		"page.continue_reading": "Continue reading",
		"footer.analysts_report": "Analyst reports",
		"footer.for_enterprises": "For enterprises",
		"footer.getting_started": "Getting Started",
		"footer.learning_center": "Learning Center",
		"footer.project_galileo": "Project Galileo",
		"pagination.newer_posts": "Newer Posts",
		"pagination.older_posts": "Older Posts",
		"posts.social_buttons.x": "Discuss on X",
		"search.icon_aria_label": "Search",
		"search.source_location": "Source/Location",
		"footer.about_cloudflare": "About Cloudflare",
		"footer.athenian_project": "Athenian Project",
		"footer.become_a_partner": "Become a partner",
		"footer.cloudflare_radar": "Cloudflare Radar",
		"footer.network_services": "Network services",
		"footer.trust_and_safety": "Trust & Safety",
		"header.get_started_free": "Get Started Free",
		"page.search.placeholder": "Search Cloudflare",
		"footer.cloudflare_status": "Cloudflare Status",
		"footer.cookie_preference": "Cookie Preferences",
		"header.valid_email_error": "Must be valid email.",
		"search.result_stat_empty": "Results <strong>{search_range}</strong> of <strong>{search_total}</strong>",
		"footer.connectivity_cloud": "Connectivity cloud",
		"footer.developer_services": "Developer services",
		"footer.investor_relations": "Investor relations",
		"page.not_found.error_code": "Error Code: 404",
		"search.autocomplete_title": "Insert a query. Press enter to send",
		"footer.logos_and_press_kit": "Logos & press kit",
		"footer.application_services": "Application services",
		"footer.get_a_recommendation": "Get a recommendation",
		"posts.social_buttons.reddit": "Discuss on Reddit",
		"footer.sse_and_sase_services": "SSE and SASE services",
		"page.not_found.outdated_link": "You may have used an outdated link, or you may have typed the address incorrectly.",
		"footer.report_security_issues": "Report Security Issues",
		"page.error.error_message_page": "Sorry, we can't find the page you are looking for.",
		"header.subscribe_notifications": "Subscribe to receive notifications of new posts:",
		"footer.cloudflare_for_campaigns": "Cloudflare for Campaigns",
		"header.subscription_confimation": "Subscription confirmed. Thank you for subscribing!",
		"posts.social_buttons.hackernews": "Discuss on Hacker News",
		"footer.diversity_equity_inclusion": "Diversity, equity & inclusion",
		"footer.critical_infrastructure_defense_project": "Critical Infrastructure Defense Project"
	}
}