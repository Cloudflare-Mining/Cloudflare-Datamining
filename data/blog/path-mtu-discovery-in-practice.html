<div class="mb2 gray5">5 min read</div><img class="mr2" src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/FKFVt8eR9TmTngTUD8ajx/132fa5c41574131f19c0a332d51e1cc4/path-mtu-discovery-in-practice.jpg" alt="">
<div class="post-content lh-copy gray1">
	<p>Last week, a very small number of our users who are using IP tunnels (primarily tunneling IPv6 over IPv4) were unable to access our services because a networking change broke "path MTU discovery" on our servers. In this article, I'll explain what path MTU discovery is, how we broke it, how we fixed it, and the open source code we used.</p>
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/1VOqwclfvRu2ER1ZcOorNo/e6221fd6a2d211946524d3e89066b467/11774786603_c3bfd2d22c_z.jpg" alt="Tunnel" class="kg-image" width="500" height="334" loading="lazy">

	</figure>
	<p>via <a href="https://www.flickr.com/photos/balintfoeldesi">Flickr</a></p>
	<h3>First There Was the Fragmentation</h3>
	<p>When a host on the Internet wants to send some data, it must know how to divide the data into packets. And in particular, it needs to know the maximum size of packet. The maximum size of a packet a host can send is called <a href="https://en.wikipedia.org/wiki/Maximum_transmission_unit">Maximum Transmission Unit: MTU</a>.</p>
	<p>The longer the MTU the better for performance, but the worse for reliability. This is because a lost packet means more data to be retransmitted and because many routers on the Internet can't deliver very long packets.</p>
	<p>The fathers of the Internet assumed that this problem would be solved at the IP layer with IP fragmentation. Unfortunately IP <a href="http://www.hpl.hp.com/techreports/Compaq-DEC/WRL-87-3.pdf">fragmentation has serious disadvantages</a>, and it's avoided in practice.</p>
	<h3>Do-not-fragment bit</h3>
	<p>To work around fragmentation problems the IP layer contains a "Don't Fragment" bit on every IP packet. It forbids any router on the path from performing fragmentation.</p>
	<p>So what does the router do if it can't deliver a packet and can't fragment it either?</p>
	<p>That's when the fun begins.</p>
	<p>According to <a href="https://tools.ietf.org/html/rfc1191">RFC1191</a>:</p>
	<blockquote>
		<p>When a router is unable to forward a datagram because it exceeds theMTU of the next-hop network and its Don't Fragment bit is set, therouter is required to return an ICMP Destination Unreachable messageto the source of the datagram, with the Code indicating"fragmentation needed and DF set".</p>
	</blockquote>
	<p>So a router must send <a href="http://www.networksorcery.com/enp/protocol/icmp/msg3.htm">ICMP type 3 code 4 message</a>. If you want to see one type:</p>
	<pre class="language-bash"><code class="language-bash">tcpdump -s0 -p -ni eth0 'icmp and icmp[0] == 3 and icmp[1] == 4'</code></pre>
	<p>This ICMP message is supposed to be delivered to the originating host, which in turn should adjust the MTU setting for that particular connection. This mechanism is called <a href="https://en.wikipedia.org/wiki/Path_MTU_Discovery">Path MTU Discovery</a>.</p>
	<p>In theory, it's great but unfortunately many things can go wrong when delivering ICMP packets. The most common problems are caused by misconfigured firewalls that drop ICMP packets.</p>
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/5MjJtXICfrunkPwdh3NnEB/ca04e67f09734ca399675d1d32e9f5eb/15084150039_b1bed17210_z.jpg" alt="Black Hole " class="kg-image" width="500" height="375" loading="lazy">

	</figure>
	<p>via <a href="https://www.flickr.com/photos/gsfc">Flickr</a></p>
	<h3>Another Black Hole</h3>
	<p>A situation when a router drops packets but for some reason can't deliver relevant ICMP messages is called <a href="https://en.wikipedia.org/wiki/Black_hole_(networking)">an ICMP black hole</a>.</p>
	<p>When that happens the whole connection gets stuck. The sending side constantly tries to resend lost packets, while the receiving side acknowledges only the small packets that get delivered.</p>
	<p>There are generally three solutions to this problem.</p>
	<p><i>1) Reduce the MTU on the client side.</i></p>
	<p>The network stack hints its MTU in SYN packets when it opens a TCP/IP connection. This is called the MSS TCP option. You can see it in tcpdump, for example on my host (notice "mss 1460"):</p>
	<pre class="language-bash"><code class="language-bash">$ sudo tcpdump -s0 -p -ni eth0 '(ip and ip[20+13] &amp; tcp-syn != 0)'
10:24:13.920656 IP 192.168.97.138.55991 &gt; 212.77.100.83.23: Flags [S], seq 711738977, win 29200, options [mss 1460,sackOK,TS val 6279040 ecr 0,nop,wscale 7], length 0</code></pre>
	<p>If you know you are behind a tunnel you might consider advising your operating system to reduce the advertised MTUs. In Linux it's as simple as (notice the <code>advmss 1400</code>):</p>
	<pre class="language-bash"><code class="language-bash">$ ip route change default via &lt;&gt; advmss 1400
$ sudo tcpdump -s0 -p -ni eth0 '(ip and ip[20+13] &amp; tcp-syn != 0)'
10:25:48.791956 IP 192.168.97.138.55992 &gt; 212.77.100.83.23: Flags [S], seq 389206299, win 28000, options [mss 1400,sackOK,TS val 6302758 ecr 0,nop,wscale 7], length 0</code></pre>
	<p><i>2) Reduce the MTU of all connections to the minimal value on the server side.</i></p>
	<p>It's a much harder problem to solve on the server side. A server can encounter clients with many different MTUs, and if you can't rely on Path MTU Discovery, you have a real problem. One commonly used workaround is to reduce the MTU for all of the outgoing packets. The minimal required <a href="https://www.ietf.org/rfc/rfc2460.txt">MTU for all IPv6 hosts is 1,280</a>, which is fair. Unfortunately for IPv4 the <a href="http://www.ietf.org/rfc/rfc791.txt">value is 576 bytes</a>. On the other hand <a href="http://www.ietf.org/rfc/rfc4821.txt">RFC4821</a> suggests that it's "probably safe enough" to assume minimal MTU of 1,024. To change the MTU setting you can type:</p>
	<pre class="language-bash"><code class="language-bash">ip -f inet6 route replace &lt;&gt; mtu 1280</code></pre>
	<p>(As a side note you could use <code>advmss</code> instead. The <code>mtu</code> setting is stronger, applying to all the packets, not only TCP. While <code>advmss</code> affects only the Path MTU hints given on TCP layer.)</p>
	<p>Forcing a reduced packet size is not an optimal solution though.</p>
	<p><i>3) Enable smart MTU black hole detection.</i></p>
	<p><a href="http://www.ietf.org/rfc/rfc4821.txt">RFC4821</a> proposes a mechanism to detect ICMP black holes and tries to adjust the path MTU in a smart way. To enable this on Linux type:</p>
	<pre class="language-bash"><code class="language-bash">echo 1 &gt; /proc/sys/net/ipv4/tcp_mtu_probing
echo 1024 &gt; /proc/sys/net/ipv4/tcp_base_mss</code></pre>
	<p>The second setting bumps the starting MSS used in discovery from a miserable default of 512 bytes to an RFC4821 suggested 1,024.</p>
	<h3>CloudFlare Architecture</h3>
	<p>To understand what happened last week, first we need to make a detour and discuss our architecture. At CloudFlare <a href="https://blog.cloudflare.com/cloudflares-architecture-eliminating-single-p">we use BGP in two ways</a>. We use it externally, the BGP that organizes the Internet, and internally between servers.</p>
	<p>The internal use of BGP allows us to get high availability across servers. If a machine goes down the BGP router will notice and will push the traffic to another server automatically.</p>
	<p>We've used this for a long time but last week increased its use in our network as we increased internal BGP load balancing. The load balancing mechanism is known as <a href="https://en.wikipedia.org/wiki/Equal-cost_multi-path_routing">ECMP: Equal Cost Multi Path routing</a>. From the configuration point of view the change was minimal, it only required adjusting the BGP metrics to equal values, the ECMP-enabled router does the rest of the work.</p>
	<p>But with ECMP the router must be somewhat smart. It can't do true load balancing, as packets from one TCP connections could end up on a wrong server. Instead in ECMP the router <a href="http://www.slideshare.net/SkillFactory/02-34638420">counts a hash from the usual tuple</a> extracted from every packet:</p>
	<ul>
		<li>
			<p>For TCP it hashes a tuple (src ip, src port, dst ip, dst port)</p>
		</li>
	</ul>
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/4TLVX51c8Cwvew4PFs20Ya/c94907c75563bd82026770110a1fe80e/ECMP-hashing-TCP-1.png" alt="ECMP hashing for tcp" class="kg-image" width="931" height="331" loading="lazy">

	</figure>
	<p>And uses this hash to chose destination server. This guarantees that packets from one "flow" will always hit the same server.</p>
	<h3>ECMP dislikes ICMP</h3>
	<p>ECMP will indeed forward TCP packets in a session to the appropriate server. Unfortunately, it has no special knowledge of ICMP and it hashes only (src ip, dst ip).</p>
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/4lHEwmHyDSt3UxvhO1t3uu/eb7fe22d983df3364b06db7178b2faf7/ECMP-hashing-ICMP.png" alt="ECMP hashing for icmp" class="kg-image" width="931" height="331" loading="lazy">

	</figure>
	<p>Where the the source IP is most likely an IP of a router on the Internet. In practice, this may sometimes end up delivering the ICMP packets to a different server than the one handling the TCP connection.</p>
	<p>This is exactly what happened in our case.</p>
	<h3>Temporary Fix</h3>
	<p>A couple of users who were accessing CloudFlare via IPv6 tunnels reported this problem. They were affected as almost every tunneling IPv6 has a reduced MTU.</p>
	<p>As a temporary fix we reduced the MTU for all IPv6 paths to 1,280 (solution mentioned as #2). Many other providers have the same problem and use this trick on IPv6, and never send IPv6 packets greater than 1,280 bytes.</p>
	<p>For better or worse this problem is not so prevalent on IPv4. Fewer people use IPv4 tunneling and the Path MTU problems were well understood for much longer. As temporary fix for IPv4, we deployed RFC4821 path MTU discovery (the solution mentioned in #3 above).</p>
	<h3>Our solution: PMTUD</h3>
	<p>In the meantime we were working on a comprehensive and reliable solution the restores the real MTUs. The idea is to broadcast ICMP MTU messages to all servers. That way we can guarantee that ICMP messages will hit the relevant server that handles a particular flow, no matter where the ECMP router decides to forward it.</p>
	<p>We're happy to open source our small "Path MTU Daemon" that does the work:</p>
	<ul>
		<li>
			<p><a href="https://github.com/cloudflare/pmtud">https://github.com/cloudflare/pmtud</a></p>
		</li>
	</ul>
	<h3>Summary</h3>
	<p>Delivering Path MTU ICMP messages correctly in a network that uses ECMP for internal routing is a surprisingly hard problem. While there is still plenty to improve we're happy to report:</p>
	<ul>
		<li>
			<p>Like many other companies we've reduced the MTU on IPv6 to safevalue of 1,280. We'll consider bumping it to improve performanceonce we get more confident that our other fixes are working asintended.</p>
		</li>
		<li>
			<p>We're rolling out PMTUD to ensure "proper" routing of ICMP's path MTU messages within our datacenters.</p>
		</li>
		<li>
			<p>We're enabling the Linux RFC4821 Path MTU Discovery for IPv4, whichshould help with true ICMP black holes.</p>
		</li>
	</ul>
	<p>Found this article interesting? <a href="https://www.cloudflare.com/join-our-team">Join our team</a>, including to our <a href="https://blog.cloudflare.com/cloudflare-london-were-hiring">elite office in London</a>. We're constantly looking for talented developers and network engineers!</p>
	<h4>Appendix</h4>
	<p>To simulate a black hole we used this iptables rule:</p>
	<pre class="language-bash"><code class="language-bash">iptables -I INPUT -s &lt;src_ip&gt; -m length --length 1400:1500 -j DROP</code></pre>
	<p>And this <a href="http://www.secdev.org/projects/scapy">Scapy</a> snippet to simulate MTU ICMP messages:</p>
	<pre class="language-python"><code class="language-python">#!/usr/bin/python
from scapy.all import *

def callback(pkt):
    pkt2 = IP(dst=pkt[IP].src) /
           ICMP(type=3,code=4,unused=1280) /
           str(pkt[IP])[:28]
    pkt2.show()
    send(pkt2)

if __name__ == '__main__':
    sniff(prn=callback,
          store=0,
          filter="ip and src &lt;src_ip&gt; and greater 1400")</code></pre>
</div>