<div class="mb2 gray5">7 min read</div>
<div class="post-content lh-copy gray1">
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/3E6lhMo3qjyAuKxbUDRZmq/dd3cf31af336f217bf709bd92af77f95/image4-4.png" alt="PIPEFAIL: How a missing shell option slowed Cloudflare down" class="kg-image" width="1200" height="675" loading="lazy">

	</figure>
	<p>At Cloudflare, we’re used to being the fastest in the world. However, for approximately 30 minutes last December, <a href="https://www.cloudflarestatus.com/incidents/qz5m74q8q0jl">Cloudflare was slow</a>. Between 20:10 and 20:40 UTC on December 16, 2021, web requests served by Cloudflare were artificially delayed by up to five seconds before being processed. This post tells the story of how a missing shell option called “pipefail” slowed Cloudflare down.</p>
	<div class="flex anchor relative">
		<h2 id="background">Background</h2>
		<a href="https://blog.cloudflare.com/#background" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>Before we can tell this story, we need to introduce you to some of its characters.</p>
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/2W5dv9p2aH8jQJ7TgRHg3k/623025487adbc9220f4b078cca110bb2/image7-3.png" alt="" class="kg-image" width="1999" height="1506" loading="lazy">

	</figure>
	<p>Cloudflare’s <b>Front Line</b> protects millions of users from some of the <a href="https://blog.cloudflare.com/cloudflare-thwarts-17-2m-rps-ddos-attack-the-largest-ever-reported">largest attacks</a> ever recorded. This protection is orchestrated by a sidecar service called <code><b>dosd</b></code>, which analyzes traffic and looks for attacks. When <code>dosd</code> detects an attack, it provides Front Line with a list of attack fingerprints that describe how Front Line can match and block the attack traffic.</p>
	<p>Instances of <code>dosd</code> run on every Cloudflare server, and they communicate with each other using a peer-to-peer mesh to identify malicious traffic patterns. This decentralized design allows <code>dosd</code> to perform analysis with much higher fidelity than is possible with a centralized system, but its scale also imposes some strict performance requirements. To meet these requirements, we need to provide <code>dosd</code> with very fast access to large amounts of configuration data, which naturally means that <code>dosd</code> depends on <b>Quicksilver</b>. Cloudflare developed <a href="https://blog.cloudflare.com/moving-quicksilver-into-production">Quicksilver</a> to manage configuration data and replicate it around the world in milliseconds, allowing it to be accessed by services like <code>dosd</code> in microseconds.</p>
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/5GMjP5VPXF8I00vVTs8TYG/afae530c5b580f969610bee1ba4238bf/image2-5.png" alt="" class="kg-image" width="1999" height="1506" loading="lazy">

	</figure>
	<p>One piece of configuration data that <code>dosd</code> needs comes from the <b>Addressing API</b>, which is our authoritative IP address management service. The addressing data it provides is important because <code>dosd</code> uses it to understand what kind of traffic is expected on particular IPs. Since addressing data doesn’t change very frequently, we use a simple <b>Kubernetes cron job</b> to query it at 10 minutes past each hour and write it into Quicksilver, allowing it to be efficiently accessed by <code>dosd</code>.</p>
	<p>With this context, let’s walk through the change we made on December 16 that ultimately led to the slowdown.</p>
	<div class="flex anchor relative">
		<h2 id="the-change">The Change</h2>
		<a href="https://blog.cloudflare.com/#the-change" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>Approximately once a week, all of our Bug Fixes and Performance Improvements to the Front Line codebase are released to the network. On December 16, the Front Line team released a fix for a subtle bug in how the code handled compression in the presence of a <code>Cache-Control: no-transform</code> header. Unfortunately, the team realized pretty quickly that this fix actually broke some customers who had started <i>depending</i> on that buggy behavior, so the team decided to roll back the release and work with those customers to correct the issue.</p>
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/2CY4wCLdkRVp38bNOgLSYc/458239ef68b60c77dde8ed58a4db8130/image3-5.png" alt="" class="kg-image" width="1999" height="560" loading="lazy">

	</figure>
	<p>Here’s a graph showing the progression of the rollback. While most releases and rollbacks are fully automated, this particular rollback needed to be performed manually due to its urgency. Since this was a manual rollback, SREs decided to perform it in two batches as a safety measure. The first batch went to our smaller tier 2 and 3 data centers, and the second batch went to our larger tier 1 data centers.</p>
	<p>SREs started the first batch at 19:25 UTC, and it completed in about 30 minutes. Then, after verifying that there were no issues, they started the second batch at 20:10. That’s when the slowdown started.</p>
	<div class="flex anchor relative">
		<h2 id="the-slowdown">The Slowdown</h2>
		<a href="https://blog.cloudflare.com/#the-slowdown" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>Within minutes of starting the second batch of rollbacks, alerts started firing. “Traffic levels are dropping.” “CPU utilization is dropping.” “A P0 incident has been automatically declared.” The timing could not be a coincidence. Somehow, a deployment of known-good code, which had been limited to a subset of the network and which had just been successfully performed 40 minutes earlier, appeared to be causing a global problem.</p>
	<p>A P0 incident is an “all hands on deck” emergency, so dozens of Cloudflare engineers quickly began to assess impact to their services and test their theories about the root cause. The rollback was paused, but that did not fix the problem. Then, approximately 10 minutes after the start of the incident, my team – the DOS team – received a concerning alert: “<code>dosd</code> is not running on numerous servers.” Before that alert fired we had been investigating whether the slowdown was caused by an unmitigated attack, but this required our immediate attention.</p>
	<p>Based on service logs, we were able to see that <code>dosd</code> was <a href="https://doc.rust-lang.org/book/ch09-03-to-panic-or-not-to-panic.html">panicking</a> because the customer addressing data in Quicksilver was corrupted in some way. Remember: the data in this Quicksilver key is important. Without it, <code>dosd</code> could not make correct choices anymore, so it refused to continue.</p>
	<p>Once we realized that the addressing data was corrupted, we had to figure out how it was corrupted so that we could fix it. The answer turned out to be pretty obvious: the Quicksilver key was completely empty.</p>
	<p>Following the old adage – “did you try restarting it?” – we decided to manually re-run the Kubernetes cron job that populates this key and see what happened. At 20:40 UTC, the cron job was manually triggered. Seconds after it completed, <code>dosd</code> started running again, and traffic levels began returning to normal. We confirmed that the Quicksilver key was no longer empty, and the incident was over.</p>
	<div class="flex anchor relative">
		<h2 id="the-aftermath">The Aftermath</h2>
		<a href="https://blog.cloudflare.com/#the-aftermath" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>Despite fixing the problem, we still didn’t really understand what had just happened.</p><!--kg-card-begin: html-->
	<h5>Why was the Quicksilver key empty?</h5><!--kg-card-end: html-->
	<p>It was urgent that we quickly figure out how an empty value was written into that Quicksilver key, because for all we knew, it could happen again at any moment.</p>
	<p>We started by looking at the Kubernetes cron job, which turned out to have a bug:</p>
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/3aUI8NLFqWrUDAwPCgITNq/684d8aa4209303d196d30bd50b1c4060/cron.png" alt="" class="kg-image" width="1600" height="438" loading="lazy">

	</figure>
	<p>This cron job is implemented using a small Bash script. If you’re unfamiliar with Bash (particularly <a href="https://en.wikipedia.org/wiki/Pipeline_(Unix)">shell pipelining</a>), here’s what it does:</p>
	<p>First, the <code>dos-make-addr-conf</code> executable runs. Its job is to query the Addressing API for various bits of JSON data and serialize it into a <a href="https://toml.io">Toml</a> document, which gets written Into <code>config.toml</code>. Afterward, that Toml is passed as input into the <code>dosctl</code> executable, whose job is to simply write it into a Quicksilver key called <code>template_vars</code>.</p>
	<p>Can you spot the bug? Here’s a hint: what happens if <code>dos-make-addr-conf</code> fails for some reason and exits with a non-zero error code? It turns out that, by default, the shell pipeline ignores the error code and continues executing! This means that the output of <code>dos-make-addr-conf</code> (which could be empty) gets unconditionally written into <code>dosctl</code> and used as the value of the <code>template_vars</code> key, regardless of whether <code>dos-make-addr-conf</code> succeeded or failed.</p>
	<p>30 years ago, when the first users of Bourne shell were burned by this problem, a shell option called “pipefail” was introduced. Enabling this option changes the shell’s behavior so that, when any command in a pipeline series fails, the entire pipeline fails. However, this option is not enabled by default, so it’s widely recommended as best practice that all scripts should start by enabling <a href="https://sipb.mit.edu/doc/safe-shell">this (and a few other) options</a>.</p>
	<p>Here’s the fixed version of that cron job:</p>
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/1T2JIQhn9ok8YfZPVrRBR3/ae39fe5b1ef24ca11e23afbae3c3ceaa/cron-fixed.png" alt="" class="kg-image" width="1600" height="496" loading="lazy">

	</figure>
	<p>This bug was particularly insidious because <code>dosd</code> actually did attempt to gracefully handle the case where this Quicksilver key contained invalid Toml. However, an empty string is a perfectly valid Toml document. If an error message had been accidentally written into this Quicksilver key instead of an empty string, then <code>dosd</code> would have rejected the update and continued to use the previous value.</p><!--kg-card-begin: html-->
	<h5>Why did that cause the Front Line to slow down?</h5><!--kg-card-end: html-->
	<p>We had figured out how an empty key could be written into Quicksilver, and we were confident that it wouldn’t happen again. However, we still needed to untangle how that empty key caused such a severe incident.</p>
	<p>As I mentioned earlier, the Front Line relies on <code>dosd</code> to tell it how to mitigate attacks, but it doesn’t depend on <code>dosd</code> directly to serve requests. Instead, once every few seconds, the Front Line asynchronously asks <code>dosd</code> for new attack fingerprints and stores them in an in-memory cache. This cache is consulted while serving each request, and if <code>dosd</code> ever fails to provide fresh attack fingerprints, then the stale fingerprints will continue to be used instead. So how could this have caused the impact that we saw?</p>
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/USGRrOwhXqmL8Aotx0Ydw/acc9230e82561705988a61f06702951a/image6-3.png" alt="" class="kg-image" width="1999" height="1506" loading="lazy">

	</figure>
	<p>As part of the rollback process, the Front Line’s code needed to be reloaded. Reloading this code implicitly flushed the in-memory caches, including the attack fingerprint data from <code>dosd</code>. The next time that a request tried to consult with the cache, the caching layer realized that it had no attack fingerprints to return and a “cache miss” happened.</p>
	<p>To handle a cache miss, the caching layer tried to reach out to <code>dosd</code>, and this is when the slowdown happened. While the caching layer was waiting for <code>dosd</code> to reply, it blocked all pending requests from progressing. Since <code>dosd</code> wasn’t running, the attempt eventually timed out after five seconds when the caching layer gave up. But in the meantime, each pending request was stuck waiting for the timeout to happen. Once it did, all the pending requests that were queued up over the five-second timeout period became unblocked and were finally allowed to progress. This cycle repeated over and over again every five seconds on every server until the <code>dosd</code> failure was resolved.</p>
	<p>To trigger this slowdown, not only did <code>dosd</code> have to fail, but the Front Line’s in-memory cache had to also be flushed at the same time. If <code>dosd</code> had failed, but the Front Line’s cache had not been flushed, then the stale attack fingerprints would have remained in the cache and request processing would not have been impacted.</p><!--kg-card-begin: html-->
	<h5>Why didn’t the first rollback cause this problem?</h5><!--kg-card-end: html-->
	<p>These two batches of rollbacks were performed by forcing servers to run a <a href="https://docs.saltproject.io/en/latest/ref/states/highstate.html">Salt highstate</a>. When each batch was executed, thousands of servers began running highstates at the same time. The highstate process involves, among other things, contacting the Addressing API in order to retrieve various bits of customer addressing information.</p>
	<p>The first rollback started at 19:25 UTC, and the second rollback started 45 minutes later at 20:10. Remember how I mentioned that our Kubernetes cron job only runs on the 10th minute of every hour? At 21:10 – exactly the time that our cron job started executing – thousands of servers also began to highstate, flooding the Addressing API with requests. All of these requests were queued up and <i>eventually</i> served, but it took the Addressing API a few minutes to work through the backlog. This delay was long enough to cause our cron job to time out, and, due to the “pipefail” &nbsp;bug, inadvertently clobber the Quicksilver key that it was responsible for updating.</p>
	<p>To trigger the “pipefail” bug, not only did we have to flood the Addressing API with requests, we also had to do it at exactly 10 minutes after the hour. If SREs had started the second batch of rollbacks a few minutes earlier or later, this bug would have continued to lay dormant.</p>
	<div class="flex anchor relative">
		<h2 id="lessons-learned">Lessons Learned</h2>
		<a href="https://blog.cloudflare.com/#lessons-learned" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>This was a unique incident where a chain of small or unlikely failures cascaded into a severe and painful outage that we deeply regret. In response, we have hardened each link in the chain:</p>
	<ul>
		<li>
			<p>A manual rollback inadvertently triggered the thundering herd problem, which overwhelmed the Addressing API. We have since significantly scaled out the Addressing API, so that it can handle high request rates if it ever again has to.</p>
		</li>
		<li>
			<p>An error in a Kubernetes cron job caused invalid data to be written to Quicksilver. We have since made sure that, when this cron job fails, it is no longer possible for that failure to clobber the Quicksilver key.</p>
		</li>
		<li>
			<p><code>dosd</code> did not correctly handle all possible error conditions when loading configuration data from Quicksilver, causing it to fail. We have since taken these additional conditions into account where necessary, so that <code>dosd</code> will gracefully degrade in the face of corrupt Quicksilver data.</p>
		</li>
		<li>
			<p>The Front Line had an unexpected dependency on <code>dosd</code>, which caused it to fail when <code>dosd</code> failed. We have since removed all such dependencies, and the Front Line will now gracefully survive <code>dosd</code> failures.</p>
		</li>
	</ul>
	<p>More broadly, this incident has served as an example to us of why code and systems must always be resilient to failure, no matter how unlikely that failure may seem.</p>
</div>