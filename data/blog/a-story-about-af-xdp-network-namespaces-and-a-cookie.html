<div class="mb2 gray5">10 min read</div><img class="mr2" src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/5OFIoNwSfuVrVI5RbqVOwu/97f354d394959acd5e013d11e5489cfd/a-story-about-af-xdp-network-namespaces-and-a-cookie.png" alt="">
<div class="post-content lh-copy gray1">
	<p></p>
	<p>A crash in a development version of <a href="https://blog.cloudflare.com/announcing-flowtrackd">flowtrackd</a> (the daemon that powers our <a href="https://developers.cloudflare.com/ddos-protection/managed-rulesets/tcp-protection">Advanced TCP Protection</a>) highlighted the fact that <a href="https://www.mankier.com/3/libxdp">libxdp</a> (and specifically the <a href="https://www.kernel.org/doc/html/latest/networking/af_xdp.html">AF_XDP</a> part) was not Linux <a href="https://man7.org/linux/man-pages/man7/network_namespaces.7.html">network namespace</a> aware.</p>
	<p>This blogpost describes the debugging journey to find the bug, as well as a fix.</p>
	<p><a href="https://blog.cloudflare.com/announcing-flowtrackd">flowtrackd</a> is a volumetric denial of service defense mechanism that sits in the <a href="https://www.cloudflare.com/magic-transit">Magic Transit</a> customer’s data path and protects the network from complex randomized TCP floods. It does so by challenging TCP connection establishments and by verifying that TCP packets make sense in an ongoing flow.</p>
	<p>It uses the Linux kernel <a href="https://www.kernel.org/doc/html/latest/networking/af_xdp.html">AF_XDP</a> feature to transfer packets from a network device in kernel space to a memory buffer in user space without going through the network stack. We use most of the helper functions of the <a href="https://github.com/libbpf/libbpf/tree/v0.8.0">C libbpf</a> with the <a href="https://github.com/libbpf/libbpf-sys">Rust bindings</a> to interact with AF_XDP.</p>
	<p>In our setup, both the ingress and the egress network interfaces are in different network namespaces. When a packet is determined to be valid (after a challenge or under some thresholds), it is forwarded to the second network interface.</p>
	<p>For the rest of this post the network setup will be the following:</p>
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/gnH2TUlwgdNAxFjrw0vL6/be9a5939c06f5d10924c1942a35ee869/image13-3.png" alt="flowtrackd network setup" class="kg-image" width="1933" height="500" loading="lazy">

	</figure>
	<p>e.g. eyeball packets arrive at the outer device in the root network namespace, they are picked up by flowtrackd and then forwarded to the inner device in the inner-ns namespace.</p>
	<div class="flex anchor relative">
		<h2 id="af_xdp">AF_XDP</h2>
		<a href="https://blog.cloudflare.com/#af_xdp" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>The kernel and the userspace share a memory buffer called the UMEM. This is where packet bytes are written to and read from.</p>
	<p>The UMEM is split in contiguous equal-sized "frames" that are referenced by "descriptors" which are just offsets from the start address of the UMEM.</p>
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/48r1ecV9BRfVD8wNc103jB/8fe3ca45fc83d557af8086c27cfe91fa/image12-2.png" alt="AF_XDP UMEM representation" class="kg-image" width="1999" height="1012" loading="lazy">

	</figure>
	<p>The interactions and synchronization between the kernel and userspace happen via a set of queues (circular buffers) as well as a socket from the AF_XDP family.</p>
	<p>Most of the work is about managing the ownership of the descriptors. Which descriptors the kernel owns and which descriptors the userspace owns.</p>
	<p>The interface provided for the ownership management are a set of queues:</p>
	<table>
		<tbody>
			<tr>
				<td>
					<p><b>Queue</b></p>
				</td>
				<td>
					<p><b>User space</b></p>
				</td>
				<td>
					<p><b>Kernel space</b></p>
				</td>
				<td>
					<p><b>Content description</b></p>
				</td>
			</tr>
			<tr>
				<td>
					<p>COMPLETION</p>
				</td>
				<td>
					<p>Consumes</p>
				</td>
				<td>
					<p>Produces</p>
				</td>
				<td>
					<p>Frame descriptors that have successfully been transmitted</p>
				</td>
			</tr>
			<tr>
				<td>
					<p>FILL</p>
				</td>
				<td>
					<p>Produces</p>
				</td>
				<td>
					<p>Consumes</p>
				</td>
				<td>
					<p>Frame descriptors ready to get new packet bytes written to</p>
				</td>
			</tr>
			<tr>
				<td>
					<p>RX</p>
				</td>
				<td>
					<p>Consumes</p>
				</td>
				<td>
					<p>Produces</p>
				</td>
				<td>
					<p>Frame descriptors of a newly received packet</p>
				</td>
			</tr>
			<tr>
				<td>
					<p>TX</p>
				</td>
				<td>
					<p>Produces</p>
				</td>
				<td>
					<p>Consumes</p>
				</td>
				<td>
					<p>Frame descriptors to be transmitted</p>
				</td>
			</tr>
		</tbody>
	</table>
	<p>When the UMEM is created, a FILL and a COMPLETION queue are associated with it.</p>
	<p>An RX and a TX queue are associated with the AF_XDP socket (abbreviated <b>Xsk</b>) at its creation. This particular socket is bound to a network device queue id. The userspace can then <code>poll()</code> on the socket to know when new descriptors are ready to be consumed from the RX queue and to let the kernel deal with the descriptors that were set on the TX queue by the application.</p>
	<p>The last plumbing operation to be done to use AF_XDP is to load a BPF program attached with XDP on the network device we want to interact with and insert the Xsk file descriptor into a BPF map (of type XSKMAP). Doing so will enable the BPF program to redirect incoming packets (with the <code>bpf_redirect_map()</code> <a href="https://man7.org/linux/man-pages/man7/bpf-helpers.7.html">function</a>) to a specific socket that we created in userspace:</p>
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/3K8DUwo8pARFEwtKmpRjp0/9b6e624d5d159b87e8bb3875f338312a/image4-9.png" alt="AF_XDP BPF redirect map action" class="kg-image" width="1999" height="782" loading="lazy">

	</figure>
	<p>Once everything has been allocated and strapped together, what I call "the descriptors dance" can start. While this has nothing to do with courtship behaviors it still requires a flawless execution:</p>
	<p>When the kernel receives a packet (more specifically the device driver), it will write the packet bytes to a UMEM frame (from a descriptor that the userspace put in the FILL queue) and then insert the frame descriptor in the RX queue for the userspace to consume. The userspace can then read the packet bytes from the received descriptor, take a decision, and potentially send it back to the kernel for transmission by inserting the descriptor in the TX queue. The kernel can then transmit the content of the frame and put the descriptor from the TX to the COMPLETION queue. The userspace can then "recycle" this descriptor in the FILL or TX queue.</p>
	<p>The overview of the queue interactions from the application perspective is represented on the following diagram (note that the queues contain descriptors that point to UMEM frames):</p>
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/5TkCEuedruPJCPfzWidBjc/a600b902fea7894d12006afb8c2d46a4/image7-6.png" alt="flowtrackd interactions with AF_XDP queues" class="kg-image" width="1999" height="919" loading="lazy">

	</figure>
	<div class="flex anchor relative">
		<h2 id="flowtrackd-i-o-rewrite-project">flowtrackd I/O rewrite project</h2>
		<a href="https://blog.cloudflare.com/#flowtrackd-i-o-rewrite-project" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>To increase flowtrackd performance and to be able to scale with the growth of the Magic Transit product we decided to rewrite the I/O subsystem.</p>
	<p>There will be a public blogpost about the technical aspects of the rewrite.</p>
	<p>Prior to the rewrite, each customer had a dedicated flowtrackd instance (Unix process) that attached itself to dedicated network devices. A dedicated UMEM was created per network device (see schema on the left side below). The packets were copied from one UMEM to the other.</p>
	<p>In this blogpost, we will only focus on the new usage of the <a href="https://www.kernel.org/doc/html/latest/networking/af_xdp.html#xdp-shared-umem-bind-flag">AF_XDP shared UMEM feature</a> which enables us to handle all customer accounts with a single flowtrackd instance per server and with a single shared UMEM (see schema on the right side below).</p>
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/5fzyIwrz5hCvFe5nvvPgf8/ed34ea06fa7d9f2ccc54e1df103ecde3/unnamed-4.png" alt="previous flowtrackd version next to the I/O rewrite one" class="kg-image" width="1600" height="1537" loading="lazy">

	</figure>
	<p>The Linux kernel documentation describes the additional plumbing steps to share a UMEM across multiple AF_XDP sockets:</p>
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/7jo3Fu7TMp3riMHpFS2JVB/172fee5a98ad24fae110e75750590fe5/image14-3.png" alt="XDP_SHARED_UMEM bind flag documentation" class="kg-image" width="1239" height="286" loading="lazy">

	</figure>
	<p>Followed by the instructions for our use case:</p>
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/1SnkkPjseYCtlkpImqFUqN/4505c75ffd7ff6f07f851425c4f0eabe/image9-4.png" alt="shared UMEM use case documentation" class="kg-image" width="1214" height="339" loading="lazy">

	</figure>
	<p>Hopefully for us a helper function in libbpf does it all for us: <code>xsk_socket__create_shared()</code></p>
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/X3bAf6b0i9A5MQ8rpPwEB/659c37c28c96d55f5d33b9abc6acdc4a/image10-2.png" alt="xsk_socket__create_shared() function documentation" class="kg-image" width="1230" height="186" loading="lazy">

	</figure>
	<p>The final setup is the following: Xsks are created for each queue of the devices in their respective network namespaces. flowtrackd then handles the descriptors like a puppeteer while applying our DoS mitigation logic on the packets that they reference with one exception… (notice the red crosses on the diagram):</p>
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/5Of32s5kJrl6seldfKEnI4/76b3dfb3e7b3dd4ab47ba1f4715ae422/unnamed1.png" alt="overview of flowtrackd interactions with Xsks in namespaces" class="kg-image" width="1600" height="646" loading="lazy">

	</figure>
	<div class="flex anchor relative">
		<h2 id="what-invalid-argument">What "Invalid argument" ??!</h2>
		<a href="https://blog.cloudflare.com/#what-invalid-argument" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>We were happily near the end of the rewrite when, suddenly, after porting our integration tests in the CI, flowtrackd crashed!</p>
	<p>The following errors was displayed:</p>
	<pre class="language-bash"><code class="language-bash">[...]
Thread 'main' panicked at 'failed to create Xsk: Libbpf("Invalid argument")', flowtrack-io/src/packet_driver.rs:144:22
note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace</code></pre>
	<p>According to the line number, the first socket was created with success and flowtrackd crashed when the second Xsk was created:</p>
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/5wSlomdCWIQcOjiFLpvtmk/c90313156ae25307fc0810f99e47e092/image11-3.png" alt="Rust Xsk instantiation code snippet" class="kg-image" width="1673" height="887" loading="lazy">

	</figure>
	<p>Here is what we do: we enter the network namespace where the interface sits, load and attach the BPF program and for each queue of the interface, we create a socket. The UMEM and the config parameters are the same with the ingress Xsk creation. Only the ingress_veth and egress_veth are different.</p>
	<p>This is what the code to create a Xsk looks like:</p>
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/2xd6YfIKTsCJOEq9OHGYfW/2b06a4be3d9ab9d64842d99b68b998c6/image3-6.png" alt="Rust Xsk creation code snippet" class="kg-image" width="1354" height="1168" loading="lazy">

	</figure>
	<p>The call to the libbpf function <a href="https://github.com/libbpf/libbpf/blob/v0.8.0/src/xsk.c#L994"><code>xsk_socket__create_shared()</code></a> didn't return 0.</p>
	<p>The <a href="https://www.mankier.com/3/libxdp">libxdp manual page</a> doesn't help us here…</p>
	<p>Which argument is "invalid"? And why is this error not showing up when we run flowtrackd locally but only in the CI?</p>
	<p>We can try to reproduce locally with a similar network setup script used in the CI:</p>
	<pre class="language-bash"><code class="language-bash">#!/bin/bash
 
set -e -u -x -o pipefail
 
OUTER_VETH=${OUTER_VETH:=outer}
TEST_NAMESPACE=${TEST_NAMESPACE:=inner-ns}
INNER_VETH=${INNER_VETH:=inner}
QUEUES=${QUEUES:=$(grep -c ^processor /proc/cpuinfo)}
 
ip link delete $OUTER_VETH &amp;&gt;/dev/null || true
ip netns delete $TEST_NAMESPACE &amp;&gt;/dev/null || true
ip netns add $TEST_NAMESPACE
ip link \
  add name $OUTER_VETH numrxqueues $QUEUES numtxqueues $QUEUES type veth \
  peer name $INNER_VETH netns $TEST_NAMESPACE numrxqueues $QUEUES numtxqueues $QUEUES
ethtool -K $OUTER_VETH tx off rxvlan off txvlan off
ip link set dev $OUTER_VETH up
ip addr add 169.254.0.1/30 dev $OUTER_VETH
ip netns exec $TEST_NAMESPACE ip link set dev lo up
ip netns exec $TEST_NAMESPACE ethtool -K $INNER_VETH tx off rxvlan off txvlan off
ip netns exec $TEST_NAMESPACE ip link set dev $INNER_VETH up
ip netns exec $TEST_NAMESPACE ip addr add 169.254.0.2/30 dev $INNER_VETH</code></pre>
	<p>For the rest of the blogpost, we set the number of queues per interface to 1. If you have questions about the set command in the script, <a href="https://blog.cloudflare.com/pipefail-how-a-missing-shell-option-slowed-cloudflare-down">check this out</a>.</p>
	<p>Not much success triggering the error.</p>
	<p>What differs between my laptop setup and the CI setup?</p>
	<p>I managed to find out that when the outer and inner interface <b>index numbers</b> are the same then it crashes. Even though the interfaces don't have the same name, and they are not in the same network namespace. When the tests are run by the CI, both interfaces got index number 5 which was not the case on my laptop since I have more interfaces:</p>
	<pre class="language-bash"><code class="language-bash">$ ip -o link | cut -d' ' -f1,2
1: lo:
2: wwan0:
3: wlo1:
4: virbr0:
7: br-ead14016a14c:
8: docker0:
9: br-bafd94c79ff4:
29: outer@if2:</code></pre>
	<p>We can edit the script to set a fixed interface index number:</p>
	<pre class="language-bash"><code class="language-bash">ip link \
  add name $OUTER_VETH numrxqueues $QUEUES numtxqueues $QUEUES index 4242 type veth \
  peer name $INNER_VETH netns $TEST_NAMESPACE numrxqueues $QUEUES numtxqueues $QUEUES index 4242</code></pre>
	<p>And we can now reproduce the issue locally!</p>
	<p>Interesting observation: I was not able to reproduce this issue with the previous flowtrackd version. Is this somehow related to the shared UMEM feature that we are now using?</p>
	<p>Back to the "invalid" argument. strace to the rescue:</p>
	<pre class="language-bash"><code class="language-bash">sudo strace -f -x ./flowtrackd -v -c flowtrackd.toml --ingress outer --egress inner --egress-netns inner-ns
 
[...]
 
// UMEM allocation + first Xsk creation
 
[pid 389577] brk(0x55b485819000)        = 0x55b485819000
[pid 389577] mmap(NULL, 8396800, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f85037fe000
 
[pid 389577] socket(AF_XDP, SOCK_RAW|SOCK_CLOEXEC, 0) = 9
[pid 389577] setsockopt(9, SOL_XDP, XDP_UMEM_REG, "\x00\xf0\x7f\x03\x85\x7f\x00\x00\x00\x00\x80\x00\x00\x00\x00\x00\x00\x08\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00", 32) = 0
[pid 389577] setsockopt(9, SOL_XDP, XDP_UMEM_FILL_RING, [2048], 4) = 0
[pid 389577] setsockopt(9, SOL_XDP, XDP_UMEM_COMPLETION_RING, [2048], 4) = 0
[pid 389577] getsockopt(9, SOL_XDP, XDP_MMAP_OFFSETS, "\x00\x00\x00\x00\x00\x00\x00\x00\x80\x00\x00\x00\x00\x00\x00\x00\x40\x01\x00\x00\x00\x00\x00\x00\xc4\x00\x00\x00\x00\x00\x00\x00"..., [128]) = 0
[pid 389577] mmap(NULL, 16704, PROT_READ|PROT_WRITE, MAP_SHARED|MAP_POPULATE, 9, 0x100000000) = 0x7f852801b000
[pid 389577] mmap(NULL, 16704, PROT_READ|PROT_WRITE, MAP_SHARED|MAP_POPULATE, 9, 0x180000000) = 0x7f8528016000
[...]
[pid 389577] setsockopt(9, SOL_XDP, XDP_RX_RING, [2048], 4) = 0
[pid 389577] setsockopt(9, SOL_XDP, XDP_TX_RING, [2048], 4) = 0
[pid 389577] getsockopt(9, SOL_XDP, XDP_MMAP_OFFSETS, "\x00\x00\x00\x00\x00\x00\x00\x00\x80\x00\x00\x00\x00\x00\x00\x00\x40\x01\x00\x00\x00\x00\x00\x00\xc4\x00\x00\x00\x00\x00\x00\x00"..., [128]) = 0
[pid 389577] mmap(NULL, 33088, PROT_READ|PROT_WRITE, MAP_SHARED|MAP_POPULATE, 9, 0) = 0x7f850377e000
[pid 389577] mmap(NULL, 33088, PROT_READ|PROT_WRITE, MAP_SHARED|MAP_POPULATE, 9, 0x80000000) = 0x7f8503775000
[pid 389577] bind(9, {sa_family=AF_XDP, sa_data="\x08\x00\x92\x10\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00"}, 16) = 0
 
[...]
 
// Second Xsk creation
 
[pid 389577] socket(AF_XDP, SOCK_RAW|SOCK_CLOEXEC, 0) = 62
[...]
[pid 389577] setsockopt(62, SOL_XDP, XDP_RX_RING, [2048], 4) = 0
[pid 389577] setsockopt(62, SOL_XDP, XDP_TX_RING, [2048], 4) = 0
[pid 389577] getsockopt(62, SOL_XDP, XDP_MMAP_OFFSETS, "\x00\x00\x00\x00\x00\x00\x00\x00\x80\x00\x00\x00\x00\x00\x00\x00\x40\x01\x00\x00\x00\x00\x00\x00\xc4\x00\x00\x00\x00\x00\x00\x00"..., [128]) = 0
[pid 389577] mmap(NULL, 33088, PROT_READ|PROT_WRITE, MAP_SHARED|MAP_POPULATE, 62, 0) = 0x7f85036e4000
[pid 389577] mmap(NULL, 33088, PROT_READ|PROT_WRITE, MAP_SHARED|MAP_POPULATE, 62, 0x80000000) = 0x7f85036db000
[pid 389577] bind(62, {sa_family=AF_XDP, sa_data="\x01\x00\x92\x10\x00\x00\x00\x00\x00\x00\x09\x00\x00\x00"}, 16) = -1 EINVAL (Invalid argument)
 
[pid 389577] munmap(0x7f85036db000, 33088) = 0
[pid 389577] munmap(0x7f85036e4000, 33088) = 0
[pid 389577] close(62)                  = 0
[pid 389577] write(2, "thread '", 8thread ')    = 8
[pid 389577] write(2, "main", 4main)        = 4
[pid 389577] write(2, "' panicked at '", 15' panicked at ') = 15
[pid 389577] write(2, "failed to create Xsk: Libbpf(\"In"..., 48failed to create Xsk: Libbpf("Invalid argument")) = 48
[...]</code></pre>
	<p>Ok, the second <code>bind() syscall</code> returns the EINVAL value.</p>
	<p>The <code>sa_family</code> is the right one. Is something wrong with <code>sa_data="\x01\x00\x92\x10\x00\x00\x00\x00\x00\x00\x09\x00\x00\x00"</code> ?</p>
	<p>Let's look at the <a href="https://elixir.bootlin.com/linux/v5.15/source/net/socket.c#L1702">bind syscall kernel code</a>:</p>
	<pre class="language-bash"><code class="language-bash">err = sock-&gt;ops-&gt;bind(sock, (struct sockaddr *) &amp;address, addrlen);</code></pre>
	<p>The bind function of the protocol specific socket operations gets called. Searching for "AF_XDP" in the code, we quickly found the bind function call related to the <a href="https://elixir.bootlin.com/linux/v5.15/source/net/xdp/xsk.c#L857">AF_XDP socket address family</a>.</p>
	<p>So, where in the syscall could this value be returned?</p>
	<p>First, let's examine the syscall parameters to see if the libbpf <code>xsk_socket__create_shared()</code> function sets weird values for us.</p>
	<p>We use the <a href="https://linux.die.net/man/1/pahole">pahole</a> tool to print the structure definitions:</p>
	<pre class="language-bash"><code class="language-bash">$ pahole sockaddr
struct sockaddr {
        sa_family_t                sa_family;            /*     0     2 */
        char                       sa_data[14];          /*     2    14 */
 
        /* size: 16, cachelines: 1, members: 2 */
        /* last cacheline: 16 bytes */
};
 
$ pahole sockaddr_xdp
struct sockaddr_xdp {
        __u16                      sxdp_family;          /*     0     2 */
        __u16                      sxdp_flags;           /*     2     2 */
        __u32                      sxdp_ifindex;         /*     4     4 */
        __u32                      sxdp_queue_id;        /*     8     4 */
        __u32                      sxdp_shared_umem_fd;  /*    12     4 */
 
        /* size: 16, cachelines: 1, members: 5 */
        /* last cacheline: 16 bytes */
};</code></pre>
	<p>Translation of the arguments of the bind syscall (the 14 bytes of <code>sa_data</code>) for the first <code>bind()</code> call:</p>
	<table>
		<tbody>
			<tr>
				<td>
					<p><b>Struct member</b></p>
				</td>
				<td>
					<p><b>Big Endian value</b></p>
				</td>
				<td>
					<p><b>Decimal</b></p>
				</td>
				<td>
					<p><b>Meaning</b></p>
				</td>
				<td>
					<p><b>Observation</b></p>
				</td>
			</tr>
			<tr>
				<td>
					<p><a href="http://web.archive.org/web/20220814030152/https://elixir.bootlin.com/linux/v5.15/source/include/uapi/linux/if_xdp.h#L15">sxdp_flags</a></p>
				</td>
				<td>
					<p>\x08\x00</p>
				</td>
				<td>
					<p>8</p>
				</td>
				<td>
					<p>XDP_USE_NEED_WAKEUP</p>
				</td>
				<td>
					<p>expected</p>
				</td>
			</tr>
			<tr>
				<td>
					<p>sxdp_ifindex</p>
				</td>
				<td>
					<p>\x92\x10\x00\x00</p>
				</td>
				<td>
					<p>4242</p>
				</td>
				<td>
					<p>The network interface index</p>
				</td>
				<td>
					<p>expected</p>
				</td>
			</tr>
			<tr>
				<td>
					<p>sxdp_queue_id</p>
				</td>
				<td>
					<p>\x00\x00\x00\x00</p>
				</td>
				<td>
					<p>0</p>
				</td>
				<td>
					<p>The network interface queue </p>
				</td>
				<td>
					<p>expected</p>
				</td>
			</tr>
			<tr>
				<td>
					<p>sxdp_shared_umem_fd</p>
				</td>
				<td>
					<p>\x00\x00\x00\x00</p>
				</td>
				<td>
					<p>0</p>
				</td>
				<td>
					<p>The umem is not shared yet</p>
				</td>
				<td>
					<p>expected</p>
				</td>
			</tr>
		</tbody>
	</table>
	<p>Second <code>bind()</code>call:</p>
	<table>
		<tbody>
			<tr>
				<td>
					<p>Struct member</p>
				</td>
				<td>
					<p>Big Endian value</p>
				</td>
				<td>
					<p>Decimal</p>
				</td>
				<td>
					<p>Meaning</p>
				</td>
				<td>
					<p>Observation</p>
				</td>
			</tr>
			<tr>
				<td>
					<p><a href="http://web.archive.org/web/20220814030152/https://elixir.bootlin.com/linux/v5.15/source/include/uapi/linux/if_xdp.h#L15">sxdp_flags</a></p>
				</td>
				<td>
					<p>\x01\x00</p>
				</td>
				<td>
					<p>1</p>
				</td>
				<td>
					<p>XDP_SHARED_UMEM</p>
				</td>
				<td>
					<p>expected</p>
				</td>
			</tr>
			<tr>
				<td>
					<p>sxdp_ifindex</p>
				</td>
				<td>
					<p>\x92\x10\x00\x00</p>
				</td>
				<td>
					<p>4242</p>
				</td>
				<td>
					<p>The network interface index</p>
				</td>
				<td>
					<p>expected</p>
				</td>
			</tr>
			<tr>
				<td>
					<p>sxdp_queue_id</p>
				</td>
				<td>
					<p>\x00\x00\x00\x00</p>
				</td>
				<td>
					<p>0</p>
				</td>
				<td>
					<p>The network interface queue id</p>
				</td>
				<td>
					<p>expected</p>
				</td>
			</tr>
			<tr>
				<td>
					<p>sxdp_shared_umem_fd</p>
				</td>
				<td>
					<p>\x09\x00\x00\x00</p>
				</td>
				<td>
					<p>9</p>
				</td>
				<td>
					<p>File descriptor of the first AF_XDP socket associated to the UMEM</p>
				</td>
				<td>
					<p>expected</p>
				</td>
			</tr>
		</tbody>
	</table>
	<p>The arguments look good...</p>
	<p>We could statically try to infer where the EINVAL was returned looking at the source code. But this analysis has its limits and can be error-prone.</p>
	<p>Overall, it seems that the network namespaces are not taken into account somewhere because it seems that there is some confusion with the interface indexes.</p>
	<p>Is the issue on the kernel-side?</p>
	<div class="flex anchor relative">
		<h2 id="digging-deeper">Digging deeper</h2>
		<a href="https://blog.cloudflare.com/#digging-deeper" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>It would be nice if we had step-by-step runtime inspection of code paths and variables.</p>
	<p>Let's:</p>
	<ul>
		<li>
			<p>Compile a Linux kernel version closer to the one used on our servers (5.15) with debug symbols.</p>
		</li>
		<li>
			<p>Generate a root filesystem for the kernel to boot.</p>
		</li>
		<li>
			<p>Boot in <a href="https://www.qemu.org">QEMU</a>.</p>
		</li>
		<li>
			<p>Attach gdb to it and set a breakpoint on the syscall.</p>
		</li>
		<li>
			<p>Check where the EINVAL value is returned.</p>
		</li>
	</ul>
	<p>We could have used <a href="https://buildroot.org">buildroot</a> with a minimal reproduction code, but it wasn't funny enough. Instead, we install a minimal Ubuntu and load our custom kernel. This has the benefit of having a package manager if we need to install other debugging tools.</p>
	<p>Let's install a minimal Ubuntu server 21.10 (with ext4, no LVM and an ssh server selected in the installation wizard):</p>
	<pre class="language-bash"><code class="language-bash">qemu-img create -f qcow2 ubuntu-21.10-live-server-amd64.qcow2 20G
 
qemu-system-x86_64 \
  -smp $(nproc) \
  -m 4G \
  -hda ubuntu-21.10-live-server-amd64.qcow2 \
  -cdrom /home/bastien/Downloads/ubuntu-21.10-live-server-amd64.iso \
  -enable-kvm \
  -cpu host \
  -net nic,model=virtio \
  -net user,hostfwd=tcp::10022-:22</code></pre>
	<p>And then build a kernel (<a href="https://www.josehu.com/memo/2021/01/02/linux-kernel-build-debug.html">link</a> and <a href="https://www.starlab.io/blog/using-gdb-to-debug-the-linux-kernel">link</a>) with the following changes in the menuconfig:</p>
	<ul>
		<li>
			<p>Cryptographic API → Certificates for signature checking → Provide system-wide ring of trusted keys</p>
			<ul>
				<li>
					<p>change the additional string to be EMPTY ("")</p>
				</li>
			</ul>
		</li>
		<li>
			<p>Device drivers → Network device support → Virtio network driver</p>
			<ul>
				<li>
					<p>Set to Enable</p>
				</li>
			</ul>
		</li>
		<li>
			<p>Device Drivers → Network device support → Virtual ethernet pair device</p>
			<ul>
				<li>
					<p>Set to Enable</p>
				</li>
			</ul>
		</li>
		<li>
			<p>Device drivers → Block devices → Virtio block driver</p>
			<ul>
				<li>
					<p>Set to Enable</p>
				</li>
			</ul>
		</li>
	</ul>
	<pre class="language-bash"><code class="language-bash">git clone git://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git &amp;&amp; cd linux/
git checkout v5.15
make menuconfig
make -j$(nproc) bzImage</code></pre>
	<p>We can now run Ubuntu with our custom kernel waiting for gdb to be connected:</p>
	<pre class="language-bash"><code class="language-bash">qemu-system-x86_64 \
  -kernel /home/bastien/work/linux/arch/x86_64/boot/bzImage \
  -append "root=/dev/sda2 console=ttyS0 nokaslr" \
  -nographic \
  -smp $(nproc) \
  -m 8G \
  -hda ubuntu-21.10-live-server-amd64.qcow2 \
  -boot c \
  -cpu host \
  -net nic,model=virtio \
  -net user,hostfwd=tcp::10022-:22 \
  -enable-kvm \
  -s -S</code></pre>
	<p>And we can fire up gdb and set a breakpoint on the xsk_bind function:</p>
	<pre class="language-bash"><code class="language-bash">$ gdb  -ex "add-auto-load-safe-path $(pwd)" -ex "file vmlinux" -ex "target remote :1234" -ex "hbreak start_kernel" -ex "continue"
(gdb) b xsk_bind
(gdb) continue</code></pre>
	<p>After executing the network setup script and running flowtrackd, we hit the <code>xsk_bind</code> breakpoint:</p>
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/5gubA71gEppejdOkeME2rL/61ea127cb68fdfd4a6603445bb28630c/image6-7.png" alt="gdb with breakpoint hit at xsk_bind" class="kg-image" width="1076" height="820" loading="lazy">

	</figure>
	<p>We continue to hit the second <code>xsk_bind</code> breakpoint (the one that returns EINVAL) and after a few <code>next</code> and <code>step</code> commands, we find <a href="https://elixir.bootlin.com/linux/v5.15/source/net/xdp/xsk.c#L938">which function</a> returned the <a href="https://elixir.bootlin.com/linux/v5.15/source/net/xdp/xsk_buff_pool.c#L201">EINVAL value</a>:</p>
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/1GRpxt6bPf3f42tL0rTmf7/a99d880dc0579353c49bb21703adfc79/image8-2.png" alt="gdb stepping next to the EINVAL value" class="kg-image" width="1276" height="906" loading="lazy">

	</figure>
	<p>In our Rust code, we allocate a new FILL and a COMPLETION queue for each queue id of the device prior to calling <code>xsk_socket__create_shared()</code>. Why are those set to NULL? Looking at the code, <code>pool-&gt;fq</code> comes from a struct field named <code>fq_tmp</code> that is accessed from the sock pointer <code>(print ((struct xdp_sock *)sock-&gt;sk)-&gt;fq_tmp)</code>. The field is set in the first call to xsk_bind() but isn't in the second call. We note that at the end of the xsk_bind() function, <a href="https://elixir.bootlin.com/linux/v5.15/source/net/xdp/xsk.c#L981">fq_tmp and cq_tmp are set to NULL</a> as per this comment: "FQ and CQ are now owned by the buffer pool and cleaned up with it.".</p>
	<p>Something is definitely going wrong in libbpf because the FILL queue and COMPLETION queue pointers are missing.</p>
	<p>Back to the libbpf <code>xsk_socket__create_shared()</code> function to check where the queues are set for the socket, and we quickly notice two functions that interact with the FILL and COMPLETION queues:</p>
	<p>The first function called is <a href="https://github.com/libbpf/libbpf/blob/v0.8.0/src/xsk.c#L879"><code>xsk_get_ctx()</code></a>:</p>
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/3ZJQXz150mnzj47exyaKUB/c0cc03d40a70873a21725abc4401886f/image18-2.png" alt="xsk_get_ctx() function code" class="kg-image" width="993" height="508" loading="lazy">

	</figure>
	<p>The second is <a href="https://github.com/libbpf/libbpf/blob/v0.8.0/src/xsk.c#L923"><code>xsk_create_ctx()</code></a>:</p>
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/37eShbafNDmVglIOd8RUPt/c1ade49287889ec4a9b31e350b47c889/image16-3.png" alt="xsk_create_ctx() function code" class="kg-image" width="795" height="1164" loading="lazy">

	</figure>
	<p>Remembering our setup, can you spot what the issue is?</p>
	<div class="flex anchor relative">
		<h2 id="the-bug-missing-feature">The bug / missing feature</h2>
		<a href="https://blog.cloudflare.com/#the-bug-missing-feature" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>The issue is in the comparison performed in the <a href="https://github.com/libbpf/libbpf/blob/v0.8.0/src/xsk.c#L888"><code>xsk_get_ctx()</code></a> to find the right socket context structure associated with the (ifindex, queue_id) pair in the linked-list. The UMEM being shared across Xsks, the same <code>umem-&gt;ctx_list</code> linked list head is used to find the sockets that use this UMEM. Remember that in our setup, flowtrackd attaches itself to two network devices that live in different network namespaces. Using the interface index and the queue_id to find the right context (FILL and COMPLETION queues) associated to a socket is not sufficient because another network interface with the same interface index can exist at the same time in another network namespace.</p>
	<div class="flex anchor relative">
		<h2 id="what-can-we-do-about-it">What can we do about it?</h2>
		<a href="https://blog.cloudflare.com/#what-can-we-do-about-it" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>We need to tell apart two network devices "system-wide". That means across the network namespace boundaries.</p>
	<p>Could we fetch and store the network namespace inode number of the current process (<code>stat -c%i -L /proc/self/ns/net</code>) at the context creation and then use it in the comparison? According to <a href="https://man7.org/linux/man-pages/man7/inode.7.html">man 7 inode</a>: "Each file in a filesystem has a unique inode number. Inode numbers are guaranteed to be unique only within a filesystem". However, inode numbers can be reused:</p>
	<pre class="language-bash"><code class="language-bash"># ip netns add a
# stat -c%i /run/netns/a
4026532570
# ip netns delete a
# ip netns add b
# stat -c%i /run/netns/b
4026532570</code></pre>
	<p>Here are our options:</p>
	<ul>
		<li>
			<p>Do a quick hack to ensure that the interface indexes are not the same (as done in the integration tests).</p>
		</li>
		<li>
			<p>Explain our use case to the libbpf maintainers and see how the API for the <code>xsk_socket__create_shared()</code> function should change. It could be possible to pass an opaque "cookie" as a parameter at the socket creation and pass it to the functions that access the socket contexts.</p>
		</li>
		<li>
			<p>Take our chances and look for Linux patches that contain the words “netns” and “cookie”</p>
		</li>
	</ul>
	<p>Well, well, well: <a href="https://lore.kernel.org/bpf/c47d2346982693a9cf9da0e12690453aded4c788.1585323121.git.daniel@iogearbox.net">[PATCH bpf-next 3/7] bpf: add netns cookie and enable it for bpf cgroup hooks</a></p>
	<p>This is almost what we need! This patch adds a kernel function named <code>bpf_get_netns_cookie()</code> that would get us the network namespace cookie linked to a socket:</p>
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/4yeyvSXN19FPw3gJvKGPdh/31c07d8740a52ce7de8073565d181de0/image15-3.png" alt="bpf_get_netns_cookie manual page" class="kg-image" width="987" height="413" loading="lazy">

	</figure>
	<p>A <a href="https://patchwork.kernel.org/project/linux-parisc/patch/20210210120425.53438-2-lmb@cloudflare.com">second patch</a> enables us to get this cookie from userspace:</p>
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/2fkodeI6Um1we4uArDAl9w/864571b824f22e85d43265d964efc138/image17-2.png" alt="add SO_NETNS_COOKIE socket option patch" class="kg-image" width="1108" height="527" loading="lazy">

	</figure>
	<p>I know this Lorenz from somewhere :D</p>
	<p>Note that this patch was shipped with the Linux <b>v5.14</b> release.</p>
	<p>We have more guaranties now:</p>
	<ul>
		<li>
			<p>The cookie is generated for us by the kernel.</p>
		</li>
		<li>
			<p>There is a strong bound to the socket from its creation (the netns cookie value is present in the socket structure).</p>
		</li>
		<li>
			<p>The network namespace cookie remains stable for its lifetime.</p>
		</li>
		<li>
			<p>It provides a global identifier that can be assumed unique and not reused.</p>
		</li>
	</ul>
	<div class="flex anchor relative">
		<h2 id="a-patch">A patch</h2>
		<a href="https://blog.cloudflare.com/#a-patch" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>At the socket creation, we retrieve the netns_cookie from the Xsk file descriptor with <code>getsockopt()</code>, insert it in the xsk_ctx struct and add it in the comparison performed in <code>xsk_get_ctx()</code>.</p>
	<p>Our initial patch was tested on Linux v5.15 with libbpf v0.8.0.</p>
	<div class="flex anchor relative">
		<h3 id="testing-the-patch">Testing the patch</h3>
		<a href="https://blog.cloudflare.com/#testing-the-patch" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>We keep the same network setup script, but we set the number of queues per interface to two (QUEUES=2). This will help us check that two sockets created in the same network namespace have the same netns_cookie.</p>
	<p>After recompiling flowtrackd to use our patched libbpf, we can run it inside <b>our guest</b> with gdb and set breakpoints on <code>xsk_get_ctx</code> as well as <code>xsk_create_ctx</code>. We now have two instances of gdb running at the same time, one debugging the system and the other debugging the application running in that system. Here is the gdb guest view:</p>
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/1sbLABtvPacBjuX9oFVn0Z/c01cd118d1d3d6e15d178a0689e92691/unnamed2-1.png" alt="gdb guest view of the xsk context breakpoint hits" class="kg-image" width="1415" height="845" loading="lazy">

	</figure>
	<p>Here is the gdb system view:</p>
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/4HDyMoI08Ipp4ppwCRZmH8/256e62125a122c3e8a493059dd3f718e/unnamed3.png" alt="gdb system view of the xsk_bind breakpoint hits" class="kg-image" width="1241" height="751" loading="lazy">

	</figure>
	<p>We can see that the netns_cookie value for the first two Xsks is 1 (root namespace) and the <code>net_cookie</code> value for the two other Xsks is 8193 (inner-ns namespace).</p>
	<p>flowtrackd didn't crash and is behaving as expected. It works!</p>
	<div class="flex anchor relative">
		<h2 id="conclusion">Conclusion</h2>
		<a href="https://blog.cloudflare.com/#conclusion" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>

	<div class="flex anchor relative">
		<h3 id="situation">Situation</h3>
		<a href="https://blog.cloudflare.com/#situation" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>Creating AF_XDP sockets with the XDP_SHARED_UMEM flag set fails when the two devices’ ifindex (and the queue_id) are the same. This can happen with devices in different network namespaces.</p>
	<p>In the shared UMEM mode, each Xsk is expected to have a dedicated fill and completion queue. Context data about those queues are set by libbpf in a linked-list stored by the UMEM object. The comparison performed to pick the right context in the linked-list only takes into account the device ifindex and the queue_id which can be the same when devices are in different network namespaces.</p>
	<div class="flex anchor relative">
		<h3 id="resolution">Resolution</h3>
		<a href="https://blog.cloudflare.com/#resolution" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>We retrieve the <code>netns_cookie</code> associated with the socket at its creation and add it in the comparison operation.</p>
	<p>The <a href="https://github.com/xdp-project/xdp-tools/pull/205">fix</a> has been submitted and merged in libxdp which is where the AF_XDP parts of libbpf now <a href="https://github.com/libbpf/libbpf/wiki/Libbpf:-the-road-to-v1.0#xskch-is-moving-into-libxdp">live</a>.</p>
	<p>We’ve also <a href="https://github.com/libbpf/libbpf/releases/tag/v0.8.1">backported the fix</a> in libbpf and updated the <a href="https://github.com/libbpf/libbpf-sys/releases/tag/0.8.2%2Bv0.8.1">libbpf-sys Rust crate</a> accordingly.</p>
</div>