<div class="mb2 gray5">2 min read</div>
<div class="mt4">This post is also available in <a href="https://blog.cloudflare.com/ja-jp/why-replicate-joining-cloudflare">日本語</a> and <a href="https://blog.cloudflare.com/ko-kr/why-replicate-joining-cloudflare">한국어</a>.</div><img class="mr2" src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/4lZ2Fxzql7vr2UiHIQxpEk/cd805ef6e6f83d5d7e7eb0b0bcadda36/BLOG-3088_Meta_Card.png" alt="">
<div class="post-content lh-copy gray1">
	<p></p>
	<p></p>
	<p>We're happy to announce that as of today Replicate is officially part of Cloudflare.</p>
	<p>When we started Replicate in 2019, OpenAI had just open sourced GPT-2, and few people outside of the machine learning community paid much attention to AI. But for those of us in the field, it felt like something big was about to happen. Remarkable models were being created in academic labs, but you needed a metaphorical lab coat to be able to run them.</p>
	<p>We made it our mission to get research models out of the lab into the hands of developers. We wanted programmers to creatively bend and twist these models into products that the researchers would never have thought of.</p>
	<p>We approached this as a tooling problem. Just like tools like Heroku made it possible to run websites without managing web servers, we wanted to build tools for running models without having to understand backpropagation or deal with CUDA errors.</p>
	<p>The first tool we built was <a href="https://github.com/replicate/cog"><u>Cog</u></a>: a standard packaging format for machine learning models. Then we built <a href="https://replicate.com"><u>Replicate</u></a> as the platform to run Cog models as API endpoints in the cloud. We abstracted away both the low-level machine learning, and the complicated GPU cluster management you need to run inference at scale.</p>
	<p>It turns out the timing was just right. When <a href="https://replicate.com/stability-ai/stable-diffusion"><u>Stable Diffusion</u></a> was released in 2022 we had mature infrastructure that could handle the massive developer interest in running these models. A ton of fantastic apps and products were built on Replicate, apps that often ran a single model packaged in a slick UI to solve a particular use case.</p>
	<p>Since then, <a href="https://www.latent.space/p/ai-engineer"><i><u>AI Engineering</u></i></a> has matured into a serious craft. AI apps are no longer just about running models. The modern AI stack has model inference, but also microservices, content delivery, object storage, caching, databases, telemetry, etc. We see many of our customers building complex heterogenous stacks where the Replicate models are one part of a higher-order system across several platforms.</p>
	<p><i>This is why we’re joining Cloudflare</i>. Replicate has the tools and primitives for running models. Cloudflare has the best network, Workers, R2, Durable Objects, and all the other primitives you need to build a full AI stack.</p>
	<p>The AI stack lives entirely on the network. Models run on data center GPUs and are glued together by small cloud functions that call out to vector databases, fetch objects from blob storage, call MCP servers, etc. “<a href="https://blog.cloudflare.com/the-network-is-the-computer"><u>The network is the computer</u></a>” has never been more true.</p>
	<p>At Cloudflare, we’ll now be able to build the AI infrastructure layer we have dreamed of since we started. We’ll be able to do things like run fast models on the edge, run model pipelines on instantly-booting Workers, stream model inputs and outputs with WebRTC, etc.</p>
	<p>We’re proud of what we’ve built at Replicate. We were the first generative AI serving platform, and we defined the abstractions and design patterns that most of our peers have adopted. We’ve grown a wonderful community of builders and researchers around our product.</p>
</div>