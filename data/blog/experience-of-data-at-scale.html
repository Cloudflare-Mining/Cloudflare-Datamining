<div class="mb2 gray5">8 min read</div><img class="mr2" src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/1UYv7cOgccE1Cq1Hq9ID3X/83dda0f0bb0a191460f9b8ac774c7e18/1_111.png" alt="">
<div class="post-content lh-copy gray1">
	<p>Cloudflare recently announced our goal to hire <a href="https://blog.cloudflare.com/cloudflare-1111-intern-program"><u>1,111 interns</u></a> in 2026 — that’s equivalent to about 25% of our full-time workforce. This means countless opportunities to develop and ship working code into production. It also creates novel opportunities to measure aspects of the Internet that are otherwise hard to observe — and more difficult still to understand.</p>
	<p>Measurement is hard, even at Cloudflare, despite the vast amount of data generated by our traffic (much of it published via <a href="https://radar.cloudflare.com"><u>Cloudflare Radar</u></a>). A common misconception we often hear is, “Cloudflare has so much data that it must have all the answers.” Having a huge amount of data is great — but it also means much more noise to filter out, and lots of additional work to rule out alternative explanations.</p>
	<p>Ram Sundara Raman was an intern at Cloudflare in 2022 as he pursued his PhD. He’s now an assistant professor at University of California, Santa Cruz, and we’ve invited him back to share his insights about working with data at Cloudflare.</p>
	<p>Ram’s project is a great example of how insights that researchers shared and brought from their&nbsp; <a href="https://breakerspace.cs.umd.edu"><u>university research lab</u></a> can lay the groundwork for a valuable project at Cloudflare — in this case, detecting and explaining connection failures to customers. One tip for prospective interns: If you’re applying and thinking about data and measurement ideas to work on at Cloudflare, a good question to ponder is if, how, or why, <i>your</i> idea might matter to Cloudflare. We love hearing your ideas!</p>
	<p>Without further ado, here’s Ram. We hope his insights are as informative and refreshing to future interns — and practitioners — as they are to us here at Cloudflare.</p>
	<div class="flex anchor relative">
		<h2 id="insights-from-data-at-large-scale-might-just-be-a-small-miracle">Insights from data at large scale might just be a small miracle&nbsp;&nbsp;</h2>
		<a href="https://blog.cloudflare.com/#insights-from-data-at-large-scale-might-just-be-a-small-miracle" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p><i>by Ram Sundara Raman, Assistant Professor of Computer Science and Engineering, UC Santa Cruz</i></p>
	<p>Before joining Cloudflare as a research intern in the summer of 2022, I’d worked on multiple network security and privacy research problems as a PhD student at the University of Michigan. My previous experience involved <i>active measurements</i>, in which probes were carefully crafted and transmitted to detect and quantify security issues such as <a href="https://dl.acm.org/doi/10.1145/3419394.3423665"><u>HTTPS interception</u></a> and <a href="https://dl.acm.org/doi/10.1145/3372297.3417883"><u>connection tampering</u></a>. These attacks, performed by powerful network middleboxes between users and Internet servers, can block Internet content and services for numerous users in various regions, and can also reduce their security. For example, <a href="https://dl.acm.org/doi/10.1145/3419394.3423665"><u>the HTTPS Interception Man-in-the-Middle Attack in Kazakhstan in 2019</u></a> was detected in 7-24% of all measurements we performed in the country.&nbsp;</p>
	<p>Detecting such attacks is challenging. The underlying mechanisms are diverse, with both geographic and temporal variations — and they’re entirely opaque. Moreover, the Internet has no technical mechanisms to report to users when their traffic is being manipulated, and third party actors rarely, if ever, are transparent with affected users.&nbsp;</p>
	<p>My active measurement work before Cloudflare helped resolve these challenges. Along with my PI and team at the University of Michigan, I helped develop <a href="https://censoredplanet.org"><u>Censored Planet</u></a>, one of the largest active Internet censorship observatories, detecting connection tampering in more than 200 countries. However, active measurements face barriers on scale, resources, and real-world view. For instance, Censored Planet is only able to measure blocking and connection tampering for the 2,000 most popular websites, simply because of limits on time and resources.&nbsp;</p>
	<p>While working on projects like Censored Planet, I’d often look at large network operators or cloud providers and think: “<i>If only I had my hands on the data they collect, I could solve this problem so easily. They have a global view of real-world traffic from nearly every network, and probably enough resources and telemetry to scale analysis to that level of data. How hard could it be to use this data, for example, to detect when middleboxes interfere?”</i>&nbsp;</p>
	<p>As we learned through <a href="https://research.cloudflare.com/publications/SundaraRaman2023"><u>our research</u></a> published at <a href="https://www.sigcomm.org"><u>ACM SIGCOMM’23</u></a> — it can be <i>very</i> hard.</p>
	<p>My perspectives on censorship evolved as a direct result of my experience at Cloudflare, which taught me that detection at scale is hard, even with large-scale data. The research I did during my internship helped reveal that network middleboxes block or otherwise interfere with certain connections not only in limited places, but also at <a href="https://blog.cloudflare.com/tcp-resets-timeouts"><u>various scales around the world</u></a>. </p>
	<div class="flex anchor relative">
		<h3 id="an-internship-project-built-on-real-insights-using-production-data">An internship project built on real insights, using production data</h3>
		<a href="https://blog.cloudflare.com/#an-internship-project-built-on-real-insights-using-production-data" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>In this research, we built upon insights gathered by the wider active measurement community, namely that middleboxes interfere with Internet TCP connections by dropping packets, or injecting RST packets to cause connections to abort. The same insights revealed that the patterns of packet drops and RSTs are deterministic&nbsp; —&nbsp; and, as a result, potentially detectable. Such is the flexibility of active measurement: craft a custom request, or ‘probe,’ that elicits a response from the environment. However, such a targeted approach would be difficult to scale and maintain, even for Cloudflare: What probes should be crafted? Where should they be sent? What motivation would Cloudflare have to even try, if the risk of missing so much is so high?&nbsp;&nbsp;</p>
	<p>The goal of my internship was to see if we could instead flip the approach: to be passive instead of active. Everything Cloudflare does must be both scalable and <i>sustainable</i>. However, it was entirely uncertain whether a system restricted to passive observation could be constructed, even if the tampering events could be detected. The requirement was clear: Only observe and use data that comes to Cloudflare naturally. No mixing in other datasets, no running our own active measurements. Either would have made life easier: we could have controlled the variables, maybe even obtained ground truth that would help us confirm our observations. But where’s the fun in that? Besides, Cloudflare has <i>all</i> the data anyway… right?&nbsp;</p>
	<p>Yes, maybe — if it is sampled appropriately, can be teased out reliably, and correctly interpreted.</p>
	<p>Here’s a useful insight: I’ve often heard people say that finding middleboxes that tamper with Internet connections using active measurements is like finding a needle in a haystack — rare, finicky, and hard to pin down. When we started looking at this problem from the lens of Cloudflare’s passive dataset, we quickly realized we were still looking for the same needle — and in some ways, it was now even harder to find.</p>
	<p>That’s because as a passive observer we lose the ability to choose where to look. Also, the haystack now stretches across continents, millions of users, and — I’m not exaggerating here — thousands of ways connections can be made and broken. Not only did we have to identify tampering from millions of real-world data points, we had to do it with data that was full of obstacles and pitfalls. It felt a lot like working with unseen traps and their tripwires.&nbsp;</p>
	<div class="flex anchor relative">
		<h3 id="the-traps-and-tripwires-of-large-scale-passive-data">The traps and tripwires of large-scale passive data</h3>
		<a href="https://blog.cloudflare.com/#the-traps-and-tripwires-of-large-scale-passive-data" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>There were multiple challenges that I only truly understood once faced with them. Let’s start with the obvious one: <b>scale</b>.</p>
	<p>First, there was a glut of large-scale datasets, primarily associated with incoming connections to Cloudflare. For example, at the time of my internship, Cloudflare was serving more than 45 million HTTP requests per second globally, across more than 285 data centers. Cloudflare also gets TCP connections to its 1.1.1.1 DNS server. We also explored <a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Guides/Network_Error_Logging"><u>Network Error Logging</u><b><u> </u></b><u>(NEL)</u></a> data. Usually, in measurement research, we’re dealing with the issue of <i>too little scale. </i>Here, we had the opposite problem: too much of a good thing. In practice, each of these datasets had their own independent sampling methods, making it all but impossible to utilize them all together. Moreover, datasets like NEL are biased since only some clients support it, and because only some websites enable it. After evaluating these biases, NEL did not make the final cut. </p>
	<p>To manage the scale, we constructed special <a href="https://blog.cloudflare.com/tcp-resets-timeouts/#first-sample-connections"><u>IPTABLES rules</u></a> to log and store incoming TCP connections across all of Cloudflare’s points of presence — every server in each of 285 datacenters. However, due to the extremely large scale of the data, we had to limit ourselves to work with a uniformly random sample of one in every 10,000 connections. For each sample, we only logged the first 10 inbound packets of each connection. That meant we could not detect certain infrequent types of tampering, or any tampering that occurs later in a flow, after the first 10 packets.&nbsp;</p>
	<p>Still, within those constraints, we managed to develop tampering signatures — distinctive packet patterns that reveal when middleboxes interfere. However, developing these signatures was anything but straightforward, due to the second tripwire: <b>noisy data.&nbsp;</b></p>
	<p>It’s difficult to imagine that we could have anticipated all the different sources of noise. For example, the resolution of time-keeping in event records was milliseconds, but many packets could arrive in a single millisecond, which meant we could not trust the ordering of logged packets. We eventually learned that some denial-of-service attack traffic, as well as port scans, can look eerily like tampering events, and certain “best practices” designed to help improve the Internet, such as <a href="https://datatracker.ietf.org/doc/html/rfc6555"><u>Happy Eyeballs</u></a>, became quirks that messed with our detection. We spent a lot of time analyzing these sources of noise and iterating on our signatures to understand them. We accepted events as tampering only if supported by other sources of evidence that we identified, including but not limited to inconsistent changes in the Time-To-Live (TTL) field in the IP header.</p>
	<p>That brings me to our last tripwire: a <b>lack of ground truth.</b></p>
	<p>Without active, controlled experiments, it would have been extremely difficult for us to confirm when something we detected was indeed tampering, and not one of the thousand other phenomena on the Internet. Fortunately, thanks to the <a href="https://censorbib.nymity.ch"><u>amazing work of many researchers in the censorship measurement space</u></a>, we were able to recognize at least some known signals and patterns in the data, and these helped us confirm many cases of tampering.&nbsp;</p>
	<p>There were plenty more tripwires. But the key realization for me was this: While providers have lots of data that can tell you <i>things</i>, it’s incredibly hard to know which thing, how much of it, and about what. Large infrastructure operators see a filtered, sampled, and often partial view of the Internet. For example,</p>
	<ul>
		<li>
			<p>Services like Cloudflare can see only which connections were affected and where the connections were initiated, but <i>not who did the tampering;</i></p>
		</li>
		<li>
			<p>It was sometimes possible to understand which domains were blocked, but not always, because the necessary packets can be dropped before they get to Cloudflare;</p>
		</li>
		<li>
			<p>As a passive observer, it’s possible only to see users' activity that is affected, not what <i>could</i> be affected.</p>
		</li>
	</ul>
	<p>For a company that handles a double-digit percentage of Internet websites and services, these were surprising — but understandable –&nbsp; limitations.

		It may seem like the exercise is impossible, but it’s not. It’s just more challenging than I expected it to be. Despite all that, we found ways to extract meaning from chaos. For example, we carefully and painstakingly enumerated all common packet sequences Cloudflare observed, and extracted from them those that might indicate tampering, based on prior work. Moreover, we used signals like the TTL field mentioned above as supporting evidence that these packet signatures did indeed show tampering.&nbsp;</p>
	<p>All of this adds up to a simple but important conclusion: large infrastructure providers are not omniscient.<b> </b>Having a global view can be powerful, but doesn’t automatically translate into <i>easy</i> observations. You can have all the data in the world and still struggle to tell the difference between a middlebox, a security filter, a confused IoT device, and even regular users closing tabs and browsers.&nbsp;</p>
	<p>But that dichotomy is also the beauty of the problem space. Working with imperfect data forces us to be creative, to find patterns in the noise, and to design methods that work despite what’s missing. And no, before you ask, you can’t just throw machine learning at the problem, nor do you need to — even with all the noise, the protocols are tightly specified, meaning patterns can be enumerated easily but must still be debated manually.&nbsp;</p>
	<div class="flex anchor relative">
		<h3 id="an-internship-project-built-on-real-insights-using-production-data">An internship project built on real insights, using production data</h3>
		<a href="https://blog.cloudflare.com/#an-internship-project-built-on-real-insights-using-production-data" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>Using our packet-level samples and <b>19 tampering signatures</b>, we saw distinctive tampering behaviors across hundreds of networks, including being able to track large increases in tampering rates (Figure 1). And it worked because, despite the data’s limits, Cloudflare’s networks let us see the <i>real-world effects</i> of tampering. Also, thanks to the tireless efforts of <a href="https://research.cloudflare.com/about/people/luke-valenta"><u>Luke Valenta</u></a> and the Cloudflare Radar team, the data from our project is continuously being <a href="https://radar.cloudflare.com/security/network-layer#tcp-resets-and-timeouts"><u>published on Cloudflare Radar</u></a> (Figure 2).</p>
	<figure class="kg-card kg-image-card">
		<img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/306MKIUSWYPDewkUmckP4p/74227ea6d9a9f5750d6231e17aaabe0f/image1.png" alt="Figure 1: Increase in mach rates of our 19 tampering signatures during a period of nationwide protests in Iran in late-2022." class="kg-image" width="1071" height="922" loading="lazy">
	</figure>
	<p><sup>Figure 1: Increase in mach rates of our 19 tampering signatures during a period of nationwide protests in Iran in late-2022.</sup></p>
	<figure class="kg-card kg-image-card">
		<img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/26qYosPoBquXSZrUACTbYp/a9adbfce9c04cb1831f4b8610fe69445/image2.png" alt="Figure 2: Data from our connection tampering research is available live on Radar." class="kg-image" width="1600" height="874" loading="lazy">
	</figure>
	<p><sup>Figure 2: Data from our connection tampering research is available live on Radar.</sup></p>
	<p>In the future, though, I think solving challenges like these will require a <b>combination of passive and active probing</b>, using the scale of providers like Cloudflare together with targeted, controlled measurements to paint the full picture of Internet tampering. My team at&nbsp; <a href="https://randlab.engineering.ucsc.edu"><u>UCSC’s RANDLab</u></a> and the research group at <a href="https://censoredplanet.org"><u>Censored Planet</u></a> continue to work on this problem, especially asking how we can automatically identify tampering when attacks happen or networks change.&nbsp;</p>
	<p>While collaborations between academia and industry aren’t always straightforward, they hold strong potential to help build a better Internet. If you’re interested in an internship adventure like the one I described, <a href="https://www.cloudflare.com/en-gb/careers/jobs/?department=Early+Talent"><u>apply today</u></a>! </p>
</div>