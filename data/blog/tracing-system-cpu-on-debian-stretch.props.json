{
	"footerBlurb": "Cloudflare's connectivity cloud protects <a target='_blank' href='https://www.cloudflare.com/network-services/' rel='noreferrer'>entire corporate networks</a>, helps customers build <a target='_blank' href='https://workers.cloudflare.com/' rel='noreferrer'>Internet-scale applications efficiently</a>, accelerates any <a target='_blank' href='https://www.cloudflare.com/performance/accelerate-internet-applications/' rel='noreferrer'>website or Internet application</a>, <a target='_blank' href='https://www.cloudflare.com/ddos/' rel='noreferrer'>wards off DDoS attacks</a>, keeps <a target='_blank' href='https://www.cloudflare.com/application-security/' rel='noreferrer'>hackers at bay</a>, and can help you on <a target='_blank' href='https://www.cloudflare.com/products/zero-trust/' rel='noreferrer'>your journey to Zero Trust</a>.<br/><br/>Visit <a target='_blank' href='https://one.one.one.one/' rel='noreferrer'>1.1.1.1</a> from any device to get started with our free app that makes your Internet faster and safer.<br/><br/>To learn more about our mission to help build a better Internet, <a target='_blank' href='https://www.cloudflare.com/learning/what-is-cloudflare/' rel='noreferrer'>start here</a>. If you&apos;re looking for a new career direction, check out <a target='_blank' href='http://www.cloudflare.com/careers' rel='noreferrer'>our open positions</a>.",
	"initialReadingTime": "4",
	"locale": "en-us",
	"localesAvailable": [],
	"post": {
		"authors": [
			{
				"name": "Ivan Babrou",
				"slug": "ivan",
				"bio": null,
				"profile_image": "https://cf-assets.www.cloudflare.com/zkvhlag99gkb/2PiutF6RYsPG4R5cGpKJgf/0fde0b6e5edc08cda56ea09f5062f7be/ivan.png",
				"location": null,
				"website": null,
				"twitter": null,
				"facebook": null,
				"publiclyIndex": true
			}
		],
		"excerpt": "How an innocent OS upgrade triggered a cascade of issues and forced us into tracing Linux networking internals.",
		"feature_image": "https://cf-assets.www.cloudflare.com/zkvhlag99gkb/3g2xrmIfJIGHgqqL9PyoFb/47034cfa218818054ecb8446be9dbbec/tracing-system-cpu-on-debian-stretch.png",
		"featured": false,
		"html": "<p><i>This is a heavily truncated version of an internal blog post from August 2017. For more recent updates on Kafka, check out </i><a href=\"/squeezing-the-firehose/\"><i>another blog post on compression</i></a><i>, where we optimized throughput 4.5x for both disks and network.</i></p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/6o0r4Jk1oqG6ncMv8xWNb3/08bd0256a7509447b87aa08a7e7305f5/photo-1511971523672-53e6411f62b9\" alt=\"\" class=\"kg-image\" width=\"1080\" height=\"720\" loading=\"lazy\"/>\n            \n            </figure><p>Photo by <a href=\"https://unsplash.com/@alex_povolyashko?utm_source=ghost&utm_medium=referral&utm_campaign=api-credit\">Alex Povolyashko</a> / <a href=\"https://unsplash.com/?utm_source=ghost&utm_medium=referral&utm_campaign=api-credit\">Unsplash</a></p>\n          <div class=\"flex anchor relative\">\n            <h3 id=\"upgrading-our-systems-to-debian-stretch\">Upgrading our systems to Debian Stretch</h3>\n            <a href=\"#upgrading-our-systems-to-debian-stretch\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n              <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n            </a>\n          </div>\n        <p>For quite some time we&#39;ve been rolling out Debian Stretch, to the point where we have reached ~10% adoption in our core datacenters. As part of upgarding the underlying OS, we also evaluate the higher level software stack, e.g. taking a look at our ClickHouse and Kafka clusters.</p><p>During our upgrade of Kafka, we sucessfully migrated two smaller clusters, <code>logs</code> and <code>dns</code>, but ran into issues when attempting to upgrade one of our larger clusters, <code>http</code>.</p><p>Thankfully, we were able to roll back the <code>http</code> cluster upgrade relatively easily, due to heavy versioning of both the OS and the higher level software stack. If there&#39;s one takeaway from this blog post, it&#39;s to take advantage of consistent versioning.</p>\n          <div class=\"flex anchor relative\">\n            <h3 id=\"high-level-differences\">High level differences</h3>\n            <a href=\"#high-level-differences\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n              <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n            </a>\n          </div>\n        <p>We upgraded one Kafka <code>http</code> node, and it did not go as planned:</p>\n            <figure class=\"kg-card kg-image-card \">\n            <a href=/content/images/2018/04/1.png>\n            <Image src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/2hbPsT1ahBYgS806ztIpG3/070d402c9a5c1d38f257d65d87252f6c/1.png\" alt=\"1\" class=\"kg-image\" width=\"3104\" height=\"1972\" loading=\"lazy\"/>\n            </a>\n            </figure><p>Having 5x CPU usage was definitely an unexpected outcome. For control datapoints, we compared to a node where no upgrade happened, and an intermediary node that received a software stack upgrade, but not an OS upgrade. Neither of these two nodes experienced the same CPU saturation issues, even though their setups were practically identical.</p><p>For debugging CPU saturation issues, we call on <code>perf</code> to fish out details:</p>\n            <figure class=\"kg-card kg-image-card \">\n            <a href=/content/images/2018/04/2-3.png>\n            <Image src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/2FsdAiwWMDh5t6VTTLuSV9/14c09b1b8fc3053a3dd4d49ff467f19a/2-3.png\" alt=\"2-3\" class=\"kg-image\" width=\"1849\" height=\"1640\" loading=\"lazy\"/>\n            </a>\n            </figure><p><i>The command used was: </i><code><i>perf top -F 99</i></code><i>.</i></p>\n          <div class=\"flex anchor relative\">\n            <h3 id=\"rcu-stalls\">RCU stalls</h3>\n            <a href=\"#rcu-stalls\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n              <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n            </a>\n          </div>\n        <p>In addition to higher system CPU usage, we found secondary slowdowns, including <a href=\"http://www.rdrop.com/~paulmck/RCU/whatisRCU.html\">read-copy update (RCU)</a> stalls:</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">[ 4909.110009] logfwdr (26887) used greatest stack depth: 11544 bytes left\n[ 4909.392659] oom_reaper: reaped process 26861 (logfwdr), now anon-rss:8kB, file-rss:0kB, shmem-rss:0kB\n[ 4923.462841] INFO: rcu_sched self-detected stall on CPU\n[ 4923.462843]  13-...: (2 GPs behind) idle=ea7/140000000000001/0 softirq=1/2 fqs=4198\n[ 4923.462845]   (t=8403 jiffies g=110722 c=110721 q=6440)</pre></code>\n            <p>We&#39;ve seen RCU stalls before, and our (suboptimal) solution was to reboot the machine.</p><p>However, one can only handle so many reboots before the problem becomes severe enough to warrant a deep dive. During our deep dive, we noticed in <code>dmesg</code> that we had issues allocating memory, while trying to write errors:</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">Aug 15 21:51:35 myhost kernel: INFO: rcu_sched detected stalls on CPUs/tasks:\nAug 15 21:51:35 myhost kernel:         26-...: (1881 ticks this GP) idle=76f/140000000000000/0 softirq=8/8 fqs=365\nAug 15 21:51:35 myhost kernel:         (detected by 0, t=2102 jiffies, g=1837293, c=1837292, q=262)\nAug 15 21:51:35 myhost kernel: Task dump for CPU 26:\nAug 15 21:51:35 myhost kernel: java            R  running task    13488  1714   1513 0x00080188\nAug 15 21:51:35 myhost kernel:  ffffc9000d1f7898 ffffffff814ee977 ffff88103f410400 000000000000000a\nAug 15 21:51:35 myhost kernel:  0000000000000041 ffffffff82203142 ffffc9000d1f78c0 ffffffff814eea10\nAug 15 21:51:35 myhost kernel:  0000000000000041 ffffffff82203142 ffff88103f410400 ffffc9000d1f7920\nAug 15 21:51:35 myhost kernel: Call Trace:\nAug 15 21:51:35 myhost kernel:  [&lt;ffffffff814ee977&gt;] ? scrup+0x147/0x160\nAug 15 21:51:35 myhost kernel:  [&lt;ffffffff814eea10&gt;] ? lf+0x80/0x90\nAug 15 21:51:35 myhost kernel:  [&lt;ffffffff814eecb5&gt;] ? vt_console_print+0x295/0x3c0\nAug 15 21:51:35 myhost kernel:  [&lt;ffffffff810b1193&gt;] ? call_console_drivers.isra.22.constprop.30+0xf3/0x100\nAug 15 21:51:35 myhost kernel:  [&lt;ffffffff810b1f51&gt;] ? console_unlock+0x281/0x550\nAug 15 21:51:35 myhost kernel:  [&lt;ffffffff810b2498&gt;] ? vprintk_emit+0x278/0x430\nAug 15 21:51:35 myhost kernel:  [&lt;ffffffff810b27ef&gt;] ? vprintk_default+0x1f/0x30\nAug 15 21:51:35 myhost kernel:  [&lt;ffffffff811588df&gt;] ? printk+0x48/0x50\nAug 15 21:51:35 myhost kernel:  [&lt;ffffffff810b30ee&gt;] ? dump_stack_print_info+0x7e/0xc0\nAug 15 21:51:35 myhost kernel:  [&lt;ffffffff8142d41f&gt;] ? dump_stack+0x44/0x65\nAug 15 21:51:35 myhost kernel:  [&lt;ffffffff81162e64&gt;] ? warn_alloc+0x124/0x150\nAug 15 21:51:35 myhost kernel:  [&lt;ffffffff81163842&gt;] ? __alloc_pages_slowpath+0x932/0xb80\nAug 15 21:51:35 myhost kernel:  [&lt;ffffffff81163c92&gt;] ? __alloc_pages_nodemask+0x202/0x250\nAug 15 21:51:35 myhost kernel:  [&lt;ffffffff811ae9c2&gt;] ? alloc_pages_current+0x92/0x120\nAug 15 21:51:35 myhost kernel:  [&lt;ffffffff81159d2f&gt;] ? __page_cache_alloc+0xbf/0xd0\nAug 15 21:51:35 myhost kernel:  [&lt;ffffffff8115cdfa&gt;] ? filemap_fault+0x2ea/0x4d0\nAug 15 21:51:35 myhost kernel:  [&lt;ffffffff8136dc95&gt;] ? xfs_filemap_fault+0x45/0xa0\nAug 15 21:51:35 myhost kernel:  [&lt;ffffffff8118b3eb&gt;] ? __do_fault+0x6b/0xd0\nAug 15 21:51:35 myhost kernel:  [&lt;ffffffff81190028&gt;] ? handle_mm_fault+0xe98/0x12b0\nAug 15 21:51:35 myhost kernel:  [&lt;ffffffff8110756b&gt;] ? __seccomp_filter+0x1db/0x290\nAug 15 21:51:35 myhost kernel:  [&lt;ffffffff8104fa5c&gt;] ? __do_page_fault+0x22c/0x4c0\nAug 15 21:51:35 myhost kernel:  [&lt;ffffffff8104fd10&gt;] ? do_page_fault+0x20/0x70\nAug 15 21:51:35 myhost kernel:  [&lt;ffffffff819bea02&gt;] ? page_fault+0x22/0x30</pre></code>\n            <p>This suggested that we were logging too many errors, and the actual failure may be earlier in the process. Armed with this hypothesis, we looked at the very beginning of the error chain:</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">Aug 16 01:14:51 myhost systemd-journald[13812]: Missed 17171 kernel messages\nAug 16 01:14:51 myhost kernel:  [&lt;ffffffff81171754&gt;] shrink_inactive_list+0x1f4/0x4f0\nAug 16 01:14:51 myhost kernel:  [&lt;ffffffff8117234b&gt;] shrink_node_memcg+0x5bb/0x780\nAug 16 01:14:51 myhost kernel:  [&lt;ffffffff811725e2&gt;] shrink_node+0xd2/0x2f0\nAug 16 01:14:51 myhost kernel:  [&lt;ffffffff811728ef&gt;] do_try_to_free_pages+0xef/0x310\nAug 16 01:14:51 myhost kernel:  [&lt;ffffffff81172be5&gt;] try_to_free_pages+0xd5/0x180\nAug 16 01:14:51 myhost kernel:  [&lt;ffffffff811632db&gt;] __alloc_pages_slowpath+0x31b/0xb80</pre></code>\n            <p>As much as <code>shrink_node</code> may scream &quot;NUMA issues&quot;, you&#39;re looking primarily at:</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">Aug 16 01:14:51 myhost systemd-journald[13812]: Missed 17171 kernel messages</pre></code>\n            <p>In addition, we also found memory allocation issues:</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">[78972.506644] Mem-Info:\n[78972.506653] active_anon:3936889 inactive_anon:371971 isolated_anon:0\n[78972.506653]  active_file:25778474 inactive_file:1214478 isolated_file:2208\n[78972.506653]  unevictable:0 dirty:1760643 writeback:0 unstable:0\n[78972.506653]  slab_reclaimable:1059804 slab_unreclaimable:141694\n[78972.506653]  mapped:47285 shmem:535917 pagetables:10298 bounce:0\n[78972.506653]  free:202928 free_pcp:3085 free_cma:0\n[78972.506660] Node 0 active_anon:8333016kB inactive_anon:989808kB active_file:50622384kB inactive_file:2401416kB unevictable:0kB isolated(anon):0kB isolated(file):3072kB mapped:96624kB dirty:3422168kB writeback:0kB shmem:1261156kB shmem_thp: 0kB shmem_pmdmapped: 0kB anon_thp: 0kB writeback_tmp:0kB unstable:0kB pages_scanned:15744 all_unreclaimable? no\n[78972.506666] Node 1 active_anon:7414540kB inactive_anon:498076kB active_file:52491512kB inactive_file:2456496kB unevictable:0kB isolated(anon):0kB isolated(file):5760kB mapped:92516kB dirty:3620404kB writeback:0kB shmem:882512kB shmem_thp: 0kB shmem_pmdmapped: 0kB anon_thp: 0kB writeback_tmp:0kB unstable:0kB pages_scanned:9080974 all_unreclaimable? no\n[78972.506671] Node 0 DMA free:15900kB min:100kB low:124kB high:148kB active_anon:0kB inactive_anon:0kB active_file:0kB inactive_file:0kB unevictable:0kB writepending:0kB present:15996kB managed:15900kB mlocked:0kB slab_reclaimable:0kB slab_unreclaimable:0kB kernel_stack:0kB pagetables:0kB bounce:0kB free_pcp:0kB local_pcp:0kB free_cma:0kB\n** 9 printk messages dropped ** [78972.506716] Node 0 Normal: 15336*4kB (UMEH) 4584*8kB (MEH) 2119*16kB (UME) 775*32kB (MEH) 106*64kB (UM) 81*128kB (MH) 29*256kB (UM) 25*512kB (M) 19*1024kB (M) 7*2048kB (M) 2*4096kB (M) = 236080kB\n[78972.506725] Node 1 Normal: 31740*4kB (UMEH) 3879*8kB (UMEH) 873*16kB (UME) 353*32kB (UM) 286*64kB (UMH) 62*128kB (UMH) 28*256kB (MH) 20*512kB (UMH) 15*1024kB (UM) 7*2048kB (UM) 12*4096kB (M) = 305752kB\n[78972.506726] Node 0 hugepages_total=0 hugepages_free=0 hugepages_surp=0 hugepages_size=2048kB\n[78972.506727] Node 1 hugepages_total=0 hugepages_free=0 hugepages_surp=0 hugepages_size=2048kB\n[78972.506728] 27531091 total pagecache pages\n[78972.506729] 0 pages in swap cache\n[78972.506730] Swap cache stats: add 0, delete 0, find 0/0\n[78972.506730] Free swap  = 0kB\n[78972.506731] Total swap = 0kB\n[78972.506731] 33524975 pages RAM\n[78972.506732] 0 pages HighMem/MovableOnly\n[78972.506732] 546255 pages reserved\n[78972.620129] ntpd: page allocation stalls for 272380ms, order:0, mode:0x24000c0(GFP_KERNEL)\n[78972.620132] CPU: 16 PID: 13099 Comm: ntpd Tainted: G           O    4.9.43-cloudflare-2017.8.4 #1\n[78972.620133] Hardware name: Quanta Computer Inc D51B-2U (dual 1G LoM)/S2B-MB (dual 1G LoM), BIOS S2B_3A21 10/01/2015\n[78972.620136]  ffffc90022f9b6f8 ffffffff8142d668 ffffffff81ca31b8 0000000000000001\n[78972.620138]  ffffc90022f9b778 ffffffff81162f14 024000c022f9b740 ffffffff81ca31b8\n[78972.620140]  ffffc90022f9b720 0000000000000010 ffffc90022f9b788 ffffc90022f9b738\n[78972.620140] Call Trace:\n[78972.620148]  [&lt;ffffffff8142d668&gt;] dump_stack+0x4d/0x65\n[78972.620152]  [&lt;ffffffff81162f14&gt;] warn_alloc+0x124/0x150\n[78972.620154]  [&lt;ffffffff811638f2&gt;] __alloc_pages_slowpath+0x932/0xb80\n[78972.620157]  [&lt;ffffffff81163d42&gt;] __alloc_pages_nodemask+0x202/0x250\n[78972.620160]  [&lt;ffffffff811aeae2&gt;] alloc_pages_current+0x92/0x120\n[78972.620162]  [&lt;ffffffff8115f6ee&gt;] __get_free_pages+0xe/0x40\n[78972.620165]  [&lt;ffffffff811e747a&gt;] __pollwait+0x9a/0xe0\n[78972.620168]  [&lt;ffffffff817c9ec9&gt;] datagram_poll+0x29/0x100\n[78972.620170]  [&lt;ffffffff817b9d48&gt;] sock_poll+0x48/0xa0\n[78972.620172]  [&lt;ffffffff811e7c35&gt;] do_select+0x335/0x7b0</pre></code>\n            <p>This specific error message did seem fun:</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">[78991.546088] systemd-network: page allocation stalls for 287000ms, order:0, mode:0x24200ca(GFP_HIGHUSER_MOVABLE)</pre></code>\n            <p>You don&#39;t want your page allocations to stall for 5 minutes, especially when it&#39;s order zero allocation (smallest allocation of one 4 KiB page).</p><p>Comparing to our control nodes, the only two possible explanations were: a kernel upgrade, and the switch from Debian Jessie to Debian Stretch. We suspected the former, since CPU usage implies a kernel issue. However, just to be safe, we rolled both the kernel back to 4.4.55, and downgraded the affected nodes back to Debian Jessie. This was a reasonable compromise, since we needed to minimize downtime on production nodes.</p>\n          <div class=\"flex anchor relative\">\n            <h3 id=\"digging-a-bit-deeper\">Digging a bit deeper</h3>\n            <a href=\"#digging-a-bit-deeper\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n              <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n            </a>\n          </div>\n        <p>Keeping servers running on older kernel and distribution is not a viable long term solution. Through bisection, we found the issue lay in the Jessie to Stretch upgrade, contrary to our initial hypothesis.</p><p>Now that we knew what the problem was, we proceeded to investigate why. With the help from existing automation around <code>perf</code> and Java, we generated the following flamegraphs:</p><ul><li><p>Jessie</p></li></ul>\n            <figure class=\"kg-card kg-image-card \">\n            <a href=/content/images/2018/04/9.png>\n            <Image src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/3fhMCSmQj4IC8MLxPN2d1V/60a107967bdede0ba8c4465090fb6ec4/9.png\" alt=\"9\" class=\"kg-image\" width=\"3104\" height=\"1972\" loading=\"lazy\"/>\n            </a>\n            </figure><ul><li><p>Stretch</p></li></ul>\n            <figure class=\"kg-card kg-image-card \">\n            <a href=/content/images/2018/04/10.png>\n            <Image src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/6b523AB48TNUF2jj6OYhxi/9cadde05d8cf89187f182b56c48b3c1b/10.png\" alt=\"10\" class=\"kg-image\" width=\"3104\" height=\"1972\" loading=\"lazy\"/>\n            </a>\n            </figure><p>At first it looked like Jessie was doing <code>writev</code> instead of <code>sendfile</code>, but the full flamegraphs revealed that Strech was executing <code>sendfile</code> a lot slower.</p><p>If you highlight <code>sendfile</code>:</p><ul><li><p>Jessie</p></li></ul>\n            <figure class=\"kg-card kg-image-card \">\n            <a href=/content/images/2018/04/11.png>\n            <Image src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/6XXts4Hvy58nwa8FG3ZNfT/2e36ce3aa2b111059bcff6a21e3da712/11.png\" alt=\"11\" class=\"kg-image\" width=\"3104\" height=\"1972\" loading=\"lazy\"/>\n            </a>\n            </figure><ul><li><p>Stretch</p></li></ul>\n            <figure class=\"kg-card kg-image-card \">\n            <a href=/content/images/2018/04/12.png>\n            <Image src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/32BB3vFsnbl7ul6b6Aa5MP/10788cfa90962c2034e7be7fc6b76a1f/12.png\" alt=\"12\" class=\"kg-image\" width=\"3104\" height=\"1972\" loading=\"lazy\"/>\n            </a>\n            </figure><p>And zoomed in:</p><ul><li><p>Jessie</p></li></ul>\n            <figure class=\"kg-card kg-image-card \">\n            <a href=/content/images/2018/04/13.png>\n            <Image src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/75TU9Q58iCRcCKAt6eZxf3/4cdf2f2038bb4e7ef813ba7e21562121/13.png\" alt=\"13\" class=\"kg-image\" width=\"3104\" height=\"1972\" loading=\"lazy\"/>\n            </a>\n            </figure><ul><li><p>Stretch</p></li></ul>\n            <figure class=\"kg-card kg-image-card \">\n            <a href=/content/images/2018/04/14.png>\n            <Image src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/fgUsgL2hUrhJHg3ns5HeE/3b733ecc2ed4ebee2a751394a81804fb/14.png\" alt=\"14\" class=\"kg-image\" width=\"3104\" height=\"1972\" loading=\"lazy\"/>\n            </a>\n            </figure><p>These two look very different.</p><p>Some colleagues suggested that the differences in the graphs may be due to TCP offload being disabled, but upon checking our NIC settings, we found that the feature flags were identical.</p><p>We&#39;ll dive into the differences in the next section.</p>\n          <div class=\"flex anchor relative\">\n            <h3 id=\"and-deeper\">And deeper</h3>\n            <a href=\"#and-deeper\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n              <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n            </a>\n          </div>\n        <p>To trace latency distributions of <code>sendfile</code> syscalls between Jessie and Stretch, we used <a href=\"https://github.com/iovisor/bcc/blob/master/tools/funclatency_example.txt\"><code>funclatency</code></a> from <a href=\"https://iovisor.github.io/bcc/\">bcc-tools</a>:</p><ul><li><p>Jessie</p></li></ul>\n            <pre class=\"language-bash\"><code class=\"language-bash\">$ sudo /usr/share/bcc/tools/funclatency -uTi 1 do_sendfile\nTracing 1 functions for &quot;do_sendfile&quot;... Hit Ctrl-C to end.\n23:27:25\n     usecs               : count     distribution\n         0 -&gt; 1          : 9        |                                        |\n         2 -&gt; 3          : 47       |****                                    |\n         4 -&gt; 7          : 53       |*****                                   |\n         8 -&gt; 15         : 379      |****************************************|\n        16 -&gt; 31         : 329      |**********************************      |\n        32 -&gt; 63         : 101      |**********                              |\n        64 -&gt; 127        : 23       |**                                      |\n       128 -&gt; 255        : 50       |*****                                   |\n       256 -&gt; 511        : 7        |                                        |</pre></code>\n            <ul><li><p>Stretch</p></li></ul>\n            <pre class=\"language-bash\"><code class=\"language-bash\">$ sudo /usr/share/bcc/tools/funclatency -uTi 1 do_sendfile\nTracing 1 functions for &quot;do_sendfile&quot;... Hit Ctrl-C to end.\n23:27:28\n     usecs               : count     distribution\n         0 -&gt; 1          : 1        |                                        |\n         2 -&gt; 3          : 20       |***                                     |\n         4 -&gt; 7          : 46       |*******                                 |\n         8 -&gt; 15         : 56       |********                                |\n        16 -&gt; 31         : 65       |**********                              |\n        32 -&gt; 63         : 75       |***********                             |\n        64 -&gt; 127        : 75       |***********                             |\n       128 -&gt; 255        : 258      |****************************************|\n       256 -&gt; 511        : 144      |**********************                  |\n       512 -&gt; 1023       : 24       |***                                     |\n      1024 -&gt; 2047       : 27       |****                                    |\n      2048 -&gt; 4095       : 28       |****                                    |\n      4096 -&gt; 8191       : 35       |*****                                   |\n      8192 -&gt; 16383      : 1        |                                        |</pre></code>\n            <p>In the flamegraphs, you can see timers being set at the tip (<code>mod_timer</code> function), with these timers taking locks. On Stretch we installed 3x more timers, resulting in 10x the amount of contention:</p><ul><li><p>Jessie</p></li></ul>\n            <pre class=\"language-bash\"><code class=\"language-bash\">$ sudo /usr/share/bcc/tools/funccount -T -i 1 mod_timer\nTracing 1 functions for &quot;mod_timer&quot;... Hit Ctrl-C to end.\n00:33:36\nFUNC                                    COUNT\nmod_timer                               60482\n00:33:37\nFUNC                                    COUNT\nmod_timer                               58263\n00:33:38\nFUNC                                    COUNT\nmod_timer                               54626</pre></code>\n            \n            <pre class=\"language-bash\"><code class=\"language-bash\">$ sudo /usr/share/bcc/tools/funccount -T -i 1 lock_timer_base\nTracing 1 functions for &quot;lock_timer_base&quot;... Hit Ctrl-C to end.\n00:32:36\nFUNC                                    COUNT\nlock_timer_base                         15962\n00:32:37\nFUNC                                    COUNT\nlock_timer_base                         16261\n00:32:38\nFUNC                                    COUNT\nlock_timer_base                         15806</pre></code>\n            <ul><li><p>Stretch</p></li></ul>\n            <pre class=\"language-bash\"><code class=\"language-bash\">$ sudo /usr/share/bcc/tools/funccount -T -i 1 mod_timer\nTracing 1 functions for &quot;mod_timer&quot;... Hit Ctrl-C to end.\n00:33:28\nFUNC                                    COUNT\nmod_timer                              149068\n00:33:29\nFUNC                                    COUNT\nmod_timer                              155994\n00:33:30\nFUNC                                    COUNT\nmod_timer                              160688</pre></code>\n            \n            <pre class=\"language-bash\"><code class=\"language-bash\">$ sudo /usr/share/bcc/tools/funccount -T -i 1 lock_timer_base\nTracing 1 functions for &quot;lock_timer_base&quot;... Hit Ctrl-C to end.\n00:32:32\nFUNC                                    COUNT\nlock_timer_base                        119189\n00:32:33\nFUNC                                    COUNT\nlock_timer_base                        196895\n00:32:34\nFUNC                                    COUNT\nlock_timer_base                        140085</pre></code>\n            <p>The Linux kernel includes debugging facilities for timers, which <a href=\"https://elixir.bootlin.com/linux/v4.9.43/source/kernel/time/timer.c#L1010\">call</a> the <code>timer:timer_start</code> <a href=\"https://elixir.bootlin.com/linux/v4.9.43/source/include/trace/events/timer.h#L44\">tracepoint</a> on every timer start. This allowed us to pull up timer names:</p><ul><li><p>Jessie</p></li></ul>\n            <pre class=\"language-bash\"><code class=\"language-bash\">$ sudo perf record -e timer:timer_start -p 23485 -- sleep 10 &amp;&amp; sudo perf script | sed &#039;s/.* function=//g&#039; | awk &#039;{ print $1 }&#039; | sort | uniq -c\n[ perf record: Woken up 54 times to write data ]\n[ perf record: Captured and wrote 17.778 MB perf.data (173520 samples) ]\n      6 blk_rq_timed_out_timer\n      2 clocksource_watchdog\n      5 commit_timeout\n      5 cursor_timer_handler\n      2 dev_watchdog\n     10 garp_join_timer\n      2 ixgbe_service_timer\n     36 reqsk_timer_handler\n   4769 tcp_delack_timer\n    171 tcp_keepalive_timer\n 168512 tcp_write_timer</pre></code>\n            <ul><li><p>Stretch</p></li></ul>\n            <pre class=\"language-bash\"><code class=\"language-bash\">$ sudo perf record -e timer:timer_start -p 3416 -- sleep 10 &amp;&amp; sudo perf script | sed &#039;s/.* function=//g&#039; | awk &#039;{ print $1 }&#039; | sort | uniq -c\n[ perf record: Woken up 671 times to write data ]\n[ perf record: Captured and wrote 198.273 MB perf.data (1988650 samples) ]\n      6 clocksource_watchdog\n      4 commit_timeout\n     12 cursor_timer_handler\n      2 dev_watchdog\n     18 garp_join_timer\n      4 ixgbe_service_timer\n      1 neigh_timer_handler\n      1 reqsk_timer_handler\n   4622 tcp_delack_timer\n      1 tcp_keepalive_timer\n1983978 tcp_write_timer\n      1 writeout_period</pre></code>\n            <p>So basically we install 12x more <code>tcp_write_timer</code> timers, resulting in higher kernel CPU usage.</p><p>Taking specific flamegraphs of the timers revealed the differences in their operation:</p><ul><li><p>Jessie</p></li></ul>\n            <figure class=\"kg-card kg-image-card \">\n            <a href=/content/images/2018/04/15.png>\n            <Image src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/4PJjYK3FzgAeQxpbHPGn5i/06f546c8ea1cda3d58c4c54dd3618a15/15.png\" alt=\"15\" class=\"kg-image\" width=\"3104\" height=\"1972\" loading=\"lazy\"/>\n            </a>\n            </figure><ul><li><p>Stretch</p></li></ul>\n            <figure class=\"kg-card kg-image-card \">\n            <a href=/content/images/2018/04/16.png>\n            <Image src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/7M8XyRvy7vHDytdpWJQXAr/784aa92acf4f92c8896d08e2fede9bcd/16.png\" alt=\"16\" class=\"kg-image\" width=\"3104\" height=\"1972\" loading=\"lazy\"/>\n            </a>\n            </figure><p>We then traced the functions that were different:</p><ul><li><p>Jessie</p></li></ul>\n            <pre class=\"language-bash\"><code class=\"language-bash\">$ sudo /usr/share/bcc/tools/funccount -T -i 1 tcp_sendmsg\nTracing 1 functions for &quot;tcp_sendmsg&quot;... Hit Ctrl-C to end.\n03:33:33\nFUNC                                    COUNT\ntcp_sendmsg                             21166\n03:33:34\nFUNC                                    COUNT\ntcp_sendmsg                             21768\n03:33:35\nFUNC                                    COUNT\ntcp_sendmsg                             21712</pre></code>\n            \n            <pre class=\"language-bash\"><code class=\"language-bash\">$ sudo /usr/share/bcc/tools/funccount -T -i 1 tcp_push_one\nTracing 1 functions for &quot;tcp_push_one&quot;... Hit Ctrl-C to end.\n03:37:14\nFUNC                                    COUNT\ntcp_push_one                              496\n03:37:15\nFUNC                                    COUNT\ntcp_push_one                              432\n03:37:16\nFUNC                                    COUNT\ntcp_push_one                              495</pre></code>\n            \n            <pre class=\"language-bash\"><code class=\"language-bash\">$ sudo /usr/share/bcc/tools/trace -p 23485 &#039;tcp_sendmsg &quot;%d&quot;, arg3&#039; -T -M 100000 | awk &#039;{ print $NF }&#039; | sort | uniq -c | sort -n | tail\n   1583 4\n   2043 54\n   3546 18\n   4016 59\n   4423 50\n   5349 8\n   6154 40\n   6620 38\n  17121 51\n  39528 44</pre></code>\n            <ul><li><p>Stretch</p></li></ul>\n            <pre class=\"language-bash\"><code class=\"language-bash\">$ sudo /usr/share/bcc/tools/funccount -T -i 1 tcp_sendmsg\nTracing 1 functions for &quot;tcp_sendmsg&quot;... Hit Ctrl-C to end.\n03:33:30\nFUNC                                    COUNT\ntcp_sendmsg                             53834\n03:33:31\nFUNC                                    COUNT\ntcp_sendmsg                             49472\n03:33:32\nFUNC                                    COUNT\ntcp_sendmsg                             51221</pre></code>\n            \n            <pre class=\"language-bash\"><code class=\"language-bash\">$ sudo /usr/share/bcc/tools/funccount -T -i 1 tcp_push_one\nTracing 1 functions for &quot;tcp_push_one&quot;... Hit Ctrl-C to end.\n03:37:10\nFUNC                                    COUNT\ntcp_push_one                            64483\n03:37:11\nFUNC                                    COUNT\ntcp_push_one                            65058\n03:37:12\nFUNC                                    COUNT\ntcp_push_one                            72394</pre></code>\n            \n            <pre class=\"language-bash\"><code class=\"language-bash\">$ sudo /usr/share/bcc/tools/trace -p 3416 &#039;tcp_sendmsg &quot;%d&quot;, arg3&#039; -T -M 100000 | awk &#039;{ print $NF }&#039; | sort | uniq -c | sort -n | tail\n    396 46\n    409 4\n   1124 50\n   1305 18\n   1547 40\n   1672 59\n   1729 8\n   2181 38\n  19052 44\n  64504 4096</pre></code>\n            <p>The traces showed huge variations of <code>tcp_sendmsg</code> and <code>tcp_push_one</code> within <code>sendfile</code>.</p><p>To further introspect, we leveraged a kernel feature available since 4.9: the ability to count stacks. This led us to measuring what hits <code>tcp_push_one</code>:</p><ul><li><p>Jessie</p></li></ul>\n            <pre class=\"language-bash\"><code class=\"language-bash\">$ sudo /usr/share/bcc/tools/stackcount -i 10 tcp_push_one\nTracing 1 functions for &quot;tcp_push_one&quot;... Hit Ctrl-C to end.\n  tcp_push_one\n  inet_sendmsg\n  sock_sendmsg\n  sock_write_iter\n  do_iter_readv_writev\n  do_readv_writev\n  vfs_writev\n  do_writev\n  SyS_writev\n  do_syscall_64\n  return_from_SYSCALL_64\n    1\n  tcp_push_one\n  inet_sendpage\n  kernel_sendpage\n  sock_sendpage\n  pipe_to_sendpage\n  __splice_from_pipe\n  splice_from_pipe\n  generic_splice_sendpage\n  direct_splice_actor\n  splice_direct_to_actor\n  do_splice_direct\n  do_sendfile\n  sys_sendfile64\n  do_syscall_64\n  return_from_SYSCALL_64\n    4950</pre></code>\n            <ul><li><p>Stretch</p></li></ul>\n            <pre class=\"language-bash\"><code class=\"language-bash\">$ sudo /usr/share/bcc/tools/stackcount -i 10 tcp_push_one\nTracing 1 functions for &quot;tcp_push_one&quot;... Hit Ctrl-C to end.\n  tcp_push_one\n  inet_sendmsg\n  sock_sendmsg\n  sock_write_iter\n  do_iter_readv_writev\n  do_readv_writev\n  vfs_writev\n  do_writev\n  SyS_writev\n  do_syscall_64\n  return_from_SYSCALL_64\n    123\n  tcp_push_one\n  inet_sendmsg\n  sock_sendmsg\n  sock_write_iter\n  __vfs_write\n  vfs_write\n  SyS_write\n  do_syscall_64\n  return_from_SYSCALL_64\n    172\n  tcp_push_one\n  inet_sendmsg\n  sock_sendmsg\n  kernel_sendmsg\n  sock_no_sendpage\n  tcp_sendpage\n  inet_sendpage\n  kernel_sendpage\n  sock_sendpage\n  pipe_to_sendpage\n  __splice_from_pipe\n  splice_from_pipe\n  generic_splice_sendpage\n  direct_splice_actor\n  splice_direct_to_actor\n  do_splice_direct\n  do_sendfile\n  sys_sendfile64\n  do_syscall_64\n  return_from_SYSCALL_64\n    735110</pre></code>\n            <p>If you diff the most popular stacks, you&#39;ll get:</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">--- jessie.txt  2017-08-16 21:14:13.000000000 -0700\n+++ stretch.txt 2017-08-16 21:14:20.000000000 -0700\n@@ -1,4 +1,9 @@\n tcp_push_one\n+inet_sendmsg\n+sock_sendmsg\n+kernel_sendmsg\n+sock_no_sendpage\n+tcp_sendpage\n inet_sendpage\n kernel_sendpage\n sock_sendpage</pre></code>\n            <p>Let&#39;s look closer at <a href=\"https://elixir.bootlin.com/linux/v4.9.43/source/net/ipv4/tcp.c#L1012\"><code>tcp_sendpage</code></a>:</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">int tcp_sendpage(struct sock *sk, struct page *page, int offset,\n         size_t size, int flags)\n{\n    ssize_t res;\n\n    if (!(sk-&gt;sk_route_caps &amp; NETIF_F_SG) ||\n        !sk_check_csum_caps(sk))\n        return sock_no_sendpage(sk-&gt;sk_socket, page, offset, size,\n                    flags);\n\n    lock_sock(sk);\n\n    tcp_rate_check_app_limited(sk);  /* is sending application-limited? */\n\n    res = do_tcp_sendpages(sk, page, offset, size, flags);\n    release_sock(sk);\n    return res;\n}</pre></code>\n            <p>It looks like we don&#39;t enter the <code>if</code> body. We looked up what <a href=\"https://elixir.bootlin.com/linux/v4.9.43/source/include/linux/netdev_features.h#L115\">NET_F_SG</a> does: <a href=\"https://en.wikipedia.org/wiki/Large_send_offload\">segmentation offload</a>. This difference is peculiar, since both OS&#39;es should have this enabled.</p>\n          <div class=\"flex anchor relative\">\n            <h3 id=\"even-deeper-to-the-crux\">Even deeper, to the crux</h3>\n            <a href=\"#even-deeper-to-the-crux\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n              <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n            </a>\n          </div>\n        <p>It turned out that we had segmentation offload enabled for only a few of our NICs: <code>eth2</code>, <code>eth3</code>, and <code>bond0</code>. Our network setup can be described as follows:</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">eth2 --&gt;|              |--&gt; vlan10\n        |---&gt; bond0 --&gt;|\neth3 --&gt;|              |--&gt; vlan100</pre></code>\n            <p><b>The missing piece was that we were missing segmentation offload on VLAN interfaces, where the actual IPs live.</b></p><p>Here&#39;s the diff from <code>ethtook -k vlan10</code>:</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">$ diff -rup &lt;(ssh jessie sudo ethtool -k vlan10) &lt;(ssh stretch sudo ethtool -k vlan10)\n--- /dev/fd/63  2017-08-16 21:21:12.000000000 -0700\n+++ /dev/fd/62  2017-08-16 21:21:12.000000000 -0700\n@@ -1,21 +1,21 @@\n Features for vlan10:\n rx-checksumming: off [fixed]\n-tx-checksumming: off\n+tx-checksumming: on\n        tx-checksum-ipv4: off [fixed]\n-       tx-checksum-ip-generic: off\n+       tx-checksum-ip-generic: on\n        tx-checksum-ipv6: off [fixed]\n        tx-checksum-fcoe-crc: off\n        tx-checksum-sctp: off\n-scatter-gather: off\n-       tx-scatter-gather: off\n+scatter-gather: on\n+       tx-scatter-gather: on\n        tx-scatter-gather-fraglist: off\n-tcp-segmentation-offload: off\n-       tx-tcp-segmentation: off [requested on]\n-       tx-tcp-ecn-segmentation: off [requested on]\n-       tx-tcp-mangleid-segmentation: off [requested on]\n-       tx-tcp6-segmentation: off [requested on]\n-udp-fragmentation-offload: off [requested on]\n-generic-segmentation-offload: off [requested on]\n+tcp-segmentation-offload: on\n+       tx-tcp-segmentation: on\n+       tx-tcp-ecn-segmentation: on\n+       tx-tcp-mangleid-segmentation: on\n+       tx-tcp6-segmentation: on\n+udp-fragmentation-offload: on\n+generic-segmentation-offload: on\n generic-receive-offload: on\n large-receive-offload: off [fixed]\n rx-vlan-offload: off [fixed]</pre></code>\n            <p>So we entusiastically enabled segmentation offload:</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">$ sudo ethtool -K vlan10 sg on</pre></code>\n            <p>And it didn&#39;t help! Will the suffering ever end? Let&#39;s also enable TCP transmission checksumming offload:</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">$ sudo ethtool -K vlan10 tx on\nActual changes:\ntx-checksumming: on\n        tx-checksum-ip-generic: on\ntcp-segmentation-offload: on\n        tx-tcp-segmentation: on\n        tx-tcp-ecn-segmentation: on\n        tx-tcp-mangleid-segmentation: on\n        tx-tcp6-segmentation: on\nudp-fragmentation-offload: on</pre></code>\n            <p>Nothing. The diff is essentially empty now:</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">$ diff -rup &lt;(ssh jessie sudo ethtool -k vlan10) &lt;(ssh stretch sudo ethtool -k vlan10)\n--- /dev/fd/63  2017-08-16 21:31:27.000000000 -0700\n+++ /dev/fd/62  2017-08-16 21:31:27.000000000 -0700\n@@ -4,11 +4,11 @@ tx-checksumming: on\n        tx-checksum-ipv4: off [fixed]\n        tx-checksum-ip-generic: on\n        tx-checksum-ipv6: off [fixed]\n-       tx-checksum-fcoe-crc: off [requested on]\n-       tx-checksum-sctp: off [requested on]\n+       tx-checksum-fcoe-crc: off\n+       tx-checksum-sctp: off\n scatter-gather: on\n        tx-scatter-gather: on\n-       tx-scatter-gather-fraglist: off [requested on]\n+       tx-scatter-gather-fraglist: off\n tcp-segmentation-offload: on\n        tx-tcp-segmentation: on\n        tx-tcp-ecn-segmentation: on</pre></code>\n            <p>The last missing piece we found was that offload changes are applied only during connection initiation, so we restarted Kafka, and we immediately saw a performance improvement (green line):</p>\n            <figure class=\"kg-card kg-image-card \">\n            <a href=/content/images/2018/04/17.png>\n            <Image src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/73uyXt5y4F1L6AUULX8S9g/e80494d09daf7d0b87884c62fd5341e6/17.png\" alt=\"17\" class=\"kg-image\" width=\"3104\" height=\"1972\" loading=\"lazy\"/>\n            </a>\n            </figure><p>Not enabling offload features when possible seems like a pretty bad regression, so we filed a ticket for <code>systemd</code>:</p><ul><li><p><a href=\"https://github.com/systemd/systemd/issues/6629\">https://github.com/systemd/systemd/issues/6629</a></p></li></ul><p>In the meantime, we work around our upstream issue by enabling offload features automatically on boot if they are disabled on VLAN interfaces.</p><p>Having a fix enabled, we rebooted our <code>logs</code> Kafka cluster to upgrade to the latest kernel, and our 5 day CPU usage history yielded positive results:</p>\n            <figure class=\"kg-card kg-image-card \">\n            <a href=/content/images/2018/04/18.png>\n            <Image src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/VBuiRySNEfFiN8LQ9nUg5/a5a1881b229cb1e173663af52f3eb136/18.png\" alt=\"18\" class=\"kg-image\" width=\"3104\" height=\"1972\" loading=\"lazy\"/>\n            </a>\n            </figure><p>The DNS cluster also yielded positive results, with just 2 nodes rebooted (purple line going down):</p>\n            <figure class=\"kg-card kg-image-card \">\n            <a href=/content/images/2018/04/19.png>\n            <Image src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/4CWuJQCmMt7QarvAdU0b3g/c35ad9f7a9ab6113614f736f0e682d64/19.png\" alt=\"19\" class=\"kg-image\" width=\"3104\" height=\"1972\" loading=\"lazy\"/>\n            </a>\n            </figure>\n          <div class=\"flex anchor relative\">\n            <h3 id=\"conclusion\">Conclusion</h3>\n            <a href=\"#conclusion\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n              <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n            </a>\n          </div>\n        <p>It was an error on our part to hit performance degradation without a good regression framework in place to catch the issue. Luckily, due to our heavy use of version control, we managed to bisect the issue rather quickly, and had a temp rollback in place while root causing the problem.</p><p>In the end, enabling offload also removed RCU stalls. It&#39;s not really clear whether it was the cause or just a catalyst, but the end result speaks for itself.</p><p>On the bright side, we dug pretty deep into Linux kernel internals, and although there were fleeting moments of giving up, moving to the woods to become a park ranger, we persevered and came out of the forest successful.</p><hr/><p><i>If deep diving from high level symptoms to kernel/OS issues makes you excited, </i><a href=\"https://www.cloudflare.com/careers/\"><i>drop us a line</i></a><i>.</i></p><hr/>",
		"id": "29dWe9XJa54DvzHbTBAzEk",
		"localeList": {
			"name": "Tracing System CPU on Debian Stretch Config",
			"enUS": "English for Locale",
			"zhCN": "No Page for Locale",
			"zhHansCN": "No Page for Locale",
			"zhTW": "No Page for Locale",
			"frFR": "No Page for Locale",
			"deDE": "No Page for Locale",
			"itIT": "No Page for Locale",
			"jaJP": "No Page for Locale",
			"koKR": "No Page for Locale",
			"ptBR": "No Page for Locale",
			"esLA": "No Page for Locale",
			"esES": "No Page for Locale",
			"enAU": "No Page for Locale",
			"enCA": "No Page for Locale",
			"enIN": "No Page for Locale",
			"enGB": "No Page for Locale",
			"idID": "No Page for Locale",
			"ruRU": "No Page for Locale",
			"svSE": "No Page for Locale",
			"viVN": "No Page for Locale",
			"plPL": "No Page for Locale",
			"arAR": "No Page for Locale",
			"nlNL": "No Page for Locale",
			"thTH": "No Page for Locale",
			"trTR": "No Page for Locale",
			"heIL": "No Page for Locale",
			"lvLV": "No Page for Locale",
			"etEE": "No Page for Locale",
			"ltLT": "No Page for Locale"
		},
		"meta_description": null,
		"metadata": {
			"imgPreview": ""
		},
		"primary_author": {},
		"publicly_index": true,
		"published_at": "2018-05-13T17:00:00.000+01:00",
		"slug": "tracing-system-cpu-on-debian-stretch",
		"tags": [
			{
				"id": "48r7QV00gLMWOIcM1CSDRy",
				"name": "Speed & Reliability",
				"slug": "speed-and-reliability"
			},
			{
				"id": "4WdWDf1411wmpAnMnDwVDY",
				"name": "Kafka",
				"slug": "kafka"
			},
			{
				"id": "44lQSGkjHLg5W8THkxpMnh",
				"name": "eBPF",
				"slug": "ebpf"
			},
			{
				"id": "383iv0UQ6Lp0GZwOAxGq2p",
				"name": "Linux",
				"slug": "linux"
			}
		],
		"title": "Tracing System CPU on Debian Stretch",
		"updated_at": "2024-10-10T00:33:25.822Z",
		"url": "https://blog.cloudflare.com/tracing-system-cpu-on-debian-stretch"
	},
	"translations": {
		"posts.by": "By",
		"footer.gdpr": "GDPR",
		"lang_blurb1": "This post is also available in {lang1}.",
		"lang_blurb2": "This post is also available in {lang1} and {lang2}.",
		"lang_blurb3": "This post is also available in {lang1}, {lang2} and {lang3}.",
		"footer.press": "Press",
		"header.title": "The Cloudflare Blog",
		"search.clear": "Clear",
		"search.filter": "Filter",
		"search.source": "Source",
		"footer.careers": "Careers",
		"footer.company": "Company",
		"footer.support": "Support",
		"footer.the_net": "theNet",
		"search.filters": "Filters",
		"footer.our_team": "Our team",
		"footer.webinars": "Webinars",
		"page.more_posts": "More posts",
		"posts.time_read": "{time} min read",
		"search.language": "Language",
		"footer.community": "Community",
		"footer.resources": "Resources",
		"footer.solutions": "Solutions",
		"footer.trademark": "Trademark",
		"header.subscribe": "Subscribe",
		"footer.compliance": "Compliance",
		"footer.free_plans": "Free plans",
		"footer.impact_ESG": "Impact/ESG",
		"posts.follow_on_X": "Follow on X",
		"footer.help_center": "Help center",
		"footer.network_map": "Network Map",
		"header.please_wait": "Please Wait",
		"page.related_posts": "Related posts",
		"search.result_stat": "Results <strong>{search_range}</strong> of <strong>{search_total}</strong> for <strong>{search_keyword}</strong>",
		"footer.case_studies": "Case Studies",
		"footer.connect_2024": "Connect 2024",
		"footer.terms_of_use": "Terms of Use",
		"footer.white_papers": "White Papers",
		"footer.cloudflare_tv": "Cloudflare TV",
		"footer.community_hub": "Community Hub",
		"footer.compare_plans": "Compare plans",
		"footer.contact_sales": "Contact Sales",
		"header.contact_sales": "Contact Sales",
		"header.email_address": "Email Address",
		"page.error.not_found": "Page not found",
		"footer.developer_docs": "Developer docs",
		"footer.privacy_policy": "Privacy Policy",
		"footer.request_a_demo": "Request a demo",
		"page.continue_reading": "Continue reading",
		"footer.analysts_report": "Analyst reports",
		"footer.for_enterprises": "For enterprises",
		"footer.getting_started": "Getting Started",
		"footer.learning_center": "Learning Center",
		"footer.project_galileo": "Project Galileo",
		"pagination.newer_posts": "Newer Posts",
		"pagination.older_posts": "Older Posts",
		"posts.social_buttons.x": "Discuss on X",
		"search.icon_aria_label": "Search",
		"search.source_location": "Source/Location",
		"footer.about_cloudflare": "About Cloudflare",
		"footer.athenian_project": "Athenian Project",
		"footer.become_a_partner": "Become a partner",
		"footer.cloudflare_radar": "Cloudflare Radar",
		"footer.network_services": "Network services",
		"footer.trust_and_safety": "Trust & Safety",
		"header.get_started_free": "Get Started Free",
		"page.search.placeholder": "Search Cloudflare",
		"footer.cloudflare_status": "Cloudflare Status",
		"footer.cookie_preference": "Cookie Preferences",
		"header.valid_email_error": "Must be valid email.",
		"search.result_stat_empty": "Results <strong>{search_range}</strong> of <strong>{search_total}</strong>",
		"footer.connectivity_cloud": "Connectivity cloud",
		"footer.developer_services": "Developer services",
		"footer.investor_relations": "Investor relations",
		"page.not_found.error_code": "Error Code: 404",
		"search.autocomplete_title": "Insert a query. Press enter to send",
		"footer.logos_and_press_kit": "Logos & press kit",
		"footer.application_services": "Application services",
		"footer.get_a_recommendation": "Get a recommendation",
		"posts.social_buttons.reddit": "Discuss on Reddit",
		"footer.sse_and_sase_services": "SSE and SASE services",
		"page.not_found.outdated_link": "You may have used an outdated link, or you may have typed the address incorrectly.",
		"footer.report_security_issues": "Report Security Issues",
		"page.error.error_message_page": "Sorry, we can't find the page you are looking for.",
		"header.subscribe_notifications": "Subscribe to receive notifications of new posts:",
		"footer.cloudflare_for_campaigns": "Cloudflare for Campaigns",
		"header.subscription_confimation": "Subscription confirmed. Thank you for subscribing!",
		"posts.social_buttons.hackernews": "Discuss on Hacker News",
		"footer.diversity_equity_inclusion": "Diversity, equity & inclusion",
		"footer.critical_infrastructure_defense_project": "Critical Infrastructure Defense Project"
	}
}