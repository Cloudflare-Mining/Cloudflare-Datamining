<div class="post-content lh-copy gray1">
	<!--kg-card-begin: markdown-->
	<p>Some time ago we discovered that certain very slow downloads were getting abruptly terminated and began investigating whether that was a client (i.e. web browser) or server (i.e. us) problem.</p>
	<p>Some users were unable to download a binary file a few megabytes in length. The story was simple—the download connection was abruptly terminated even though the file was in the process of being downloaded. After a brief investigation we confirmed the problem: somewhere in our stack there was a bug.</p>
	<p>Describing the problem was simple, reproducing the problem was easy with a single <code>curl</code> command, but fixing it took surprising amount of effort.</p>
	<p><img src="https://blog.cloudflare.com/content/images/2016/03/6784957048_4661ea7dfc_z.jpg" alt=""><br>
		<small><a href="https://creativecommons.org/licenses/by/2.0/" target="_blank">CC BY 2.0</a> <a href="https://www.flickr.com/photos/jonicdao/6784957048/" target="_blank">image</a> by <a href="https://www.flickr.com/photos/jonicdao" target="_blank">jojo nicdao</a></small>
	</p>
	<p>In this article I'll describe the symptoms we saw, how we reproduced it and how we fixed it. Hopefully, by sharing our experiences we will save others from the tedious debugging we went through.</p>
	<h3 id="failingdownloads">Failing downloads</h3>
	<p>Two things caught our attention in the bug report. First, only users on mobile phones were experiencing the problem. Second, the asset causing issues—a binary file—was pretty large, at around 30MB.</p>
	<p>After a fruitful session with <code>tcpdump</code> one of our engineers was able to prepare a test case that reproduced the problem. As so often happens, once you know what you are looking for reproducing a problem is easy. In this case setting up a large file on a test domain and using the <code>--limit-rate</code> option to <code>curl</code> was enough:</p>
	<pre><code>$ curl -v http://example.com/large.bin --limit-rate 10k &gt; /dev/null
* Closing connection #0
curl: (56) Recv failure: Connection reset by peer
</code></pre>
	<p>Poking with <code>tcpdump</code> showed there was RST packet coming from our server exactly 60 seconds after the connection was established:</p>
	<pre><code>$ tcpdump -tttttni eth0 port 80
00:00:00 IP 192.168.1.10.50112 &gt; 1.2.3.4.80: Flags [S], seq 3193165162, win 43690, options [mss 65495,sackOK,TS val 143660119 ecr 0,nop,wscale 7], length 0
...
00:01:00 IP 1.2.3.4.80 &gt; 192.168.1.10.50112: Flags [R.], seq 1579198, ack 88, win 342, options [nop,nop,TS val 143675137 ecr 143675135], length 0

</code></pre>
	<p>Clearly our server was doing something wrong. The RST packet coming from CloudFlare server is just bad. The client behaves, sends ACK packets politely, consumes the data at its own pace, and then we just abruptly cut the conversation.</p>
	<h3 id="notourproblem">Not our problem</h3>
	<p>We are a heavy users of NGINX. In order to isolate the problem we set up a basic off-the-shelf NGINX server. The issue was easily <a href="https://github.com/cloudflare/cloudflare-blog/blob/master/2016-03-slow-downloads/nginx.conf" target="_blank">reproducible locally</a>:</p>
	<pre><code>$ curl --limit-rate 10k  localhost:8080/large.bin &gt; /dev/null
* Closing connection #0
curl: (56) Recv failure: Connection reset by peer
</code></pre>
	<p>This proved the problem was not specific to our setup—it was a broader NGINX issue!</p>
	<p>After some further poking we found two culprits. First, we were using the <a href="https://nginx.org/en/docs/http/ngx_http_core_module.html#reset_timedout_connection" target="_blank"><code>reset_timedout_connection</code></a> setting. This causes NGINX to close connections abruptly. When NGINX wants to time out a connection it sets <code>SO_LINGER</code> without a timeout on a socket, followed by a <code>close()</code>. This triggers the RST packet, instead of a usual graceful TCP finalization. Here's an <code>strace</code> log from NGINX:</p>
	<pre><code>04:20:22 setsockopt(5, SOL_SOCKET, SO_LINGER, {onoff=1, linger=0}, 8) = 0
04:20:22 close(5) = 0
</code></pre>
	<p>We could just have disabled the <code>reset_timedout_connection</code> setting, but that wouldn't have solved the underlying problem. Why was NGINX closing the connection in the first place?</p>
	<p>After further investigation we looked at the <a href="https://nginx.org/en/docs/http/ngx_http_core_module.html#send_timeout" target="_blank"><code>send_timeout</code></a> configuration option. The default value is 60 seconds, exactly the timeout we were seeing.</p>
	<pre><code>http {
     send_timeout 60s;
     ...
</code></pre>
	<p>The <code>send_timeout</code> option is used by NGINX to ensure that all connections will eventually drain. It controls the time allowed between successive <code>send</code>/<code>sendfile</code> calls on each connection. Generally speaking it's not fine for a single connection to use precious server resources for too long. If the download is going on too long or is plain stuck, it's okay for the HTTP server to be upset.</p>
	<p>But there was more to it than that.</p>
	<h3 id="notannginxproblemeither">Not an NGINX problem either</h3>
	<p>Armed with <code>strace</code> we investigated what NGINX actually did:</p>
	<pre><code>04:54:05 accept4(4, ...) = 5
04:54:05 sendfile(5, 9, [0], 51773484) = 5325752
04:55:05 close(5) = 0
</code></pre>
	<p>In the config we ordered NGINX to use <code>sendfile</code> to transmit the data. The call to <code>sendfile</code> succeeds and pushes 5MB of data to the send buffer. This value is interesting—it's about the amount of space we have in our default write buffer setting:</p>
	<pre><code>$ sysctl net.ipv4.tcp_wmem
net.ipv4.tcp_wmem = 4096 5242880 33554432
</code></pre>
	<p>A minute after the first long <code>sendfile</code> the socket is closed. Let's see what happens when we increase the <code>send_timeout</code> value to some big value (like 600 seconds):</p>
	<pre><code>08:21:37 accept4(4, ...) = 5
08:21:37 sendfile(5, 9, [0], 51773484) = 6024754
08:24:21 sendfile(5, 9, [6024754], 45748730) = 1768041
08:27:09 sendfile(5, 9, [7792795], 43980689) = 1768041
08:30:07 sendfile(5, 9, [9560836], 42212648) = 1768041
...
</code></pre>
	<p>After the first large push of data, <code>sendfile</code> is called more times. Between each successive call it transfers about 1.7 MB. Between these syscalls, about every 180 seconds, the socket was constantly being drained by the slow <code>curl</code>, so why didn't NGINX refill it constantly?</p>
	<h3 id="theasymmetry">The asymmetry</h3>
	<p>A motto of Unix design is "everything is a file". I prefer to think about this as: "in Unix everything can be readable and writeable when given to <code>poll</code>". But what exactly does "being readable" mean? Let's discuss the behavior of network sockets on Linux.</p>
	<p>The semantics of reading from a socket are simple:</p>
	<ul>
		<li>Calling <code>read()</code> will return the data available on the socket, until it's empty.</li>
		<li><code>poll</code> reports the socket as readable when any data is available on it.</li>
	</ul>
	<p>One might think this is symmetrical and similar conditions hold for writing to a socket, like this:</p>
	<ul>
		<li>Calling <code>write()</code> will copy data to the write buffer, up until "send buffer" memory is exhausted.</li>
		<li><code>poll</code> reports the socket is writeable if there is any space available in the send buffer.</li>
	</ul>
	<p>Surprisingly, the last point is <em>not</em> true.</p>
	<h3 id="differentcodepaths">Different code paths</h3>
	<p>It's very important to realize that in the Linux Kernel, there are two separate code paths: one for sending data and another one for checking if a socket is writeable.</p>
	<p>In order for <a href="http://lxr.free-electrons.com/source/net/ipv4/tcp.c?v=4.5#L1167" target="_blank"><code>send()</code></a> to succeed two conditions must be met:</p>
	<ul>
		<li>There must be <a href="http://lxr.free-electrons.com/source/include/net/sock.h?v=4.5#L1070" target="_blank">some space available in the send buffer</a>.</li>
		<li>The amount of enqueued, not sent, data <a href="http://lxr.free-electrons.com/source/include/net/tcp.h?v=4.5#L1691" target="_blank">must be lower than the LOWAT setting</a>. We will ignore the <a href="https://lwn.net/Articles/560082/" target="_blank">LOWAT setting</a> in this blog post.</li>
	</ul>
	<p>On the other hand, the conditions for a socket to be reported as <a href="http://lxr.free-electrons.com/source/net/ipv4/tcp.c?v=4.5#L519" target="_blank">"writeable" by <code>poll</code></a> are slightly narrower:</p>
	<ul>
		<li>There must be some space available in the send buffer.</li>
		<li>The amount of enqueued, not sent, data must be lower than the LOWAT setting.</li>
		<li>The amount of <a href="http://lxr.free-electrons.com/source/include/net/sock.h?v4.5#L1079" target="_blank">free buffer space</a> in the send buffer must be greater than <a href="http://lxr.free-electrons.com/source/include/net/sock.h?v.4.5#L785" target="_blank">half of the used send buffer space</a>.</li>
	</ul>
	<p>The last condition is critical. This means that after you fill the socket send buffer to 100%, the socket will become writeable again only when it's drained below 66% of send buffer size.</p>
	<p>Going back to our NGINX trace, the second <code>sendfile</code> we saw:</p>
	<pre><code>08:24:21 sendfile(5, 9, [6024754], 45748730) = 1768041
</code></pre>
	<p>The call succeeded in sending 1.7 MiB of data. This is close to 33% of 5 MiB, our default <code>wmem</code> send buffer size.</p>
	<p>I presume this threshold was implemented in Linux in order to avoid refilling the buffers too often. It is undesirable to wake up the sending program after each byte of the data was acknowledged by the client.</p>
	<h3 id="thesolution">The solution</h3>
	<p>With full understanding of the problem we can decisively say when it happens:</p>
	<ol>
		<li>
			<p>The socket send buffer is filled to at least 66%.</p>
		</li>
		<li>
			<p>The customer download speed is poor and it fails to drain the buffer to below 66% in 60 seconds.</p>
		</li>
		<li>
			<p>When that happens, the send buffer is not refilled in time, it's not reported as writeable, and the connection gets reset with a timeout.</p>
		</li>
	</ol>
	<p>There are a couple of ways to fix the problem.</p>
	<p>One option is to increase the <code>send_timeout</code> to, say, 280 seconds. This would ensure that given the default send buffer size, consumers faster than 50Kbps will never time out.</p>
	<p>Another choice is to reduce the <code>tcp_wmem</code> send buffers sizes.</p>
	<p>The final option is to patch NGINX to react differently on timeout. Instead of closing the connection, we could inspect the amount of data remaining in the send buffer. We can do that with <code>ioctl(TIOCOUTQ)</code>. With this information we know exactly how quickly the connection is being drained. If it's above some configurable threshold, we could decide to grant the connection some more time.</p>
	<p>My colleague Chris Branch prepared <a href="https://github.com/cloudflare/cloudflare-blog/blob/master/2016-03-slow-downloads/nginx_send_minimum_rate.patch" target="_blank">a Linux specific patch to NGINX</a>. It implements a <code>send_minimum_rate</code> option, which is used to specify the minimum permitted client throughput.</p>
	<h3 id="summary">Summary</h3>
	<p>The Linux networking stack is very complex. While it usually works really well, sometimes it gives us a surprise. Even very experienced programmers don't fully understand all the corner cases. During debugging we learned that setting timeouts in the "write" path of the code requires special attention. You can't just treat the "write" timeouts in the same way as "read" timeouts.</p>
	<p>It was a surprise to me that the semantics of a socket being "writeable" are not symmetrical to the "readable" state.</p>
	<p>In past <a href="https://blog.cloudflare.com/the-story-of-one-latency-spike/">we found that raising receive buffers</a> can have unexpected consequences. Now we know tweaking <code>wmem</code> values can affect something totally different—NGINX send timeouts.</p>
	<p>Tuning a CDN to work well for all the users takes a lot of work. This write up is a result of hard work done by four engineers (special thanks to Chris Branch!). If this sounds interesting, <a href="https://www.cloudflare.com/join-our-team/" target="_blank">consider applying</a>!</p>
	<!--kg-card-end: markdown-->
</div>