{
	"footerBlurb": "Cloudflare's connectivity cloud protects <a target='_blank' href='https://www.cloudflare.com/network-services/' rel='noreferrer'>entire corporate networks</a>, helps customers build <a target='_blank' href='https://workers.cloudflare.com/' rel='noreferrer'>Internet-scale applications efficiently</a>, accelerates any <a target='_blank' href='https://www.cloudflare.com/performance/accelerate-internet-applications/' rel='noreferrer'>website or Internet application</a>, <a target='_blank' href='https://www.cloudflare.com/ddos/' rel='noreferrer'>wards off DDoS attacks</a>, keeps <a target='_blank' href='https://www.cloudflare.com/application-security/' rel='noreferrer'>hackers at bay</a>, and can help you on <a target='_blank' href='https://www.cloudflare.com/products/zero-trust/' rel='noreferrer'>your journey to Zero Trust</a>.<br/><br/>Visit <a target='_blank' href='https://one.one.one.one/' rel='noreferrer'>1.1.1.1</a> from any device to get started with our free app that makes your Internet faster and safer.<br/><br/>To learn more about our mission to help build a better Internet, <a target='_blank' href='https://www.cloudflare.com/learning/what-is-cloudflare/' rel='noreferrer'>start here</a>. If you&apos;re looking for a new career direction, check out <a target='_blank' href='http://www.cloudflare.com/careers' rel='noreferrer'>our open positions</a>.",
	"initialReadingTime": "13",
	"locale": "en-us",
	"localesAvailable": [
		"zh-cn",
		"ko-kr"
	],
	"post": {
		"authors": [
			{
				"name": "Marek Majkowski",
				"slug": "marek-majkowski",
				"bio": null,
				"profile_image": "https://cf-assets.www.cloudflare.com/zkvhlag99gkb/1JuU5qavgwVeqR8BAUrd6U/3a0d0445d41c9a3c42011046efe9c37b/marek-majkowski.jpeg",
				"location": null,
				"website": null,
				"twitter": "@majek04",
				"facebook": null,
				"publiclyIndex": true
			}
		],
		"excerpt": "We noticed something weird - the TCP sockets which we thought should have been closed - were lingering around. We realized we don't really understand when TCP sockets are supposed to time out!\n\nWe naively thought enabling TCP keepalives would be enough... but it isn't!",
		"feature_image": "https://cf-assets.www.cloudflare.com/zkvhlag99gkb/5jngEokSW0B3fkcnXiXP0x/b76883548bfb4a1132a5b3e4e9ebfeb4/when-tcp-sockets-refuse-to-die.png",
		"featured": false,
		"html": "<p>While working on our <a href=\"https://www.cloudflare.com/products/cloudflare-spectrum/\">Spectrum server</a>, we noticed something weird: the TCP sockets which we thought should have been closed were lingering around. We realized we don&#39;t really understand when TCP sockets are supposed to time out!</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/4J7NyByY5rLMwGCjildxuX/a80e344de39529860fe89230fff4259c/Tcp_state_diagram_fixed_new.svga.png\" alt=\"Tcp_state_diagram_fixed_new.svga\" class=\"kg-image\" width=\"1000\" height=\"754\" loading=\"lazy\"/>\n            \n            </figure><p><a href=\"https://commons.wikimedia.org/wiki/File:Tcp_state_diagram_fixed_new.svg\">Image</a> by Sergiodc2 CC BY SA 3.0</p><p>In our code, we wanted to make sure we don&#39;t hold connections to dead hosts. In our early code we naively thought enabling TCP keepalives would be enough... but it isn&#39;t. It turns out a fairly modern <a href=\"https://tools.ietf.org/html/rfc5482\">TCP_USER_TIMEOUT</a> socket option is equally important. Furthermore, it interacts with TCP keepalives in subtle ways. <a href=\"http://codearcana.com/posts/2015/08/28/tcp-keepalive-is-a-lie.html\">Many people</a> are confused by this.</p><p>In this blog post, we&#39;ll try to show how these options work. We&#39;ll show how a TCP socket can time out during various stages of its lifetime, and how TCP keepalives and user timeout influence that. To better illustrate the internals of TCP connections, we&#39;ll mix the outputs of the <code>tcpdump</code> and the <code>ss -o</code> commands. This nicely shows the transmitted packets and the changing parameters of the TCP connections.</p>\n    <div class=\"flex anchor relative\">\n      <h2 id=\"syn-sent\">SYN-SENT</h2>\n      <a href=\"#syn-sent\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>Let&#39;s start from the simplest case - what happens when one attempts to establish a connection to a server which discards inbound SYN packets?</p><p>The scripts used here <a href=\"https://github.com/cloudflare/cloudflare-blog/tree/master/2019-09-tcp-keepalives\">are available on our GitHub</a>.</p><p><code>$ sudo ./test-syn-sent.py\n# all packets dropped\n00:00.000 IP host.2 &gt; host.1: Flags [S] # initial SYN\n\nState    Recv-Q Send-Q Local:Port Peer:Port\nSYN-SENT 0      1      host:2     host:1    timer:(on,940ms,0)\n\n00:01.028 IP host.2 &gt; host.1: Flags [S] # first retry\n00:03.044 IP host.2 &gt; host.1: Flags [S] # second retry\n00:07.236 IP host.2 &gt; host.1: Flags [S] # third retry\n00:15.427 IP host.2 &gt; host.1: Flags [S] # fourth retry\n00:31.560 IP host.2 &gt; host.1: Flags [S] # fifth retry\n01:04.324 IP host.2 &gt; host.1: Flags [S] # sixth retry\n02:10.000 connect ETIMEDOUT</code></p><p>Ok, this was easy. After the <code>connect()</code> syscall, the operating system sends a SYN packet. Since it didn&#39;t get any response the OS will by default retry sending it 6 times. This can be tweaked by the sysctl:</p><p><code>$ sysctl net.ipv4.tcp_syn_retries\nnet.ipv4.tcp_syn_retries = 6</code></p><p>It&#39;s possible to overwrite this setting per-socket with the TCP_SYNCNT setsockopt:</p><p><code>setsockopt(sd, IPPROTO_TCP, TCP_SYNCNT, 6);</code></p><p>The retries are staggered at 1s, 3s, 7s, 15s, 31s, 63s marks (the inter-retry time starts at 2s and then doubles each time). By default, the whole process takes 130 seconds, until the kernel gives up with the ETIMEDOUT errno. At this moment in the lifetime of a connection, SO_KEEPALIVE settings are ignored, but TCP_USER_TIMEOUT is not. For example, setting it to 5000ms, will cause the following interaction:</p><p><code>$ sudo ./test-syn-sent.py 5000\n# all packets dropped\n00:00.000 IP host.2 &gt; host.1: Flags [S] # initial SYN\n\nState    Recv-Q Send-Q Local:Port Peer:Port\nSYN-SENT 0      1      host:2     host:1    timer:(on,996ms,0)\n\n00:01.016 IP host.2 &gt; host.1: Flags [S] # first retry\n00:03.032 IP host.2 &gt; host.1: Flags [S] # second retry\n00:05.016 IP host.2 &gt; host.1: Flags [S] # what is this?\n00:05.024 IP host.2 &gt; host.1: Flags [S] # what is this?\n00:05.036 IP host.2 &gt; host.1: Flags [S] # what is this?\n00:05.044 IP host.2 &gt; host.1: Flags [S] # what is this?\n00:05.050 connect ETIMEDOUT</code></p><p>Even though we set user-timeout to 5s, we still saw the six SYN retries on the wire. This behaviour is probably a bug (as tested on 5.2 kernel): we would expect only two retries to be sent - at 1s and 3s marks and the socket to expire at 5s mark. Instead, we saw this, but also we saw further 4 retransmitted SYN packets aligned to 5s mark - which makes no sense. Anyhow, we learned a thing - the TCP_USER_TIMEOUT does affect the behaviour of <code>connect()</code>.</p>\n    <div class=\"flex anchor relative\">\n      <h2 id=\"syn-recv\">SYN-RECV</h2>\n      <a href=\"#syn-recv\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>SYN-RECV sockets are usually hidden from the application. They live as mini-sockets on the SYN queue. We wrote about <a href=\"/syn-packet-handling-in-the-wild/\">the SYN and Accept queues in the past</a>. Sometimes, when SYN cookies are enabled, the sockets may skip the SYN-RECV state altogether.</p><p>In SYN-RECV state, the socket will retry sending SYN+ACK 5 times as controlled by:</p><p><code>$ sysctl net.ipv4.tcp_synack_retries\nnet.ipv4.tcp_synack_retries = 5</code></p><p>Here is how it looks on the wire:</p><p><code>$ sudo ./test-syn-recv.py\n00:00.000 IP host.2 &gt; host.1: Flags [S]\n# all subsequent packets dropped\n00:00.000 IP host.1 &gt; host.2: Flags [S.] # initial SYN+ACK\n\nState    Recv-Q Send-Q Local:Port Peer:Port\nSYN-RECV 0      0      host:1     host:2    timer:(on,996ms,0)\n\n00:01.033 IP host.1 &gt; host.2: Flags [S.] # first retry\n00:03.045 IP host.1 &gt; host.2: Flags [S.] # second retry\n00:07.301 IP host.1 &gt; host.2: Flags [S.] # third retry\n00:15.493 IP host.1 &gt; host.2: Flags [S.] # fourth retry\n00:31.621 IP host.1 &gt; host.2: Flags [S.] # fifth retry\n01:04:610 SYN-RECV disappears</code></p><p>With default settings, the SYN+ACK is re-transmitted at 1s, 3s, 7s, 15s, 31s marks, and the SYN-RECV socket disappears at the 64s mark.</p><p>Neither SO_KEEPALIVE nor TCP_USER_TIMEOUT affect the lifetime of SYN-RECV sockets.</p>\n    <div class=\"flex anchor relative\">\n      <h2 id=\"final-handshake-ack\">Final handshake ACK</h2>\n      <a href=\"#final-handshake-ack\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>After receiving the second packet in the TCP handshake - the SYN+ACK - the client socket moves to an ESTABLISHED state. The server socket remains in SYN-RECV until it receives the final ACK packet.</p><p>Losing this ACK doesn&#39;t change anything - the server socket will just take a bit longer to move from SYN-RECV to ESTAB. Here is how it looks:</p><p><code>00:00.000 IP host.2 &gt; host.1: Flags [S]\n00:00.000 IP host.1 &gt; host.2: Flags [S.]\n00:00.000 IP host.2 &gt; host.1: Flags [.] # initial ACK, dropped\n\nState    Recv-Q Send-Q Local:Port  Peer:Port\nSYN-RECV 0      0      host:1      host:2 timer:(on,1sec,0)\nESTAB    0      0      host:2      host:1\n\n00:01.014 IP host.1 &gt; host.2: Flags [S.]\n00:01.014 IP host.2 &gt; host.1: Flags [.]  # retried ACK, dropped\n\nState    Recv-Q Send-Q Local:Port Peer:Port\nSYN-RECV 0      0      host:1     host:2    timer:(on,1.012ms,1)\nESTAB    0      0      host:2     host:1</code></p><p>As you can see SYN-RECV, has the &quot;on&quot; timer, the same as in example before. We might argue this final ACK doesn&#39;t really carry much weight. This thinking lead to the development of TCP_DEFER_ACCEPT feature - it basically causes the third ACK to be silently dropped. With this flag set the socket remains in SYN-RECV state until it receives the first packet with actual data:</p><p><code>$ sudo ./test-syn-ack.py\n00:00.000 IP host.2 &gt; host.1: Flags [S]\n00:00.000 IP host.1 &gt; host.2: Flags [S.]\n00:00.000 IP host.2 &gt; host.1: Flags [.] # delivered, but the socket stays as SYN-RECV\n\nState    Recv-Q Send-Q Local:Port Peer:Port\nSYN-RECV 0      0      host:1     host:2    timer:(on,7.192ms,0)\nESTAB    0      0      host:2     host:1\n\n00:08.020 IP host.2 &gt; host.1: Flags [P.], length 11  # payload moves the socket to ESTAB\n\nState Recv-Q Send-Q Local:Port Peer:Port\nESTAB 11     0      host:1     host:2\nESTAB 0      0      host:2     host:1</code></p><p>The server socket remained in the SYN-RECV state even after receiving the final TCP-handshake ACK. It has a funny &quot;on&quot; timer, with the counter stuck at 0 retries. It is converted to ESTAB - and moved from the SYN to the accept queue - after the client sends a data packet or after the TCP_DEFER_ACCEPT timer expires. Basically, with DEFER ACCEPT the SYN-RECV mini-socket <a href=\"https://marc.info/?l=linux-netdev&m=118793048828251&w=2\">discards the data-less inbound ACK</a>.</p>\n    <div class=\"flex anchor relative\">\n      <h2 id=\"idle-estab-is-forever\">Idle ESTAB is forever</h2>\n      <a href=\"#idle-estab-is-forever\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>Let&#39;s move on and discuss a fully-established socket connected to an unhealthy (dead) peer. After completion of the handshake, the sockets on both sides move to the ESTABLISHED state, like:</p><p><code>State Recv-Q Send-Q Local:Port Peer:Port\nESTAB 0      0      host:2     host:1\nESTAB 0      0      host:1     host:2</code></p><p>These sockets have no running timer by default - they will remain in that state forever, even if the communication is broken. The TCP stack will notice problems only when one side attempts to send something. This raises a question - what to do if you don&#39;t plan on sending any data over a connection? How do you make sure an idle connection is healthy, without sending any data over it?</p><p>This is where TCP keepalives come in. Let&#39;s see it in action - in this example we used the following toggles:</p><ul><li><p>SO_KEEPALIVE = 1 - Let&#39;s enable keepalives.</p></li><li><p>TCP_KEEPIDLE = 5 - Send first keepalive probe after 5 seconds of idleness.</p></li><li><p>TCP_KEEPINTVL = 3 - Send subsequent keepalive probes after 3 seconds.</p></li><li><p>TCP_KEEPCNT = 3 - Time out after three failed probes.</p></li></ul><p><code>$ sudo ./test-idle.py\n00:00.000 IP host.2 &gt; host.1: Flags [S]\n00:00.000 IP host.1 &gt; host.2: Flags [S.]\n00:00.000 IP host.2 &gt; host.1: Flags [.]\n\nState Recv-Q Send-Q Local:Port Peer:Port\nESTAB 0      0      host:1     host:2\nESTAB 0      0      host:2     host:1  timer:(keepalive,2.992ms,0)\n\n# all subsequent packets dropped\n00:05.083 IP host.2 &gt; host.1: Flags [.], ack 1 # first keepalive probe\n00:08.155 IP host.2 &gt; host.1: Flags [.], ack 1 # second keepalive probe\n00:11.231 IP host.2 &gt; host.1: Flags [.], ack 1 # third keepalive probe\n00:14.299 IP host.2 &gt; host.1: Flags [R.], seq 1, ack 1</code></p><p>Indeed! We can clearly see the first probe sent at the 5s mark, two remaining probes 3s apart - exactly as we specified. After a total of three sent probes, and a further three seconds of delay, the connection dies with ETIMEDOUT, and final the RST is transmitted.</p><p>For keepalives to work, the send buffer must be empty. You can notice the keepalive timer active in the &quot;timer:(keepalive)&quot; line.</p>\n    <div class=\"flex anchor relative\">\n      <h2 id=\"keepalives-with-tcp_user_timeout-are-confusing\">Keepalives with TCP_USER_TIMEOUT are confusing</h2>\n      <a href=\"#keepalives-with-tcp_user_timeout-are-confusing\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>We mentioned the TCP_USER_TIMEOUT option before. It sets the maximum amount of time that transmitted data may remain unacknowledged before the kernel forcefully closes the connection. On its own, it doesn&#39;t do much in the case of idle connections. The sockets will remain ESTABLISHED even if the connectivity is dropped. However, this socket option does change the semantics of TCP keepalives. <a href=\"https://linux.die.net/man/7/tcp\">The tcp(7) manpage</a> is somewhat confusing:</p><p><i>Moreover, when used with the TCP keepalive (SO_KEEPALIVE) option, TCP_USER_TIMEOUT will override keepalive to determine when to close a connection due to keepalive failure.</i></p><p>The original commit message has slightly more detail:</p><ul><li><p><a href=\"https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=dca43c75e7e545694a9dd6288553f55c53e2a3a3\">tcp: Add TCP_USER_TIMEOUT socket option</a></p></li></ul><p>To understand the semantics, we need to look at the <a href=\"https://github.com/torvalds/linux/blob/b41dae061bbd722b9d7fa828f35d22035b218e18/net/ipv4/tcp_timer.c#L693-L697\">kernel code in linux/net/ipv4/tcp_timer.c:693</a>:</p><p><code>if ((icsk-&gt;icsk_user_timeout != 0 &amp;&amp;\nelapsed &gt;= msecs_to_jiffies(icsk-&gt;icsk_user_timeout) &amp;&amp;\nicsk-&gt;icsk_probes_out &gt; 0) ||</code></p><p>For the user timeout to have any effect, the <code>icsk_probes_out</code> must not be zero. The check for user timeout is done only <i>after</i> the first probe went out. Let&#39;s check it out. Our connection settings:</p><ul><li><p>TCP_USER_TIMEOUT = 5*1000 - 5 seconds</p></li><li><p>SO_KEEPALIVE = 1 - enable keepalives</p></li><li><p>TCP_KEEPIDLE = 1 - send first probe quickly - 1 second idle</p></li><li><p>TCP_KEEPINTVL = 11 - subsequent probes every 11 seconds</p></li><li><p>TCP_KEEPCNT = 3 - send three probes before timing out</p></li></ul><p><code>00:00.000 IP host.2 &gt; host.1: Flags [S]\n00:00.000 IP host.1 &gt; host.2: Flags [S.]\n00:00.000 IP host.2 &gt; host.1: Flags [.]\n\n# all subsequent packets dropped\n00:01.001 IP host.2 &gt; host.1: Flags [.], ack 1 # first probe\n00:12.233 IP host.2 &gt; host.1: Flags [R.] # timer for second probe fired, socket aborted due to TCP_USER_TIMEOUT</code></p><p>So what happened? The connection sent the first keepalive probe at the 1s mark. Seeing no response the TCP stack then woke up 11 seconds later to send a second probe. This time though, it executed the USER_TIMEOUT code path, which decided to terminate the connection immediately.</p><p>What if we bump TCP_USER_TIMEOUT to larger values, say between the second and third probe? Then, the connection will be closed on the third probe timer. With TCP_USER_TIMEOUT set to 12.5s:</p><p><code>00:01.022 IP host.2 &gt; host.1: Flags [.] # first probe\n00:12.094 IP host.2 &gt; host.1: Flags [.] # second probe\n00:23.102 IP host.2 &gt; host.1: Flags [R.] # timer for third probe fired, socket aborted due to TCP_USER_TIMEOUT</code></p><p>We’ve shown how TCP_USER_TIMEOUT interacts with keepalives for small and medium values. The last case is when TCP_USER_TIMEOUT is extraordinarily large. Say we set it to 30s:</p><p><code>00:01.027 IP host.2 &gt; host.1: Flags [.], ack 1 # first probe\n00:12.195 IP host.2 &gt; host.1: Flags [.], ack 1 # second probe\n00:23.207 IP host.2 &gt; host.1: Flags [.], ack 1 # third probe\n00:34.211 IP host.2 &gt; host.1: Flags [.], ack 1 # fourth probe! But TCP_KEEPCNT was only 3!\n00:45.219 IP host.2 &gt; host.1: Flags [.], ack 1 # fifth probe!\n00:56.227 IP host.2 &gt; host.1: Flags [.], ack 1 # sixth probe!\n01:07.235 IP host.2 &gt; host.1: Flags [R.], seq 1 # TCP_USER_TIMEOUT aborts conn on 7th probe timer</code></p><p>We saw six keepalive probes on the wire! With TCP_USER_TIMEOUT set, the TCP_KEEPCNT is totally ignored. If you want TCP_KEEPCNT to make sense, the only sensible USER_TIMEOUT value is slightly smaller than:</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">TCP_KEEPIDLE + TCP_KEEPINTVL * TCP_KEEPCNT</pre></code>\n            \n    <div class=\"flex anchor relative\">\n      <h2 id=\"busy-estab-socket-is-not-forever\">Busy ESTAB socket is not forever</h2>\n      <a href=\"#busy-estab-socket-is-not-forever\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>Thus far we have discussed the case where the connection is idle. Different rules apply when the connection has unacknowledged data in a send buffer.</p><p>Let&#39;s prepare another experiment - after the three-way handshake, let&#39;s set up a firewall to drop all packets. Then, let&#39;s do a <code>send</code> on one end to have some dropped packets in-flight. An experiment shows the sending socket dies after ~16 minutes:</p><p><code>00:00.000 IP host.2 &gt; host.1: Flags [S]\n00:00.000 IP host.1 &gt; host.2: Flags [S.]\n00:00.000 IP host.2 &gt; host.1: Flags [.]\n\n# All subsequent packets dropped\n00:00.206 IP host.2 &gt; host.1: Flags [P.], length 11 # first data packet\n00:00.412 IP host.2 &gt; host.1: Flags [P.], length 11 # early retransmit, doesn&#39;t count\n00:00.620 IP host.2 &gt; host.1: Flags [P.], length 11 # 1nd retry\n00:01.048 IP host.2 &gt; host.1: Flags [P.], length 11 # 2rd retry\n00:01.880 IP host.2 &gt; host.1: Flags [P.], length 11 # 3th retry\n\nState Recv-Q Send-Q Local:Port Peer:Port\nESTAB 0      0      host:1     host:2\nESTAB 0      11     host:2     host:1    timer:(on,1.304ms,3)\n\n00:03.543 IP host.2 &gt; host.1: Flags [P.], length 11 # 4th\n00:07.000 IP host.2 &gt; host.1: Flags [P.], length 11 # 5th\n00:13.656 IP host.2 &gt; host.1: Flags [P.], length 11 # 6th\n00:26.968 IP host.2 &gt; host.1: Flags [P.], length 11 # 7th\n00:54.616 IP host.2 &gt; host.1: Flags [P.], length 11 # 8th\n01:47.868 IP host.2 &gt; host.1: Flags [P.], length 11 # 9th\n03:34.360 IP host.2 &gt; host.1: Flags [P.], length 11 # 10th\n05:35.192 IP host.2 &gt; host.1: Flags [P.], length 11 # 11th\n07:36.024 IP host.2 &gt; host.1: Flags [P.], length 11 # 12th\n09:36.855 IP host.2 &gt; host.1: Flags [P.], length 11 # 13th\n11:37.692 IP host.2 &gt; host.1: Flags [P.], length 11 # 14th\n13:38.524 IP host.2 &gt; host.1: Flags [P.], length 11 # 15th\n15:39.500 connection ETIMEDOUT</code></p><p>The data packet is retransmitted 15 times, as controlled by:</p><p><code>$ sysctl net.ipv4.tcp_retries2\nnet.ipv4.tcp_retries2 = 15</code></p><p>From the <a href=\"https://www.kernel.org/doc/Documentation/networking/ip-sysctl.txt\"><code>ip-sysctl.txt</code></a> documentation:</p><p><i>The default value of 15 yields a hypothetical timeout of 924.6 seconds and is a lower bound for the effective timeout. TCP will effectively time out at the first RTO which exceeds the hypothetical timeout.</i></p><p>The connection indeed died at ~940 seconds. Notice the socket has the &quot;on&quot; timer running. It doesn&#39;t matter at all if we set SO_KEEPALIVE - when the &quot;on&quot; timer is running, keepalives are not engaged.</p><p>TCP_USER_TIMEOUT keeps on working though. The connection will be aborted <i>exactly</i> after user-timeout specified time since the last received packet. With the user timeout set the <code>tcp_retries2</code> value is ignored.</p>\n    <div class=\"flex anchor relative\">\n      <h2 id=\"zero-window-estab-is-forever\">Zero window ESTAB is... forever?</h2>\n      <a href=\"#zero-window-estab-is-forever\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>There is one final case worth mentioning. If the sender has plenty of data, and the receiver is slow, then TCP flow control kicks in. At some point the receiver will ask the sender to stop transmitting new data. This is a slightly different condition than the one described above.</p><p>In this case, with flow control engaged, there is no in-flight or unacknowledged data. Instead the receiver throttles the sender with a &quot;zero window&quot; notification. Then the sender periodically checks if the condition is still valid with &quot;window probes&quot;. In this experiment we reduced the receive buffer size for simplicity. Here&#39;s how it looks on the wire:</p><p><code>00:00.000 IP host.2 &gt; host.1: Flags [S]\n00:00.000 IP host.1 &gt; host.2: Flags [S.], win 1152\n00:00.000 IP host.2 &gt; host.1: Flags [.]</code></p><p><code>00:00.202 IP host.2 &gt; host.1: Flags [.], length 576 # first data packet\n00:00.202 IP host.1 &gt; host.2: Flags [.], ack 577, win 576\n00:00.202 IP host.2 &gt; host.1: Flags [P.], length 576 # second data packet\n00:00.244 IP host.1 &gt; host.2: Flags [.], ack 1153, win 0 # throttle it! zero-window</code></p><p><code>00:00.456 IP host.2 &gt; host.1: Flags [.], ack 1 # zero-window probe\n00:00.456 IP host.1 &gt; host.2: Flags [.], ack 1153, win 0 # nope, still zero-window</code></p><p><code>State Recv-Q Send-Q Local:Port Peer:Port\nESTAB 1152   0      host:1     host:2\nESTAB 0      129920 host:2     host:1  timer:(persist,048ms,0)</code></p><p>The packet capture shows a couple of things. First, we can see two packets with data, each 576 bytes long. They both were immediately acknowledged. The second ACK had &quot;win 0&quot; notification: the sender was told to stop sending data.</p><p>But the sender is eager to send more! The last two packets show a first &quot;window probe&quot;: the sender will periodically send payload-less &quot;ack&quot; packets to check if the window size had changed. As long as the receiver keeps on answering, the sender will keep on sending such probes forever.</p><p>The socket information shows three important things:</p><ul><li><p>The read buffer of the reader is filled - thus the &quot;zero window&quot; throttling is expected.</p></li><li><p>The write buffer of the sender is filled - we have more data to send.</p></li><li><p>The sender has a &quot;persist&quot; timer running, counting the time until the next &quot;window probe&quot;.</p></li></ul><p>In this blog post we are interested in timeouts - what will happen if the window probes are lost? Will the sender notice?</p><p>By default, the window probe is retried 15 times - adhering to the usual <code>tcp_retries2</code> setting.</p><p>The tcp timer is in <code>persist</code> state, so the TCP keepalives will <i>not</i> be running. The SO_KEEPALIVE settings don&#39;t make any difference when window probing is engaged.</p><p>As expected, the TCP_USER_TIMEOUT toggle keeps on working. A slight difference is that similarly to user-timeout on keepalives, it&#39;s engaged only when the retransmission timer fires. During such an event, if more than user-timeout seconds since the last good packet passed, the connection will be aborted.</p>\n    <div class=\"flex anchor relative\">\n      <h2 id=\"note-about-using-application-timeouts\">Note about using application timeouts</h2>\n      <a href=\"#note-about-using-application-timeouts\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>In the past we have shared an interesting war story:</p><ul><li><p><a href=\"/the-curious-case-of-slow-downloads/\">The curious case of slow downloads</a></p></li></ul><p>Our HTTP server gave up on the connection after an application-managed timeout fired. This was a bug - a slow connection might have correctly slowly drained the send buffer, but the application server didn&#39;t notice that.</p><p>We abruptly dropped slow downloads, even though this wasn&#39;t our intention. We just wanted to make sure the client connection was still healthy. It would be better to use TCP_USER_TIMEOUT than rely on application-managed timeouts.</p><p>But this is not sufficient. We also wanted to guard against a situation where a client stream is valid, but is stuck and doesn&#39;t drain the connection. The only way to achieve this is to periodically check the amount of unsent data in the send buffer, and see if it shrinks at a desired pace.</p><p>For typical applications sending data to the Internet, I would recommend:</p><ol><li><p>Enable TCP keepalives. This is needed to keep some data flowing in the idle-connection case.</p></li><li><p>Set TCP_USER_TIMEOUT to <code>TCP_KEEPIDLE + TCP_KEEPINTVL * TCP_KEEPCNT</code>.</p></li><li><p>Be careful when using application-managed timeouts. To detect TCP failures use TCP keepalives and user-timeout. If you want to spare resources and make sure sockets don&#39;t stay alive for too long, consider periodically checking if the socket is draining at the desired pace. You can use <code>ioctl(TIOCOUTQ)</code> for that, but it counts both data buffered (notsent) on the socket and in-flight (unacknowledged) bytes. A better way is to use TCP_INFO tcpi_notsent_bytes parameter, which reports only the former counter.</p></li></ol><p>An example of checking the draining pace:</p><p><code>while True:\nnotsent1 = get_tcp_info(c).tcpi_notsent_bytes\nnotsent1_ts = time.time()\n...\npoll.poll(POLL_PERIOD)\n...\nnotsent2 = get_tcp_info(c).tcpi_notsent_bytes\nnotsent2_ts = time.time()\npace_in_bytes_per_second = (notsent1 - notsent2) / (notsent2_ts - notsent1_ts)\nif pace_in_bytes_per_second &gt; 12000:\n# pace is above effective rate of 96Kbps, ok!\nelse:\n# socket is too slow...</code></p><p>There are ways to further improve this logic. We could use <a href=\"https://lwn.net/Articles/560082/\"><code>TCP_NOTSENT_LOWAT</code></a>, although it&#39;s generally only useful for situations where the send buffer is relatively empty. Then we could use the <a href=\"https://www.kernel.org/doc/Documentation/networking/timestamping.txt\"><code>SO_TIMESTAMPING</code></a> interface for notifications about when data gets delivered. Finally, if we are done sending the data to the socket, it&#39;s possible to just call <code>close()</code> and defer handling of the socket to the operating system. Such a socket will be stuck in FIN-WAIT-1 or LAST-ACK state until it correctly drains.</p>\n    <div class=\"flex anchor relative\">\n      <h2 id=\"summary\">Summary</h2>\n      <a href=\"#summary\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>In this post we discussed five cases where the TCP connection may notice the other party going away:</p><ul><li><p>SYN-SENT: The duration of this state can be controlled by <code>TCP_SYNCNT</code> or <code>tcp_syn_retries</code>.</p></li><li><p>SYN-RECV: It&#39;s usually hidden from application. It is tuned by <code>tcp_synack_retries</code>.</p></li><li><p>Idling ESTABLISHED connection, will never notice any issues. A solution is to use TCP keepalives.</p></li><li><p>Busy ESTABLISHED connection, adheres to <code>tcp_retries2</code> setting, and ignores TCP keepalives.</p></li><li><p>Zero-window ESTABLISHED connection, adheres to <code>tcp_retries2</code> setting, and ignores TCP keepalives.</p></li></ul><p>Especially the last two ESTABLISHED cases can be customized with TCP_USER_TIMEOUT, but this setting also affects other situations. Generally speaking, it can be thought of as a hint to the kernel to abort the connection after so-many seconds since the last good packet. This is a dangerous setting though, and if used in conjunction with TCP keepalives should be set to a value slightly lower than <code>TCP_KEEPIDLE + TCP_KEEPINTVL * TCP_KEEPCNT</code>. Otherwise it will affect, and potentially cancel out, the TCP_KEEPCNT value.</p><p>In this post we presented scripts showing the effects of timeout-related socket options under various network conditions. Interleaving the <code>tcpdump</code> packet capture with the output of <code>ss -o</code> is a great way of understanding the networking stack. We were able to create reproducible test cases showing the &quot;on&quot;, &quot;keepalive&quot; and &quot;persist&quot; timers in action. This is a very useful framework for further experimentation.</p><p>Finally, it&#39;s surprisingly hard to tune a TCP connection to be confident that the remote host is actually up. During our debugging we found that looking at the send buffer size and currently active TCP timer can be very helpful in understanding whether the socket is actually healthy. The bug in our Spectrum application turned out to be a wrong TCP_USER_TIMEOUT setting - without it sockets with large send buffers were lingering around for way longer than we intended.</p><p>The scripts used in this article <a href=\"https://github.com/cloudflare/cloudflare-blog/tree/master/2019-09-tcp-keepalives\">can be found on our GitHub</a>.</p><p>Figuring this out has been a collaboration across three Cloudflare offices. Thanks to <a href=\"https://twitter.com/Hirenpanchasara\">Hiren Panchasara</a> from San Jose, <a href=\"https://twitter.com/warrncn\">Warren Nelson</a> from Austin and <a href=\"https://twitter.com/jkbs0\">Jakub Sitnicki</a> from Warsaw. Fancy joining the team? <a href=\"https://www.cloudflare.com/careers/departments/?utm_referrer=blog\">Apply here!</a></p>",
		"id": "PTYUwpDIf4wDZ50CejAvL",
		"localeList": {
			"name": "When TCP sockets refuse to die Config",
			"enUS": "English for Locale",
			"zhCN": "Translated for Locale",
			"zhHansCN": "No Page for Locale",
			"zhTW": "No Page for Locale",
			"frFR": "No Page for Locale",
			"deDE": "No Page for Locale",
			"itIT": "No Page for Locale",
			"jaJP": "No Page for Locale",
			"koKR": "Translated for Locale",
			"ptBR": "No Page for Locale",
			"esLA": "No Page for Locale",
			"esES": "No Page for Locale",
			"enAU": "No Page for Locale",
			"enCA": "No Page for Locale",
			"enIN": "No Page for Locale",
			"enGB": "No Page for Locale",
			"idID": "No Page for Locale",
			"ruRU": "No Page for Locale",
			"svSE": "No Page for Locale",
			"viVN": "No Page for Locale",
			"plPL": "No Page for Locale",
			"arAR": "No Page for Locale",
			"nlNL": "No Page for Locale",
			"thTH": "No Page for Locale",
			"trTR": "No Page for Locale",
			"heIL": "No Page for Locale",
			"lvLV": "No Page for Locale",
			"etEE": "No Page for Locale",
			"ltLT": "No Page for Locale"
		},
		"meta_description": "We noticed something weird - the TCP sockets which we thought should have been closed - were lingering around. We realized we don't really understand when TCP sockets are supposed to time out!\n",
		"metadata": {
			"imgPreview": ""
		},
		"primary_author": {},
		"publicly_index": true,
		"published_at": "2019-09-20T16:53:33.000+01:00",
		"slug": "when-tcp-sockets-refuse-to-die",
		"tags": [
			{
				"id": "7EzKI8YGWRmxWB5cGJxqDJ",
				"name": "SYN",
				"slug": "syn"
			},
			{
				"id": "5NpgoTJYJjhgjSLaY7Gt3p",
				"name": "TCP",
				"slug": "tcp"
			},
			{
				"id": "7GsUXAEWttzfewhzSJXA6W",
				"name": "Spectrum",
				"slug": "spectrum"
			},
			{
				"id": "ddJrV1bYzbMmaXkRKrp3G",
				"name": "Tech Talks",
				"slug": "tech-talks"
			}
		],
		"title": "When TCP sockets refuse to die",
		"updated_at": "2024-12-06T14:55:09.565Z",
		"url": "https://blog.cloudflare.com/when-tcp-sockets-refuse-to-die"
	},
	"translations": {
		"posts.by": "By",
		"footer.gdpr": "GDPR",
		"lang_blurb1": "This post is also available in {lang1}.",
		"lang_blurb2": "This post is also available in {lang1} and {lang2}.",
		"lang_blurb3": "This post is also available in {lang1}, {lang2} and {lang3}.",
		"footer.press": "Press",
		"header.title": "The Cloudflare Blog",
		"search.clear": "Clear",
		"search.filter": "Filter",
		"search.source": "Source",
		"footer.careers": "Careers",
		"footer.company": "Company",
		"footer.support": "Support",
		"footer.the_net": "theNet",
		"search.filters": "Filters",
		"footer.our_team": "Our team",
		"footer.webinars": "Webinars",
		"page.more_posts": "More posts",
		"posts.time_read": "{time} min read",
		"search.language": "Language",
		"footer.community": "Community",
		"footer.resources": "Resources",
		"footer.solutions": "Solutions",
		"footer.trademark": "Trademark",
		"header.subscribe": "Subscribe",
		"footer.compliance": "Compliance",
		"footer.free_plans": "Free plans",
		"footer.impact_ESG": "Impact/ESG",
		"posts.follow_on_X": "Follow on X",
		"footer.help_center": "Help center",
		"footer.network_map": "Network Map",
		"header.please_wait": "Please Wait",
		"page.related_posts": "Related posts",
		"search.result_stat": "Results <strong>{search_range}</strong> of <strong>{search_total}</strong> for <strong>{search_keyword}</strong>",
		"footer.case_studies": "Case Studies",
		"footer.connect_2024": "Connect 2024",
		"footer.terms_of_use": "Terms of Use",
		"footer.white_papers": "White Papers",
		"footer.cloudflare_tv": "Cloudflare TV",
		"footer.community_hub": "Community Hub",
		"footer.compare_plans": "Compare plans",
		"footer.contact_sales": "Contact Sales",
		"header.contact_sales": "Contact Sales",
		"header.email_address": "Email Address",
		"page.error.not_found": "Page not found",
		"footer.developer_docs": "Developer docs",
		"footer.privacy_policy": "Privacy Policy",
		"footer.request_a_demo": "Request a demo",
		"page.continue_reading": "Continue reading",
		"footer.analysts_report": "Analyst reports",
		"footer.for_enterprises": "For enterprises",
		"footer.getting_started": "Getting Started",
		"footer.learning_center": "Learning Center",
		"footer.project_galileo": "Project Galileo",
		"pagination.newer_posts": "Newer Posts",
		"pagination.older_posts": "Older Posts",
		"posts.social_buttons.x": "Discuss on X",
		"search.icon_aria_label": "Search",
		"search.source_location": "Source/Location",
		"footer.about_cloudflare": "About Cloudflare",
		"footer.athenian_project": "Athenian Project",
		"footer.become_a_partner": "Become a partner",
		"footer.cloudflare_radar": "Cloudflare Radar",
		"footer.network_services": "Network services",
		"footer.trust_and_safety": "Trust & Safety",
		"header.get_started_free": "Get Started Free",
		"page.search.placeholder": "Search Cloudflare",
		"footer.cloudflare_status": "Cloudflare Status",
		"footer.cookie_preference": "Cookie Preferences",
		"header.valid_email_error": "Must be valid email.",
		"search.result_stat_empty": "Results <strong>{search_range}</strong> of <strong>{search_total}</strong>",
		"footer.connectivity_cloud": "Connectivity cloud",
		"footer.developer_services": "Developer services",
		"footer.investor_relations": "Investor relations",
		"page.not_found.error_code": "Error Code: 404",
		"search.autocomplete_title": "Insert a query. Press enter to send",
		"footer.logos_and_press_kit": "Logos & press kit",
		"footer.application_services": "Application services",
		"footer.get_a_recommendation": "Get a recommendation",
		"posts.social_buttons.reddit": "Discuss on Reddit",
		"footer.sse_and_sase_services": "SSE and SASE services",
		"page.not_found.outdated_link": "You may have used an outdated link, or you may have typed the address incorrectly.",
		"footer.report_security_issues": "Report Security Issues",
		"page.error.error_message_page": "Sorry, we can't find the page you are looking for.",
		"header.subscribe_notifications": "Subscribe to receive notifications of new posts:",
		"footer.cloudflare_for_campaigns": "Cloudflare for Campaigns",
		"header.subscription_confimation": "Subscription confirmed. Thank you for subscribing!",
		"posts.social_buttons.hackernews": "Discuss on Hacker News",
		"footer.diversity_equity_inclusion": "Diversity, equity & inclusion",
		"footer.critical_infrastructure_defense_project": "Critical Infrastructure Defense Project"
	}
}