{
	"footerBlurb": "Cloudflare's connectivity cloud protects <a target='_blank' href='https://www.cloudflare.com/network-services/' rel='noreferrer'>entire corporate networks</a>, helps customers build <a target='_blank' href='https://workers.cloudflare.com/' rel='noreferrer'>Internet-scale applications efficiently</a>, accelerates any <a target='_blank' href='https://www.cloudflare.com/performance/accelerate-internet-applications/' rel='noreferrer'>website or Internet application</a>, <a target='_blank' href='https://www.cloudflare.com/ddos/' rel='noreferrer'>wards off DDoS attacks</a>, keeps <a target='_blank' href='https://www.cloudflare.com/application-security/' rel='noreferrer'>hackers at bay</a>, and can help you on <a target='_blank' href='https://www.cloudflare.com/products/zero-trust/' rel='noreferrer'>your journey to Zero Trust</a>.<br/><br/>Visit <a target='_blank' href='https://one.one.one.one/' rel='noreferrer'>1.1.1.1</a> from any device to get started with our free app that makes your Internet faster and safer.<br/><br/>To learn more about our mission to help build a better Internet, <a target='_blank' href='https://www.cloudflare.com/learning/what-is-cloudflare/' rel='noreferrer'>start here</a>. If you&apos;re looking for a new career direction, check out <a target='_blank' href='http://www.cloudflare.com/careers' rel='noreferrer'>our open positions</a>.",
	"initialReadingTime": "5",
	"locale": "en-us",
	"localesAvailable": [],
	"post": {
		"authors": [
			{
				"name": "Marek Majkowski",
				"slug": "marek-majkowski",
				"bio": null,
				"profile_image": "https://cf-assets.www.cloudflare.com/zkvhlag99gkb/1JuU5qavgwVeqR8BAUrd6U/3a0d0445d41c9a3c42011046efe9c37b/marek-majkowski.jpeg",
				"location": null,
				"website": null,
				"twitter": "@majek04",
				"facebook": null
			}
		],
		"excerpt": "Scaling up TCP servers is usually straightforward. Most deployments start by using a single process setup. When the need arises more worker processes are added. ",
		"feature_image": "https://cf-assets.www.cloudflare.com/zkvhlag99gkb/68PjZCIcE81b67peknf4n4/9ecd378ae7c13ff67f0a71ca817d5899/the-sad-state-of-linux-socket-balancing.jpg",
		"featured": false,
		"html": "<p>Scaling up TCP servers is usually straightforward. Most deployments start by using a single process setup. When the need arises more worker processes are added. This is a scalability model for many applications, including HTTP servers like Apache, NGINX or Lighttpd.</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/1nJQZt9rw38jtA65oSJL2M/35021ac4d6ba6b0e7dac0881e9e70eb1/37470469351_49281d8b66_b.jpg\" alt=\"\" class=\"kg-image\" width=\"1024\" height=\"740\" loading=\"lazy\"/>\n            \n            </figure><p><a href=\"https://creativecommons.org/licenses/by-sa/2.0/\">CC BY-SA 2.0</a> <a href=\"https://www.flickr.com/photos/brizzlebornandbred/37470469351/in/photolist-WWESK-arHswn-YCzFLW-Z68X62-Ys7F95-5PkLkJ-7WevtH-reYKNA-bHdogM-PQtEbn-PrTv2Q-PQtEG2-P39C9u-P39CCL-X1waW-ynbVS2-27aa3n-7qy2iy-47YUTQ-u6Su9-Py1w8y-Py4WNE-PzgZY7-PnSq1A-PquMda-P39D4f-NnLGfG-Pssk7K-9qPn21\">image</a> by <a href=\"https://www.flickr.com/photos/brizzlebornandbred/\">Paul Townsend</a></p><p>Increasing the number of worker processes is a great way to overcome a single CPU core bottleneck, but opens a whole new set of problems.</p><p>There are generally three ways of designing a TCP server with regard to performance:</p><p>(a) Single listen socket, single worker process.</p><p>(b) Single listen socket, multiple worker processes.</p><p>(c) Multiple worker processes, each with separate listen socket.</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/3V8kyqCxWxtkfmGRx8crj1/772a19786cfddd5c111ba868b5470522/worker1.png\" alt=\"\" class=\"kg-image\" width=\"640\" height=\"102\" loading=\"lazy\"/>\n            \n            </figure><p>(a) <b>Single listen socket, single worker process</b> This is the simplest model, where processing is limited to a single CPU. A single worker process is doing both accept() calls to receive the new connections and processing of the requests themselves. This model is the preferred Lighttpd setup.</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/3AyXvT4YAP3TExZCEHVIs2/5c25d8c269437f9936b5591a59a53644/worker2.png\" alt=\"\" class=\"kg-image\" width=\"640\" height=\"250\" loading=\"lazy\"/>\n            \n            </figure><p>(b) <b>Single listen socket, multiple worker process</b> The new connections sit in a single kernel data structure (the listen socket). Multiple worker processes are doing both the accept() calls and processing of the requests. This model enables some spreading of the inbound connections across multiple CPUs. This is the standard model for NGINX.</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/7g6h7dlOAQpesnC2hmTotP/fe587e2b60bb8c77b4bd0f638f7e290a/worker3.png\" alt=\"\" class=\"kg-image\" width=\"640\" height=\"250\" loading=\"lazy\"/>\n            \n            </figure><p>(c) <b>Separate listen socket for each worker process</b> By using <a href=\"https://lwn.net/Articles/542629/\">the SO_REUSEPORT socket option</a> it&#39;s possible to create a dedicated kernel data structure (the listen socket) for each worker process. This can avoid listen socket contention, which isn&#39;t really an issue unless you run at Google scale. It can also help in better balancing the load. More on that later.</p><p>At Cloudflare we run NGINX, and we are most familiar with the (b) model. In this blog post we&#39;ll describe a specific problem with this model, but let&#39;s start from the beginning.</p>\n          <div class=\"flex anchor relative\">\n            <h3 id=\"spreading-the-accept-load\">Spreading the accept() load</h3>\n            <a href=\"#spreading-the-accept-load\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n              <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n            </a>\n          </div>\n        <p>Not many people realize that there are two different ways of spreading the accept() new connection load across multiple processes. Consider these two code snippets. Let&#39;s call the first one <i>blocking-accept</i>. It&#39;s best described with this pseudo code:</p>\n            <pre class=\"language-.py\"><code class=\"language-.py\">sd = bind((&#039;127.0.0.1&#039;, 1024))\nfor i in range(3):\n    if os.fork () == 0:\n        while True:\n            cd, _ = sd.accept()\n            cd.close()\n            print &#039;worker %d&#039; % (i,)</pre></code>\n            <p>The idea is to share an accept queue across processes, by calling blocking accept() from multiple workers concurrently.</p><p>The second model should be called <i>epoll-and-accept</i>:</p>\n            <pre class=\"language-.py\"><code class=\"language-.py\">sd = bind((&#039;127.0.0.1&#039;, 1024))\nsd.setblocking(False)\nfor i in range(3):\n    if os.fork () == 0:\n        ed = select.epoll()\n        ed.register(sd, EPOLLIN | EPOLLEXCLUSIVE)\n        while True:\n            ed.poll()\n            cd, _ = sd.accept()\n            cd.close()\n            print &#039;worker %d&#039; % (i,)</pre></code>\n            <p>The intention is to have a dedicated epoll in each worker process. The worker will call non-blocking accept() only when epoll reports new connections. We can avoid <a href=\"https://idea.popcount.org/2017-02-20-epoll-is-fundamentally-broken-12/\">the usual thundering-herd issue</a> by using the EPOLLEXCLUSIVE flag.</p><p>(<a href=\"https://github.com/cloudflare/cloudflare-blog/blob/master/2017-10-accept-balancing\">Full code is available here</a>)</p><p>While these programs look similar, their behavior differs subtly<a href=\"#fn1\">[1]</a> <a href=\"#fn2\">[2]</a>. Let&#39;s see what happens when we establish a couple of connections to each:</p>\n            <pre class=\"language-.sh\"><code class=\"language-.sh\">$ ./blocking-accept.py &amp;\n$ for i in `seq 6`; do nc localhost 1024; done\nworker 2\nworker 1\nworker 0\nworker 2\nworker 1\nworker 0\n\n$ ./epoll-and-accept.py &amp;\n$ for i in `seq 6`; do nc localhost 1024; done\nworker 0\nworker 0\nworker 0\nworker 0\nworker 0\nworker 0</pre></code>\n            <p>The <i>blocking-accept</i> model distributed connections across all workers - each got exactly 2 connections. The <i>epoll-and-accept</i> model on the other hand forwarded all the connections to the first worker. The remaining workers got no traffic whatsoever.</p><p>It might catch you by surprise, but Linux does different load balancing in both cases.</p><p>In the first one Linux will do proper FIFO-like <a href=\"https://www.cloudflare.com/learning/dns/glossary/round-robin-dns/\">round robin load balancing</a>. Each process waiting on accept() is added to a queue and they will be served connections in order.</p><p>In the <i>epoll-and-accept</i> the load balancing algorithm differs: Linux seems to choose the last added process, a LIFO-like behavior. The process added to the waiting queue most recently will get the new connection. This behavior causes the busiest process, the one that only just went back to event loop, to receive the majority of the new connections. Therefore, the busiest worker is likely to get most of the load.</p><p>In fact, this is what we see in NGINX. Here&#39;s a dump of one of our synthetic tests where one worker is taking most of the load, while others are relatively underutilized:</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/48ZLQMh607BBPPkosXK1fL/517efe3c29ce4e152a6150b0fbd25c51/sharedqueue.png\" alt=\"\" class=\"kg-image\" width=\"640\" height=\"207\" loading=\"lazy\"/>\n            \n            </figure><p>Notice the last worker got almost no load, while the busiest is using 30% of CPU.</p>\n          <div class=\"flex anchor relative\">\n            <h3 id=\"so_reuseport-to-the-rescue\">SO_REUSEPORT to the rescue</h3>\n            <a href=\"#so_reuseport-to-the-rescue\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n              <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n            </a>\n          </div>\n        <p>Linux supports a feature to work around this balancing problem - the SO_REUSEPORT socket option. We explained this in the (c) model, where the incoming connections are split into multiple separate accept queues. Usually it&#39;s one dedicated queue for each worker process.</p><p>Since the accept queues are not shared, and Linux spreads the load by a simple hashing logic, each worker will get statistically the same number of incoming connections. This results in much better <a href=\"https://www.cloudflare.com/application-services/products/load-balancing/\">balancing of the load</a>. Each worker gets roughly a similar amount of traffic to handle:</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/5qhELjLr86d9VIXg7LoFge/03244209b07489c2d9b6134da0651b69/reuseport.png\" alt=\"\" class=\"kg-image\" width=\"640\" height=\"207\" loading=\"lazy\"/>\n            \n            </figure><p>Here all the workers are handling the work, with the busiest at 13.2% CPU while the least busy uses 9.3%. Much better load distribution than before.</p><p>This is better, however the balancing of the load is not the end of the story. Splitting the accept queue worsens the latency distribution in some circumstances! It&#39;s best explained by The Engineer guy:</p><p>I call this problem - the Waitrose vs Tesco Superstore cashiers. The Waitrose &quot;combined queue model&quot; is better at reducing the maximum <a href=\"https://www.cloudflare.com/learning/performance/glossary/what-is-latency/\">latency</a>. A single clogged cashier will not significantly affect the latency of whole system. The remainder of the load will be spread across the other less busy cashiers. The shared queue will be drained relatively promptly. On the other hand the Tesco Superstore model - of separate queues to each cashier - will suffer from the large latency issue. If a single cashier gets blocked,  all the traffic waiting in its queue will stall. The maximum latency will grow if any single queue gets stuck.</p><p>In a case of increased load the (b) single accept queue model, while is not balancing the load evenly, is better for latency. We can show this by running another synthetic benchmark. Here is the latency distribution for 100k relatively CPU-intensive <a href=\"https://www.cloudflare.com/learning/ddos/glossary/hypertext-transfer-protocol-http/\">HTTP requests</a>, with HTTP keepalives disabled, concurrency set to 200 and running against a single-queue (b) NGINX:</p>\n            <pre class=\"language-.sh\"><code class=\"language-.sh\">$ ./benchhttp -n 100000 -c 200 -r target:8181 http://a.a/\n        | cut -d &quot; &quot; -f 1\n        | ./mmhistogram -t &quot;Duration in ms (single queue)&quot;\nmin:3.61 avg:30.39 med=30.28 max:72.65 dev:1.58 count:100000\nDuration in ms (single queue):\n value |-------------------------------------------------- count\n     0 |                                                   0\n     1 |                                                   0\n     2 |                                                   1\n     4 |                                                   16\n     8 |                                                   67\n    16 |************************************************** 91760\n    32 |                                              **** 8155\n    64 |                                                   1</pre></code>\n            <p>As you can see the latency is very predictable. The median is almost equal to the average and standard deviation is small.</p><p>Here is the same test run against the SO_REUSEPORT multi-queue NGINX setup (c):</p>\n            <pre class=\"language-.sh\"><code class=\"language-.sh\">$ ./benchhttp -n 100000 -c 200 -r target:8181 http://a.a/\n        | cut -d &quot; &quot; -f 1\n        | ./mmhistogram -t &quot;Duration in ms (multiple queues)&quot;\nmin:1.49 avg:31.37 med=24.67 max:144.55 dev:25.27 count:100000\nDuration in ms (multiple queues):\n value |-------------------------------------------------- count\n     0 |                                                   0\n     1 |                                                 * 1023\n     2 |                                         ********* 5321\n     4 |                                 ***************** 9986\n     8 |                  ******************************** 18443\n    16 |    ********************************************** 25852\n    32 |************************************************** 27949\n    64 |                              ******************** 11368\n   128 |                                                   58</pre></code>\n            <p>The average is comparable, the median dropped, the max value significantly increased, and most importantly the deviation is now gigantic!</p><p>The latency distribution is all over the place - it&#39;s not something you want to have on a production server.</p><p>(<a href=\"https://github.com/cloudflare/cloudflare-blog/blob/master/2017-10-accept-balancing\">Instructions to reproduce the test</a>)</p><p>Take this benchmark with a grain of salt though. We try to generate substantial load in order to prove the point. Depending on your setup it might be possible to shield your server from excessive traffic and prevent it from entering this degraded-latency state<a href=\"#fn3\">[3]</a>.</p>\n          <div class=\"flex anchor relative\">\n            <h3 id=\"conclusion\">Conclusion</h3>\n            <a href=\"#conclusion\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n              <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n            </a>\n          </div>\n        <p>Balancing the incoming connections across multiple application workers is far from a solved problem. The single queue approach (b) scales well and keeps the max latency in check, but due to the epoll LIFO behavior the worker processes won&#39;t be evenly load balanced.</p><p>For workloads that require even balancing it might be beneficial to use the SO_REUSEPORT pattern (c). Unfortunately in high load situations the latency distribution might degrade.</p><p>The best generic solution seem to be to change the standard epoll behavior from LIFO to FIFO. There have been attempts to address this in the past by Jason Baron from Akamai (<a href=\"https://patchwork.kernel.org/patch/5803291/\">1</a>, <a href=\"https://patchwork.kernel.org/patch/5841231/\">2</a>, <a href=\"https://www.mail-archive.com/linux-kernel@vger.kernel.org/msg831609.html\">3</a>), but none had landed in mainline so far.</p><hr/><p><i>Dealing with the internals of Linux and NGINX sound interesting? Join our </i><a href=\"https://boards.greenhouse.io/cloudflare/jobs/589572\"><i>world famous team</i></a><i> in London, Austin, San Francisco and our elite office in Warsaw, Poland</i>.</p><hr/><hr/><ol><li><p>Of course comparing blocking accept() with a full featured epoll() event loop is not fair. Epoll is more powerful and allows us to create rich event driven programs. Using blocking accept is rather cumbersome or just not useful at all. To make any sense, blocking accept programs would require careful multi-threading programming, with a dedicated thread per request. <a href=\"#fnref1\">↩︎</a></p></li><li><p>Another surprise lurking in the corner - using blocking accept() on Linux is technically incorrect! Alan Burlison pointed out that calling <a href=\"https://www.mail-archive.com/netdev@vger.kernel.org/msg83683.html\">close() on listen socket that has blocking accepts() will not interrupt them</a>. This can result in a buggy behavior - you may get <a href=\"https://bugzilla.kernel.org/show_bug.cgi?id=106241\">a successful accept() on a listen socket that no longer exists</a>. When in doubt - avoid using blocking accept() in multithreaded programs. The workaround is to call shutdown() first, but this is not POSIX compliant. It&#39;s a mess. <a href=\"#fnref2\">↩︎</a></p></li><li><p>There are a couple of things you need to take into account when using NGINX reuseport implementation. First make sure you run 1.13.6+ or have <a href=\"https://github.com/nginx/nginx/commit/da165aae88601628cef8db1646cd0ce3f0ee661f\">this patch applied</a>. Then, remember that due to a <a href=\"https://lwn.net/Articles/542629/\">defect in Linux TCP REUSEPORT implementation</a> reducing number of REUSEPORT queues will cause some waiting TCP connections to be dropped. <a href=\"#fnref3\">↩︎</a></p></li></ol>",
		"id": "72wV8FUYMYOwm15C8vFlWT",
		"localeList": {
			"name": "Why does one NGINX worker take all the load? Config",
			"enUS": "English for Locale",
			"zhCN": "No Page for Locale",
			"zhHansCN": "No Page for Locale",
			"zhTW": "No Page for Locale",
			"frFR": "No Page for Locale",
			"deDE": "No Page for Locale",
			"itIT": "No Page for Locale",
			"jaJP": "No Page for Locale",
			"koKR": "No Page for Locale",
			"ptBR": "No Page for Locale",
			"esLA": "No Page for Locale",
			"esES": "No Page for Locale",
			"enAU": "No Page for Locale",
			"enCA": "No Page for Locale",
			"enIN": "No Page for Locale",
			"enGB": "No Page for Locale",
			"idID": "No Page for Locale",
			"ruRU": "No Page for Locale",
			"svSE": "No Page for Locale",
			"viVN": "No Page for Locale",
			"plPL": "No Page for Locale",
			"arAR": "No Page for Locale",
			"nlNL": "No Page for Locale",
			"thTH": "No Page for Locale",
			"trTR": "No Page for Locale",
			"heIL": "No Page for Locale",
			"lvLV": "No Page for Locale",
			"etEE": "No Page for Locale",
			"ltLT": "No Page for Locale"
		},
		"meta_description": null,
		"metadata": {
			"imgPreview": ""
		},
		"primary_author": {},
		"published_at": "2017-10-23T13:57:36.000+01:00",
		"slug": "the-sad-state-of-linux-socket-balancing",
		"tags": [
			{
				"id": "3FBpuRfF7HUFga2Z5jgAFf",
				"name": "NGINX",
				"slug": "nginx"
			},
			{
				"id": "4HIPcb68qM0e26fIxyfzwQ",
				"name": "Developers",
				"slug": "developers"
			},
			{
				"id": "6hbkItfupogJP3aRDAq6v8",
				"name": "Cloudflare Workers",
				"slug": "workers"
			},
			{
				"id": "5cye1Bh5KxFh3pKSnX8Dsy",
				"name": "Serverless",
				"slug": "serverless"
			},
			{
				"id": "78aSAeMjGNmCuetQ7B4OgU",
				"name": "JavaScript",
				"slug": "javascript"
			},
			{
				"id": "5NpgoTJYJjhgjSLaY7Gt3p",
				"name": "TCP",
				"slug": "tcp"
			},
			{
				"id": "3JAY3z7p7An94s6ScuSQPf",
				"name": "Developer Platform",
				"slug": "developer-platform"
			}
		],
		"title": "Why does one NGINX worker take all the load?",
		"updated_at": "2024-10-10T00:33:50.654Z",
		"url": "https://blog.cloudflare.com/the-sad-state-of-linux-socket-balancing"
	},
	"translations": {
		"posts.by": "By",
		"footer.gdpr": "GDPR",
		"lang_blurb1": "This post is also available in {lang1}.",
		"lang_blurb2": "This post is also available in {lang1} and {lang2}.",
		"lang_blurb3": "This post is also available in {lang1}, {lang2} and {lang3}.",
		"footer.press": "Press",
		"header.title": "The Cloudflare Blog",
		"search.clear": "Clear",
		"search.filter": "Filter",
		"search.source": "Source",
		"footer.careers": "Careers",
		"footer.company": "Company",
		"footer.support": "Support",
		"footer.the_net": "theNet",
		"search.filters": "Filters",
		"footer.our_team": "Our team",
		"footer.webinars": "Webinars",
		"page.more_posts": "More posts",
		"posts.time_read": "{time} min read",
		"search.language": "Language",
		"footer.community": "Community",
		"footer.resources": "Resources",
		"footer.solutions": "Solutions",
		"footer.trademark": "Trademark",
		"header.subscribe": "Subscribe",
		"footer.compliance": "Compliance",
		"footer.free_plans": "Free plans",
		"footer.impact_ESG": "Impact/ESG",
		"posts.follow_on_X": "Follow on X",
		"footer.help_center": "Help center",
		"footer.network_map": "Network Map",
		"header.please_wait": "Please Wait",
		"page.related_posts": "Related posts",
		"search.result_stat": "Results <strong>{search_range}</strong> of <strong>{search_total}</strong> for <strong>{search_keyword}</strong>",
		"footer.case_studies": "Case Studies",
		"footer.connect_2024": "Connect 2024",
		"footer.terms_of_use": "Terms of Use",
		"footer.white_papers": "White Papers",
		"footer.cloudflare_tv": "Cloudflare TV",
		"footer.community_hub": "Community Hub",
		"footer.compare_plans": "Compare plans",
		"footer.contact_sales": "Contact Sales",
		"header.contact_sales": "Contact Sales",
		"header.email_address": "Email Address",
		"page.error.not_found": "Page not found",
		"footer.developer_docs": "Developer docs",
		"footer.privacy_policy": "Privacy Policy",
		"footer.request_a_demo": "Request a demo",
		"page.continue_reading": "Continue reading",
		"footer.analysts_report": "Analyst reports",
		"footer.for_enterprises": "For enterprises",
		"footer.getting_started": "Getting Started",
		"footer.learning_center": "Learning Center",
		"footer.project_galileo": "Project Galileo",
		"pagination.newer_posts": "Newer Posts",
		"pagination.older_posts": "Older Posts",
		"posts.social_buttons.x": "Discuss on X",
		"search.source_location": "Source/Location",
		"footer.about_cloudflare": "About Cloudflare",
		"footer.athenian_project": "Athenian Project",
		"footer.become_a_partner": "Become a partner",
		"footer.cloudflare_radar": "Cloudflare Radar",
		"footer.network_services": "Network services",
		"footer.trust_and_safety": "Trust & Safety",
		"header.get_started_free": "Get Started Free",
		"page.search.placeholder": "Search Cloudflare",
		"footer.cloudflare_status": "Cloudflare Status",
		"footer.cookie_preference": "Cookie Preferences",
		"header.valid_email_error": "Must be valid email.",
		"footer.connectivity_cloud": "Connectivity cloud",
		"footer.developer_services": "Developer services",
		"footer.investor_relations": "Investor relations",
		"page.not_found.error_code": "Error Code: 404",
		"footer.logos_and_press_kit": "Logos & press kit",
		"footer.application_services": "Application services",
		"footer.get_a_recommendation": "Get a recommendation",
		"posts.social_buttons.reddit": "Discuss on Reddit",
		"footer.sse_and_sase_services": "SSE and SASE services",
		"page.not_found.outdated_link": "You may have used an outdated link, or you may have typed the address incorrectly.",
		"footer.report_security_issues": "Report Security Issues",
		"page.error.error_message_page": "Sorry, we can't find the page you are looking for.",
		"header.subscribe_notifications": "Subscribe to receive notifications of new posts:",
		"footer.cloudflare_for_campaigns": "Cloudflare for Campaigns",
		"header.subscription_confimation": "Subscription confirmed. Thank you for subscribing!",
		"posts.social_buttons.hackernews": "Discuss on Hacker News",
		"footer.diversity_equity_inclusion": "Diversity, equity & inclusion",
		"footer.critical_infrastructure_defense_project": "Critical Infrastructure Defense Project"
	}
}