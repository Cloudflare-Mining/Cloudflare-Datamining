{
	"footerBlurb": "Cloudflare's connectivity cloud protects <a target='_blank' href='https://www.cloudflare.com/network-services/' rel='noreferrer'>entire corporate networks</a>, helps customers build <a target='_blank' href='https://workers.cloudflare.com/' rel='noreferrer'>Internet-scale applications efficiently</a>, accelerates any <a target='_blank' href='https://www.cloudflare.com/performance/accelerate-internet-applications/' rel='noreferrer'>website or Internet application</a>, <a target='_blank' href='https://www.cloudflare.com/ddos/' rel='noreferrer'>wards off DDoS attacks</a>, keeps <a target='_blank' href='https://www.cloudflare.com/application-security/' rel='noreferrer'>hackers at bay</a>, and can help you on <a target='_blank' href='https://www.cloudflare.com/products/zero-trust/' rel='noreferrer'>your journey to Zero Trust</a>.<br/><br/>Visit <a target='_blank' href='https://one.one.one.one/' rel='noreferrer'>1.1.1.1</a> from any device to get started with our free app that makes your Internet faster and safer.<br/><br/>To learn more about our mission to help build a better Internet, <a target='_blank' href='https://www.cloudflare.com/learning/what-is-cloudflare/' rel='noreferrer'>start here</a>. If you&apos;re looking for a new career direction, check out <a target='_blank' href='http://www.cloudflare.com/careers' rel='noreferrer'>our open positions</a>.",
	"initialReadingTime": "6",
	"locale": "en-us",
	"localesAvailable": [
		"zh-cn",
		"fr-fr",
		"de-de",
		"ko-kr",
		"es-es"
	],
	"post": {
		"authors": [
			{
				"name": "Marek Majkowski",
				"slug": "marek-majkowski",
				"bio": null,
				"profile_image": "https://cf-assets.www.cloudflare.com/zkvhlag99gkb/1JuU5qavgwVeqR8BAUrd6U/3a0d0445d41c9a3c42011046efe9c37b/marek-majkowski.jpeg",
				"location": null,
				"website": null,
				"twitter": "@majek04",
				"facebook": null,
				"publiclyIndex": true
			}
		],
		"excerpt": "Internally our DDoS mitigation team is sometimes called \"the packet droppers\". When other teams build exciting products to do smart things with the traffic that passed through our network, we take joy in discovering novel ways of discarding it.",
		"feature_image": "https://cf-assets.www.cloudflare.com/zkvhlag99gkb/2AaLqkJOn4yMNV7GNOhWxy/7f2dfba0311ce439865f1f9063c264d0/how-to-drop-10-million-packets.png",
		"featured": false,
		"html": "<p>Internally our DDoS mitigation team is sometimes called &quot;the packet droppers&quot;. When other teams build exciting products to do smart things with the traffic that passes through our network, we take joy in discovering novel ways of discarding it.</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/7yS2j3vUrdmAkLkGjcunWg/cabb3814dc89260f1338ee65cd38466f/38464589350_d00908ee98_b.jpg\" alt=\"38464589350_d00908ee98_b\" class=\"kg-image\" width=\"1024\" height=\"820\" loading=\"lazy\"/>\n            \n            </figure><p><a href=\"https://creativecommons.org/licenses/by-sa/2.0/\">CC BY-SA 2.0</a> <a href=\"https://www.flickr.com/photos/beegee49/38464589350\">image</a> by <a href=\"https://www.flickr.com/photos/beegee49\">Brian Evans</a></p><p>Being able to quickly discard packets is very important to withstand DDoS attacks.</p><p>Dropping packets hitting our servers, as simple as it sounds, can be done on multiple layers. Each technique has its advantages and limitations. In this blog post we&#39;ll review all the techniques we tried thus far.</p>\n    <div class=\"flex anchor relative\">\n      <h3 id=\"test-bench\">Test bench</h3>\n      <a href=\"#test-bench\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>To illustrate the relative performance of the methods we&#39;ll show some numbers. The benchmarks are synthetic, so take the numbers with a grain of salt. We&#39;ll use one of our Intel servers, with a 10Gbps network card. The hardware details aren&#39;t too important, since the tests are prepared to show the operating system, not hardware, limitations.</p><p>Our testing setup is prepared as follows:</p><ul><li><p>We transmit a large number of tiny UDP packets, reaching 14Mpps (millions packets per second).</p></li><li><p>This traffic is directed towards a single CPU on a target server.</p></li><li><p>We measure the number of packets handled by the kernel on that one CPU.</p></li></ul><p>We&#39;re not trying to maximize userspace application speed, nor packet throughput - instead, we&#39;re trying to specifically show kernel bottlenecks.</p><p>The synthetic traffic is prepared to put maximum stress on conntrack - it uses random source IP and port fields. Tcpdump will show it like this:</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">$ tcpdump -ni vlan100 -c 10 -t udp and dst port 1234\nIP 198.18.40.55.32059 &gt; 198.18.0.12.1234: UDP, length 16\nIP 198.18.51.16.30852 &gt; 198.18.0.12.1234: UDP, length 16\nIP 198.18.35.51.61823 &gt; 198.18.0.12.1234: UDP, length 16\nIP 198.18.44.42.30344 &gt; 198.18.0.12.1234: UDP, length 16\nIP 198.18.106.227.38592 &gt; 198.18.0.12.1234: UDP, length 16\nIP 198.18.48.67.19533 &gt; 198.18.0.12.1234: UDP, length 16\nIP 198.18.49.38.40566 &gt; 198.18.0.12.1234: UDP, length 16\nIP 198.18.50.73.22989 &gt; 198.18.0.12.1234: UDP, length 16\nIP 198.18.43.204.37895 &gt; 198.18.0.12.1234: UDP, length 16\nIP 198.18.104.128.1543 &gt; 198.18.0.12.1234: UDP, length 16</pre></code>\n            <p>On the target side all of the packets are going to be forwarded to exactly one RX queue, therefore one CPU. We do this with hardware flow steering:</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">ethtool -N ext0 flow-type udp4 dst-ip 198.18.0.12 dst-port 1234 action 2</pre></code>\n            <p>Benchmarking is always hard. When preparing the tests we learned that having any active raw sockets destroys performance. It&#39;s obvious in hindsight, but easy to miss. Before running any tests remember to make sure you don&#39;t have any stale <code>tcpdump</code> process running. This is how to check it, showing a bad process active:</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">$ ss -A raw,packet_raw -l -p|cat\nNetid  State      Recv-Q Send-Q Local Address:Port\np_raw  UNCONN     525157 0      *:vlan100          users:((&quot;tcpdump&quot;,pid=23683,fd=3))</pre></code>\n            <p>Finally, we are going to disable the Intel Turbo Boost feature on the machine:</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">echo 1 | sudo tee /sys/devices/system/cpu/intel_pstate/no_turbo</pre></code>\n            <p>While Turbo Boost is nice and increases throughput by at least 20%, it also drastically worsens the standard deviation in our tests. With turbo enabled we had Â±1.5% deviation in our numbers. With Turbo off this falls down to manageable 0.25%.</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/5oJAi3KKpj9clrG5Abebho/59ed605de89f49183c5b360a4b50d30c/layers.JPG.jpeg\" alt=\"layers\" class=\"kg-image\" width=\"1280\" height=\"782\" loading=\"lazy\"/>\n            \n            </figure>\n    <div class=\"flex anchor relative\">\n      <h4 id=\"step-1-dropping-packets-in-application\">Step 1. Dropping packets in application</h4>\n      <a href=\"#step-1-dropping-packets-in-application\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>Let&#39;s start with the idea of delivering packets to an application and ignoring them in userspace code. For the test setup, let&#39;s make sure our iptables don&#39;t affect the performance:</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">iptables -I PREROUTING -t mangle -d 198.18.0.12 -p udp --dport 1234 -j ACCEPT\niptables -I PREROUTING -t raw -d 198.18.0.12 -p udp --dport 1234 -j ACCEPT\niptables -I INPUT -t filter -d 198.18.0.12 -p udp --dport 1234 -j ACCEPT</pre></code>\n            <p>The application code is a simple loop, receiving data and immediately discarding it in the userspace:</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">s = socket.socket(AF_INET, SOCK_DGRAM)\ns.bind((&quot;0.0.0.0&quot;, 1234))\nwhile True:\n    s.recvmmsg([...])</pre></code>\n            <p><a href=\"https://github.com/cloudflare/cloudflare-blog/blob/master/2018-07-dropping-packets/recvmmsg-loop.c\">We prepared the code</a>, to run it:</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">$ ./dropping-packets/recvmmsg-loop\npackets=171261 bytes=1940176</pre></code>\n            <p>This setup allows the kernel to receive a meagre 175kpps from the hardware receive queue, as measured by <code>ethtool</code> and using our simple <a href=\"/three-little-tools-mmsum-mmwatch-mmhistogram/\"><code>mmwatch</code> tool</a>:</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">$ mmwatch &#039;ethtool -S ext0|grep rx_2&#039;\n rx2_packets: 174.0k/s</pre></code>\n            <p>The hardware technically gets 14Mpps off the wire, but it&#39;s impossible to pass it all to a single RX queue handled by only one CPU core doing kernel work. <code>mpstat</code> confirms this:</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">$ watch &#039;mpstat -u -I SUM -P ALL 1 1|egrep -v Aver&#039;\n01:32:05 PM  CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle\n01:32:06 PM    0    0.00    0.00    0.00    2.94    0.00    3.92    0.00    0.00    0.00   93.14\n01:32:06 PM    1    2.17    0.00   27.17    0.00    0.00    0.00    0.00    0.00    0.00   70.65\n01:32:06 PM    2    0.00    0.00    0.00    0.00    0.00  100.00    0.00    0.00    0.00    0.00\n01:32:06 PM    3    0.95    0.00    1.90    0.95    0.00    3.81    0.00    0.00    0.00   92.38</pre></code>\n            <p>As you can see application code is not a bottleneck, using 27% sys + 2% userspace on CPU #1, while network SOFTIRQ on CPU #2 uses 100% resources.</p><p>By the way, using <code>recvmmsg(2)</code> is important. In these post-Spectre days, syscalls got more expensive and indeed, we run kernel 4.14 with KPTI and retpolines:</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">$ tail -n +1 /sys/devices/system/cpu/vulnerabilities/*\n==&gt; /sys/devices/system/cpu/vulnerabilities/meltdown &lt;==\nMitigation: PTI\n\n==&gt; /sys/devices/system/cpu/vulnerabilities/spectre_v1 &lt;==\nMitigation: __user pointer sanitization\n\n==&gt; /sys/devices/system/cpu/vulnerabilities/spectre_v2 &lt;==\nMitigation: Full generic retpoline, IBPB, IBRS_FW</pre></code>\n            \n    <div class=\"flex anchor relative\">\n      <h4 id=\"step-2-slaughter-conntrack\">Step 2. Slaughter conntrack</h4>\n      <a href=\"#step-2-slaughter-conntrack\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>We specifically designed the test - by choosing random source IP and ports - to put stress on the conntrack layer. This can be verified by looking at number of conntrack entries, which during the test is reaching the maximum:</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">$ conntrack -C\n2095202\n\n$ sysctl net.netfilter.nf_conntrack_max\nnet.netfilter.nf_conntrack_max = 2097152</pre></code>\n            <p>You can also observe conntrack shouting in <code>dmesg</code>:</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">[4029612.456673] nf_conntrack: nf_conntrack: table full, dropping packet\n[4029612.465787] nf_conntrack: nf_conntrack: table full, dropping packet\n[4029617.175957] net_ratelimit: 5731 callbacks suppressed</pre></code>\n            <p>To speed up our tests let&#39;s disable it:</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">iptables -t raw -I PREROUTING -d 198.18.0.12 -p udp -m udp --dport 1234 -j NOTRACK</pre></code>\n            <p>And rerun the tests:</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">$ ./dropping-packets/recvmmsg-loop\npackets=331008 bytes=5296128</pre></code>\n            <p>This instantly bumps the application receive performance to 333kpps. Hurray!</p><p>PS. With SO_BUSY_POLL we can bump the numbers to 470k pps, but this is a subject for another time.</p>\n    <div class=\"flex anchor relative\">\n      <h4 id=\"step-3-bpf-drop-on-a-socket\">Step 3. BPF drop on a socket</h4>\n      <a href=\"#step-3-bpf-drop-on-a-socket\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>Going further, why deliver packets to userspace application at all? While this technique is uncommon, we can attach a classical BPF filter to a SOCK_DGRAM socket with <code>setsockopt(SO_ATTACH_FILTER)</code> and program the filter to discard packets in kernel space.</p><p><a href=\"https://github.com/cloudflare/cloudflare-blog/blob/master/2018-07-dropping-packets/bpf-drop.c\">See the code</a>, to run it:</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">$ ./bpf-drop\npackets=0 bytes=0</pre></code>\n            <p>With drops in BPF (both Classical as well as extended eBPF have similar performance) we process roughly 512kpps. All of them get dropped in the BPF filter while still in software interrupt mode, which saves us CPU needed to wake up the userspace application.</p>\n    <div class=\"flex anchor relative\">\n      <h4 id=\"step-4-iptables-drop-after-routing\">Step 4. iptables DROP after routing</h4>\n      <a href=\"#step-4-iptables-drop-after-routing\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>As a next step we can simply drop packets in the iptables firewall INPUT chain by adding rule like this:</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">iptables -I INPUT -d 198.18.0.12 -p udp --dport 1234 -j DROP</pre></code>\n            <p>Remember we disabled conntrack already with <code>-j NOTRACK</code>. These two rules give us 608kpps.</p><p>The numbers in iptables counters:</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">$ mmwatch &#039;iptables -L -v -n -x | head&#039;\n\nChain INPUT (policy DROP 0 packets, 0 bytes)\n    pkts      bytes target     prot opt in     out     source               destination\n605.9k/s    26.7m/s DROP       udp  --  *      *       0.0.0.0/0            198.18.0.12          udp dpt:1234</pre></code>\n            <p>600kpps is not bad, but we can do better!</p>\n    <div class=\"flex anchor relative\">\n      <h4 id=\"step-5-iptables-drop-in-prerouting\">Step 5. iptables DROP in PREROUTING</h4>\n      <a href=\"#step-5-iptables-drop-in-prerouting\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>An even faster technique is to drop packets before they get routed. This rule can do this:</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">iptables -I PREROUTING -t raw -d 198.18.0.12 -p udp --dport 1234 -j DROP</pre></code>\n            <p>This produces whopping 1.688mpps.</p><p>This is quite a significant jump in performance, I don&#39;t fully understand it. Either our routing layer is unusually complex or there is a bug in our server configuration.</p><p>In any case - &quot;raw&quot; iptables table is definitely way faster.</p>\n    <div class=\"flex anchor relative\">\n      <h4 id=\"step-6-nftables-drop-before-conntrack\">Step 6. nftables DROP before CONNTRACK</h4>\n      <a href=\"#step-6-nftables-drop-before-conntrack\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>Iptables is considered passÃ© these days. The new kid in town is nftables. See this <a href=\"https://www.youtube.com/watch?v=9Zr8XqdET1c\">video for a technical explanation why</a> nftables is superior. Nftables promises to be faster than gray haired iptables for many reasons, among them is a rumor that retpolines (aka: no speculation for indirect jumps) hurt iptables quite badly.</p><p>Since this article is not about comparing the nftables vs iptables speed, let&#39;s try only the fastest drop I could came up with:</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">nft add table netdev filter\nnft -- add chain netdev filter input { type filter hook ingress device vlan100 priority -500 \\; policy accept \\; }\nnft add rule netdev filter input ip daddr 198.18.0.0/24 udp dport 1234 counter drop\nnft add rule netdev filter input ip6 daddr fd00::/64 udp dport 1234 counter drop</pre></code>\n            <p>The counters can be seen with this command:</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">$ mmwatch &#039;nft --handle list chain netdev filter input&#039;\ntable netdev filter {\n    chain input {\n        type filter hook ingress device vlan100 priority -500; policy accept;\n        ip daddr 198.18.0.0/24 udp dport 1234 counter packets    1.6m/s bytes    69.6m/s drop # handle 2\n        ip6 daddr fd00::/64 udp dport 1234 counter packets 0 bytes 0 drop # handle 3\n    }\n}</pre></code>\n            <p>Nftables &quot;ingress&quot; hook yields around 1.53mpps. This is slightly slower than iptables in the PREROUTING layer. This is puzzling - theoretically &quot;ingress&quot; happens before PREROUTING, so should be faster.</p><p>In our test nftables was slightly slower than iptables, but not by much. Nftables is still better :P</p>\n    <div class=\"flex anchor relative\">\n      <h4 id=\"step-7-tc-ingress-handler-drop\">Step 7. tc ingress handler DROP</h4>\n      <a href=\"#step-7-tc-ingress-handler-drop\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>A somewhat surprising fact is that a tc (traffic control) ingress hook happens before even PREROUTING. tc makes it possible to select packets based on basic criteria and indeed - action drop - them. The syntax is rather hacky, so it&#39;s recommended to <a href=\"https://github.com/netoptimizer/network-testing/blob/master/bin/tc_ingress_drop.sh\">use this script</a> to set it up. We need a tiny bit more complex tc match, here is the command line:</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">tc qdisc add dev vlan100 ingress\ntc filter add dev vlan100 parent ffff: prio 4 protocol ip u32 match ip protocol 17 0xff match ip dport 1234 0xffff match ip dst 198.18.0.0/24 flowid 1:1 action drop\ntc filter add dev vlan100 parent ffff: protocol ipv6 u32 match ip6 dport 1234 0xffff match ip6 dst fd00::/64 flowid 1:1 action drop</pre></code>\n            <p>We can verify it:</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">$ mmwatch &#039;tc -s filter  show dev vlan100  ingress&#039;\nfilter parent ffff: protocol ip pref 4 u32 \nfilter parent ffff: protocol ip pref 4 u32 fh 800: ht divisor 1 \nfilter parent ffff: protocol ip pref 4 u32 fh 800::800 order 2048 key ht 800 bkt 0 flowid 1:1  (rule hit   1.8m/s success   1.8m/s)\n  match 00110000/00ff0000 at 8 (success   1.8m/s ) \n  match 000004d2/0000ffff at 20 (success   1.8m/s ) \n  match c612000c/ffffffff at 16 (success   1.8m/s ) \n        action order 1: gact action drop\n         random type none pass val 0\n         index 1 ref 1 bind 1 installed 1.0/s sec\n        Action statistics:\n        Sent    79.7m/s bytes   1.8m/s pkt (dropped   1.8m/s, overlimits 0 requeues 0) \n        backlog 0b 0p requeues 0</pre></code>\n            <p>A tc ingress hook with u32 match allows us to drop 1.8mpps on a single CPU. This is brilliant!</p><p>But we can go even faster...</p>\n    <div class=\"flex anchor relative\">\n      <h4 id=\"step-8-xdp_drop\">Step 8. XDP_DROP</h4>\n      <a href=\"#step-8-xdp_drop\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>Finally, the ultimate weapon is XDP - <a href=\"https://prototype-kernel.readthedocs.io/en/latest/networking/XDP/\">eXpress Data Path</a>. With XDP we can run eBPF code in the context of a network driver. Most importantly, this is before the <code>skbuff</code> memory allocation, allowing great speeds.</p><p>Usually XDP projects have two parts:</p><ul><li><p>the eBPF code loaded into the kernel context</p></li><li><p>the userspace loader, which loads the code onto the right network card and manages it</p></li></ul><p>Writing the loader is pretty hard, so instead we can use the <a href=\"https://cilium.readthedocs.io/en/latest/bpf/#iproute2\">new <code>iproute2</code> feature</a> and load the code with this trivial command:</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">ip link set dev ext0 xdp obj xdp-drop-ebpf.o</pre></code>\n            <p>Tadam!</p><p>The source code for <a href=\"https://github.com/cloudflare/cloudflare-blog/blob/master/2018-07-dropping-packets/xdp-drop-ebpf.c\">the loaded eBPF XDP program is available here</a>. The program parses IP packets and looks for desired characteristics: IP transport, UDP protocol, desired target subnet and destination port:</p>\n            <pre class=\"language-.c\"><code class=\"language-.c\">if (h_proto == htons(ETH_P_IP)) {\n    if (iph-&gt;protocol == IPPROTO_UDP\n        &amp;&amp; (htonl(iph-&gt;daddr) &amp; 0xFFFFFF00) == 0xC6120000 // 198.18.0.0/24\n        &amp;&amp; udph-&gt;dest == htons(1234)) {\n        return XDP_DROP;\n    }\n}</pre></code>\n            <p>XDP program needs to be compiled with modern <code>clang</code> that can emit BPF bytecode. After this we can load and verify the running XDP program:</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">$ ip link show dev ext0\n4: ext0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 xdp qdisc fq state UP mode DEFAULT group default qlen 1000\n    link/ether 24:8a:07:8a:59:8e brd ff:ff:ff:ff:ff:ff\n    prog/xdp id 5 tag aedc195cc0471f51 jited</pre></code>\n            <p>And see the numbers in <code>ethtool -S</code> network card statistics:</p>\n            <pre class=\"language-bash\"><code class=\"language-bash\">$ mmwatch &#039;ethtool -S ext0|egrep &quot;rx&quot;|egrep -v &quot;: 0&quot;|egrep -v &quot;cache|csum&quot;&#039;\n     rx_out_of_buffer:     4.4m/s\n     rx_xdp_drop:         10.1m/s\n     rx2_xdp_drop:        10.1m/s</pre></code>\n            <p>Whooa! With XDP we can drop 10 million packets per second on a single CPU.</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/5SE8JnlqRwU9PvWk57Drtp/f399e77d9a09e6ca767caf3818d35bee/225821241_ed5da2da91_o.jpg\" alt=\"225821241_ed5da2da91_o\" class=\"kg-image\" width=\"800\" height=\"600\" loading=\"lazy\"/>\n            \n            </figure><p><a href=\"https://creativecommons.org/licenses/by-sa/2.0/\">CC BY-SA 2.0</a> <a href=\"https://www.flickr.com/photos/afiler/225821241/\">image</a> by <a href=\"https://www.flickr.com/photos/afiler/\">Andrew Filer</a></p>\n    <div class=\"flex anchor relative\">\n      <h3 id=\"summary\">Summary</h3>\n      <a href=\"#summary\" aria-hidden=\"true\" class=\"relative sm:absolute sm:-left-5\">\n        <svg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\"><path fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\"></path></svg>\n      </a>\n    </div>\n    <p>We repeated the these for both IPv4 and IPv6 and prepared this chart:</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/7gFSZtRxYtyzjQwOjcfXE/7815d540a5dcd04bacd0821d2e7067fc/numbers-noxdp.png\" alt=\"numbers-noxdp\" class=\"kg-image\" width=\"1500\" height=\"900\" loading=\"lazy\"/>\n            \n            </figure><p>Generally speaking in our setup IPv6 had slightly lower performance. Remember that IPv6 packets are slightly larger, so some performance difference is unavoidable.</p><p>Linux has numerous hooks that can be used to filter packets, each with different performance and ease of use characteristics.</p><p>For DDoS purporses, it may totally be reasonable to just receive the packets in the application and process them in userspace. Properly tuned applications can get pretty decent numbers.</p><p>For DDoS attacks with random/spoofed source IP&#39;s, it might be worthwhile disabling conntrack to gain some speed. Be careful though - there are attacks for which conntrack is very helpful.</p><p>In other circumstances it may make sense to integrate the Linux firewall into the DDoS mitigation pipeline. In such cases, remember to put the mitigations in a &quot;-t raw PREROUTING&quot; layer, since it&#39;s significantly faster than &quot;filter&quot; table.</p><p>For even more demanding workloads, we always have XDP. And boy, it is powerful. Here is the same chart as above, but including XDP:</p>\n            <figure class=\"kg-card kg-image-card \">\n            \n            <Image src=\"https://cf-assets.www.cloudflare.com/zkvhlag99gkb/2OUdMjazlfP6gpm6usqmj7/20f07e262782d9ef5c67ca46a603fed6/numbers-xdp-1.png\" alt=\"numbers-xdp-1\" class=\"kg-image\" width=\"1500\" height=\"900\" loading=\"lazy\"/>\n            \n            </figure><p>If you want to reproduce these numbers, <a href=\"https://github.com/cloudflare/cloudflare-blog/blob/master/2018-07-dropping-packets/README.md\">see the README where we documented everything</a>.</p><p>Here at Cloudflare we are using... almost all of these techniques. Some of the userspace tricks are integrated with our applications. The iptables layer is managed by <a href=\"/meet-gatebot-a-bot-that-allows-us-to-sleep/\">our Gatebot DDoS pipeline</a>. Finally, we are working on replacing our proprietary kernel offload solution with XDP.</p><p>Want to help us drop more packets? We&#39;re hiring for many roles, including packet droppers, systems engineers and more!</p><p><i>Special thanks to </i><a href=\"https://twitter.com/JesperBrouer\"><i>Jesper Dangaard Brouer</i></a><i> for helping with this work.</i></p>",
		"id": "lrRGZKcpgb4NjHk3esKtY",
		"localeList": {
			"name": "How to drop 10 million packets per second Config",
			"enUS": "English for Locale",
			"zhCN": "Translated for Locale",
			"zhHansCN": "No Page for Locale",
			"zhTW": "No Page for Locale",
			"frFR": "Translated for Locale",
			"deDE": "Translated for Locale",
			"itIT": "No Page for Locale",
			"jaJP": "No Page for Locale",
			"koKR": "Translated for Locale",
			"ptBR": "No Page for Locale",
			"esLA": "No Page for Locale",
			"esES": "Translated for Locale",
			"enAU": "No Page for Locale",
			"enCA": "No Page for Locale",
			"enIN": "No Page for Locale",
			"enGB": "No Page for Locale",
			"idID": "No Page for Locale",
			"ruRU": "No Page for Locale",
			"svSE": "No Page for Locale",
			"viVN": "No Page for Locale",
			"plPL": "No Page for Locale",
			"arAR": "No Page for Locale",
			"nlNL": "No Page for Locale",
			"thTH": "No Page for Locale",
			"trTR": "No Page for Locale",
			"heIL": "No Page for Locale",
			"lvLV": "No Page for Locale",
			"etEE": "No Page for Locale",
			"ltLT": "No Page for Locale"
		},
		"meta_description": null,
		"metadata": {
			"imgPreview": ""
		},
		"primary_author": {},
		"publicly_index": true,
		"published_at": "2018-07-06T14:00:00.000+01:00",
		"slug": "how-to-drop-10-million-packets",
		"tags": [
			{
				"id": "64g1G2mvZyb6PjJsisO09T",
				"name": "DDoS",
				"slug": "ddos"
			},
			{
				"id": "4LtMoX5pkj38ujzMRVhgvC",
				"name": "Mitigation",
				"slug": "mitigation"
			},
			{
				"id": "6QVJOBzgKXUO9xAPEpqxvK",
				"name": "Reliability",
				"slug": "reliability"
			},
			{
				"id": "5kIxDMJCg3PXQxVINDL0Cw",
				"name": "Attacks",
				"slug": "attacks"
			},
			{
				"id": "6Mp7ouACN2rT3YjL1xaXJx",
				"name": "Security",
				"slug": "security"
			}
		],
		"title": "How to drop 10 million packets per second",
		"updated_at": "2024-10-10T00:33:14.779Z",
		"url": "https://blog.cloudflare.com/how-to-drop-10-million-packets"
	},
	"translations": {
		"posts.by": "By",
		"footer.gdpr": "GDPR",
		"lang_blurb1": "This post is also available in {lang1}.",
		"lang_blurb2": "This post is also available in {lang1} and {lang2}.",
		"lang_blurb3": "This post is also available in {lang1}, {lang2} and {lang3}.",
		"footer.press": "Press",
		"header.title": "The Cloudflare Blog",
		"search.clear": "Clear",
		"search.filter": "Filter",
		"search.source": "Source",
		"footer.careers": "Careers",
		"footer.company": "Company",
		"footer.support": "Support",
		"footer.the_net": "theNet",
		"search.filters": "Filters",
		"footer.our_team": "Our team",
		"footer.webinars": "Webinars",
		"page.more_posts": "More posts",
		"posts.time_read": "{time} min read",
		"search.language": "Language",
		"footer.community": "Community",
		"footer.resources": "Resources",
		"footer.solutions": "Solutions",
		"footer.trademark": "Trademark",
		"header.subscribe": "Subscribe",
		"footer.compliance": "Compliance",
		"footer.free_plans": "Free plans",
		"footer.impact_ESG": "Impact/ESG",
		"posts.follow_on_X": "Follow on X",
		"footer.help_center": "Help center",
		"footer.network_map": "Network Map",
		"header.please_wait": "Please Wait",
		"page.related_posts": "Related posts",
		"search.result_stat": "Results <strong>{search_range}</strong> of <strong>{search_total}</strong> for <strong>{search_keyword}</strong>",
		"footer.case_studies": "Case Studies",
		"footer.connect_2024": "Connect 2024",
		"footer.terms_of_use": "Terms of Use",
		"footer.white_papers": "White Papers",
		"footer.cloudflare_tv": "Cloudflare TV",
		"footer.community_hub": "Community Hub",
		"footer.compare_plans": "Compare plans",
		"footer.contact_sales": "Contact Sales",
		"header.contact_sales": "Contact Sales",
		"header.email_address": "Email Address",
		"page.error.not_found": "Page not found",
		"footer.developer_docs": "Developer docs",
		"footer.privacy_policy": "Privacy Policy",
		"footer.request_a_demo": "Request a demo",
		"page.continue_reading": "Continue reading",
		"footer.analysts_report": "Analyst reports",
		"footer.for_enterprises": "For enterprises",
		"footer.getting_started": "Getting Started",
		"footer.learning_center": "Learning Center",
		"footer.project_galileo": "Project Galileo",
		"pagination.newer_posts": "Newer Posts",
		"pagination.older_posts": "Older Posts",
		"posts.social_buttons.x": "Discuss on X",
		"search.icon_aria_label": "Search",
		"search.source_location": "Source/Location",
		"footer.about_cloudflare": "About Cloudflare",
		"footer.athenian_project": "Athenian Project",
		"footer.become_a_partner": "Become a partner",
		"footer.cloudflare_radar": "Cloudflare Radar",
		"footer.network_services": "Network services",
		"footer.trust_and_safety": "Trust & Safety",
		"header.get_started_free": "Get Started Free",
		"page.search.placeholder": "Search Cloudflare",
		"footer.cloudflare_status": "Cloudflare Status",
		"footer.cookie_preference": "Cookie Preferences",
		"header.valid_email_error": "Must be valid email.",
		"search.result_stat_empty": "Results <strong>{search_range}</strong> of <strong>{search_total}</strong>",
		"footer.connectivity_cloud": "Connectivity cloud",
		"footer.developer_services": "Developer services",
		"footer.investor_relations": "Investor relations",
		"page.not_found.error_code": "Error Code: 404",
		"search.autocomplete_title": "Insert a query. Press enter to send",
		"footer.logos_and_press_kit": "Logos & press kit",
		"footer.application_services": "Application services",
		"footer.get_a_recommendation": "Get a recommendation",
		"posts.social_buttons.reddit": "Discuss on Reddit",
		"footer.sse_and_sase_services": "SSE and SASE services",
		"page.not_found.outdated_link": "You may have used an outdated link, or you may have typed the address incorrectly.",
		"footer.report_security_issues": "Report Security Issues",
		"page.error.error_message_page": "Sorry, we can't find the page you are looking for.",
		"header.subscribe_notifications": "Subscribe to receive notifications of new posts:",
		"footer.cloudflare_for_campaigns": "Cloudflare for Campaigns",
		"header.subscription_confimation": "Subscription confirmed. Thank you for subscribing!",
		"posts.social_buttons.hackernews": "Discuss on Hacker News",
		"footer.diversity_equity_inclusion": "Diversity, equity & inclusion",
		"footer.critical_infrastructure_defense_project": "Critical Infrastructure Defense Project"
	}
}