{
	"componentChunkName": "component---src-components-learning-center-templates-learning-center-article-template-tsx",
	"path": "/learning/ai/owasp-top-10-risks-for-llms/",
	"result": {
		"data": {
			"learningCenterArticle": {
				"contentTypeId": "learningCenterArticle",
				"contentfulId": "64QIQQpqhRqgIwWd5aQlg6",
				"urlSlug": "ai/owasp-top-10-risks-for-llms",
				"metaTags": {
					"metaTitle": "What are the OWASP Top 10 risks for LLMs?",
					"metaDescription": "Discover the top 10 cyber security risks with deploying and managing large language model (LLM) applications, according to OWASP. ",
					"twitterCustomImage": null,
					"metaImage": {
						"file": {
							"publicURL": "https://cf-assets.www.cloudflare.com/slt3lc6tev37/53qCYhQbir5WtIU0VDWESo/954a48bfb17f429acf469e5f14345d83/unnamed-3.png"
						},
						"description": "DO NOT REMOVE, THIS IS CLOUDFLARE'S GLOBAL OG META ASSET"
					},
					"facebookCustomImage": {
						"file": {
							"publicURL": "https://cf-assets.www.cloudflare.com/slt3lc6tev37/53qCYhQbir5WtIU0VDWESo/954a48bfb17f429acf469e5f14345d83/unnamed-3.png"
						},
						"description": "DO NOT REMOVE, THIS IS CLOUDFLARE'S GLOBAL OG META ASSET"
					}
				},
				"metaTitle": null,
				"metaDescription": null,
				"learningCenterArticleSubHeader": {
					"contentTypeId": "learningCenterArticleSubHeader",
					"contentfulId": "5JjQb2yV1d1T7WTVZQkNEW",
					"learningCenterName": "artificialintelligence",
					"links": [
						{
							"contentTypeId": "learningCenterArticleLink",
							"contentfulId": "7HZD3CNwacCcZXLQxHtX8k",
							"displayText": "What is artificial intelligence (AI)?",
							"url": "/learning/ai/what-is-artificial-intelligence/"
						},
						{
							"contentTypeId": "learningCenterArticleLink",
							"contentfulId": "7lSEVfyAjPWCZDHNj1v27k",
							"displayText": "What is a large language model (LLM)?",
							"url": "/learning/ai/what-is-large-language-model/"
						},
						{
							"contentTypeId": "learningCenterArticleLinkedList",
							"contentfulId": "41JHcwdZJopZYPQOHjdkkE",
							"name": "Machine learning",
							"displayText": "Machine learning",
							"links": [
								{
									"contentTypeId": "learningCenterArticleLink",
									"contentfulId": "3KWX7josW5KFyGkx9WGxmj",
									"displayText": "What is machine learning?",
									"url": "/learning/ai/what-is-machine-learning/"
								},
								{
									"contentTypeId": "learningCenterArticleLink",
									"contentfulId": "4Hus7gX2HZjDAzrIDfhz7Y",
									"displayText": "What is deep learning?",
									"url": "/learning/ai/what-is-deep-learning/"
								},
								{
									"contentTypeId": "learningCenterArticleLink",
									"contentfulId": "5uKuVMtEUTbsmkjOh86Dsl",
									"displayText": "Neural networks",
									"url": "/learning/ai/what-is-neural-network/"
								},
								{
									"contentTypeId": "learningCenterArticleLink",
									"contentfulId": "6COFjJFWkujpQraRJD2KHy",
									"displayText": "What is generative AI?",
									"url": "/learning/ai/what-is-generative-ai/"
								},
								{
									"contentTypeId": "learningCenterArticleLink",
									"contentfulId": "1O4EuQ4MK3qvTaXomAfNzQ",
									"displayText": "Predictive AI",
									"url": "/learning/ai/what-is-predictive-ai/"
								},
								{
									"contentTypeId": "learningCenterArticleLink",
									"contentfulId": "2rqSZGk3Uk8E4RbUBTXhuV",
									"displayText": "AI image generation",
									"url": "/learning/ai/ai-image-generation/"
								},
								{
									"contentTypeId": "learningCenterArticleLink",
									"contentfulId": "7x6FotqqlG1hJShGntgbVC",
									"displayText": "What is big data?",
									"url": "/learning/ai/big-data/"
								},
								{
									"contentTypeId": "learningCenterArticleLink",
									"contentfulId": "1MP0vEt5FzyhNppPMZqEHp",
									"displayText": "What is agentic AI?",
									"url": "/learning/ai/what-is-agentic-ai/"
								}
							]
						},
						{
							"contentTypeId": "learningCenterArticleLinkedList",
							"contentfulId": "2WaoB1uAGPdfW9vr56U72B",
							"name": "Glossary - AI subheader",
							"displayText": "Glossary",
							"links": [
								{
									"contentTypeId": "learningCenterArticleLink",
									"contentfulId": "6oEjFOQyE5WSiuDLmNgmOu",
									"displayText": "What are embeddings?",
									"url": "/learning/ai/what-are-embeddings/"
								},
								{
									"contentTypeId": "learningCenterArticleLink",
									"contentfulId": "1rU1Ipdjz9cDWMUQYOjsCe",
									"displayText": "Vector database",
									"url": "/learning/ai/what-is-vector-database/"
								},
								{
									"contentTypeId": "learningCenterArticleLink",
									"contentfulId": "12RjN72z2wlElvMBzyICp9",
									"displayText": "AI inference vs. training",
									"url": "/learning/ai/inference-vs-training/"
								},
								{
									"contentTypeId": "learningCenterArticleLink",
									"contentfulId": "2pLwK8cnRhtcrFR53cRAME",
									"displayText": "What is natural language processing (NLP)?",
									"url": "/learning/ai/natural-language-processing-nlp/"
								},
								{
									"contentTypeId": "learningCenterArticleLink",
									"contentfulId": "4bUf7y8NP577uqtWUkgSwW",
									"displayText": "Low-rank adaptation (LoRA)",
									"url": "/learning/ai/what-is-lora/"
								},
								{
									"contentTypeId": "learningCenterArticleLink",
									"contentfulId": "4T93P1Kxs7ZeP2eTLMiFs9",
									"displayText": "AI hallucinations",
									"url": "/learning/ai/what-are-ai-hallucinations/"
								},
								{
									"contentTypeId": "learningCenterArticleLink",
									"contentfulId": "4xeYgczMnCqQ0wO8ORIbVY",
									"displayText": "AI quantization",
									"url": "/learning/ai/what-is-quantization/"
								},
								{
									"contentTypeId": "learningCenterArticleLink",
									"contentfulId": "3QYEJFeqaOx0hUjaQCkI4v",
									"displayText": "OWASP Top 10 for LLMs",
									"url": "/learning/ai/owasp-top-10-risks-for-llms/"
								},
								{
									"contentTypeId": "learningCenterArticleLink",
									"contentfulId": "28O79uvws5OhhzxUUPMr8z",
									"displayText": "AI data poisoning",
									"url": "/learning/ai/data-poisoning/"
								},
								{
									"contentTypeId": "learningCenterArticleLink",
									"contentfulId": "7uV0UxfanyoFf3Xrjnp2fk",
									"displayText": "Retrieval augmented generation (RAG)",
									"url": "/learning/ai/retrieval-augmented-generation-rag/"
								},
								{
									"contentTypeId": "learningCenterArticleLink",
									"contentfulId": "13XGjbidrGmhw6y7WyyG4F",
									"displayText": "Model Context Protocol (MCP)",
									"url": "/learning/ai/what-is-model-context-protocol-mcp/"
								},
								{
									"contentTypeId": "learningCenterArticleLink",
									"contentfulId": "7kvDdMXof90koTqWiUdRX5",
									"displayText": "History of AI",
									"url": "/learning/ai/history-of-ai/"
								},
								{
									"contentTypeId": "learningCenterArticleLink",
									"contentfulId": "pjgC00TeWrxtIOybBfUbh",
									"displayText": "What is vibe coding?",
									"url": "/learning/ai/ai-vibe-coding/"
								},
								{
									"contentTypeId": "learningCenterArticleLink",
									"contentfulId": "1MZdfQqWrwJ7KJel7rp6d2",
									"displayText": "Third wave of AI",
									"url": "/learning/ai/evolution-of-ai/"
								},
								{
									"contentTypeId": "learningCenterArticleLink",
									"contentfulId": "2MQDOpshVTC5lqGfPJckFU",
									"displayText": "ChatGPT plugins",
									"url": "/learning/ai/chatgpt-plugins/"
								}
							]
						}
					]
				},
				"learningCenterArticleFooter": {
					"contentTypeId": "learningCenterArticleFooter",
					"contentfulId": "4fHZ91razv1xm1g6m2sAcT",
					"learningCenterName": "artificialintelligence",
					"column1Title": "Artificial intelligence",
					"column1": {
						"contentTypeId": "learningCenterArticleLinkedList",
						"contentfulId": "504wIqF2RxToUI652YFAHo",
						"name": "AI Footer - Artificial intelligence",
						"displayText": "Artificial intelligence",
						"links": [
							{
								"contentTypeId": "learningCenterArticleLink",
								"contentfulId": "7HZD3CNwacCcZXLQxHtX8k",
								"displayText": "What is artificial intelligence (AI)?",
								"url": "/learning/ai/what-is-artificial-intelligence/"
							},
							{
								"contentTypeId": "learningCenterArticleLink",
								"contentfulId": "12RjN72z2wlElvMBzyICp9",
								"displayText": "AI inference vs. training",
								"url": "/learning/ai/inference-vs-training/"
							},
							{
								"contentTypeId": "learningCenterArticleLink",
								"contentfulId": "7kvDdMXof90koTqWiUdRX5",
								"displayText": "History of AI",
								"url": "/learning/ai/history-of-ai/"
							}
						]
					},
					"column2Title": "Machine learning",
					"column2": {
						"contentTypeId": "learningCenterArticleLinkedList",
						"contentfulId": "4WbETih2OgXGNXGsvb7kMZ",
						"name": "AI footer - Machine Learning",
						"displayText": "Machine learning",
						"links": [
							{
								"contentTypeId": "learningCenterArticleLink",
								"contentfulId": "3KWX7josW5KFyGkx9WGxmj",
								"displayText": "What is machine learning?",
								"url": "/learning/ai/what-is-machine-learning/"
							},
							{
								"contentTypeId": "learningCenterArticleLink",
								"contentfulId": "4Hus7gX2HZjDAzrIDfhz7Y",
								"displayText": "What is deep learning?",
								"url": "/learning/ai/what-is-deep-learning/"
							},
							{
								"contentTypeId": "learningCenterArticleLink",
								"contentfulId": "7lSEVfyAjPWCZDHNj1v27k",
								"displayText": "What is a large language model (LLM)?",
								"url": "/learning/ai/what-is-large-language-model/"
							},
							{
								"contentTypeId": "learningCenterArticleLink",
								"contentfulId": "4bUf7y8NP577uqtWUkgSwW",
								"displayText": "Low-rank adaptation (LoRA)",
								"url": "/learning/ai/what-is-lora/"
							},
							{
								"contentTypeId": "learningCenterArticleLink",
								"contentfulId": "2rqSZGk3Uk8E4RbUBTXhuV",
								"displayText": "AI image generation",
								"url": "/learning/ai/ai-image-generation/"
							}
						]
					},
					"column3Title": "Big data",
					"column3": {
						"contentTypeId": "learningCenterArticleLinkedList",
						"contentfulId": "3q3KTRPkBi7rS3aDeQoMNn",
						"name": "AI footer - Big Data",
						"displayText": "Big data",
						"links": [
							{
								"contentTypeId": "learningCenterArticleLink",
								"contentfulId": "6oEjFOQyE5WSiuDLmNgmOu",
								"displayText": "What are embeddings?",
								"url": "/learning/ai/what-are-embeddings/"
							},
							{
								"contentTypeId": "learningCenterArticleLink",
								"contentfulId": "7x6FotqqlG1hJShGntgbVC",
								"displayText": "What is big data?",
								"url": "/learning/ai/big-data/"
							}
						]
					},
					"column4Title": "Glossary",
					"column4": {
						"contentTypeId": "learningCenterArticleLinkedList",
						"contentfulId": "1UCfw4v8J5rpL4BsVNlgVC",
						"name": "AI footer - Glossary",
						"displayText": "AI glossary",
						"links": [
							{
								"contentTypeId": "learningCenterArticleLink",
								"contentfulId": "1rU1Ipdjz9cDWMUQYOjsCe",
								"displayText": "Vector database",
								"url": "/learning/ai/what-is-vector-database/"
							},
							{
								"contentTypeId": "learningCenterArticleLink",
								"contentfulId": "1O4EuQ4MK3qvTaXomAfNzQ",
								"displayText": "Predictive AI",
								"url": "/learning/ai/what-is-predictive-ai/"
							},
							{
								"contentTypeId": "learningCenterArticleLink",
								"contentfulId": "2MQDOpshVTC5lqGfPJckFU",
								"displayText": "ChatGPT plugins",
								"url": "/learning/ai/chatgpt-plugins/"
							},
							{
								"contentTypeId": "learningCenterArticleLink",
								"contentfulId": "5uKuVMtEUTbsmkjOh86Dsl",
								"displayText": "Neural networks",
								"url": "/learning/ai/what-is-neural-network/"
							},
							{
								"contentTypeId": "learningCenterArticleLink",
								"contentfulId": "6COFjJFWkujpQraRJD2KHy",
								"displayText": "What is generative AI?",
								"url": "/learning/ai/what-is-generative-ai/"
							},
							{
								"contentTypeId": "learningCenterArticleLink",
								"contentfulId": "2pLwK8cnRhtcrFR53cRAME",
								"displayText": "What is natural language processing (NLP)?",
								"url": "/learning/ai/natural-language-processing-nlp/"
							},
							{
								"contentTypeId": "learningCenterArticleLink",
								"contentfulId": "4T93P1Kxs7ZeP2eTLMiFs9",
								"displayText": "AI hallucinations",
								"url": "/learning/ai/what-are-ai-hallucinations/"
							},
							{
								"contentTypeId": "learningCenterArticleLink",
								"contentfulId": "4xeYgczMnCqQ0wO8ORIbVY",
								"displayText": "AI quantization",
								"url": "/learning/ai/what-is-quantization/"
							},
							{
								"contentTypeId": "learningCenterArticleLink",
								"contentfulId": "3QYEJFeqaOx0hUjaQCkI4v",
								"displayText": "OWASP Top 10 for LLMs",
								"url": "/learning/ai/owasp-top-10-risks-for-llms/"
							},
							{
								"contentTypeId": "learningCenterArticleLink",
								"contentfulId": "28O79uvws5OhhzxUUPMr8z",
								"displayText": "AI data poisoning",
								"url": "/learning/ai/data-poisoning/"
							},
							{
								"contentTypeId": "learningCenterArticleLink",
								"contentfulId": "7uV0UxfanyoFf3Xrjnp2fk",
								"displayText": "Retrieval augmented generation (RAG)",
								"url": "/learning/ai/retrieval-augmented-generation-rag/"
							},
							{
								"contentTypeId": "learningCenterArticleLink",
								"contentfulId": "1MP0vEt5FzyhNppPMZqEHp",
								"displayText": "What is agentic AI?",
								"url": "/learning/ai/what-is-agentic-ai/"
							},
							{
								"contentTypeId": "learningCenterArticleLink",
								"contentfulId": "1MZdfQqWrwJ7KJel7rp6d2",
								"displayText": "Third wave of AI",
								"url": "/learning/ai/evolution-of-ai/"
							},
							{
								"contentTypeId": "learningCenterArticleLink",
								"contentfulId": "pjgC00TeWrxtIOybBfUbh",
								"displayText": "What is vibe coding?",
								"url": "/learning/ai/ai-vibe-coding/"
							},
							{
								"contentTypeId": "learningCenterArticleLink",
								"contentfulId": "13XGjbidrGmhw6y7WyyG4F",
								"displayText": "Model Context Protocol (MCP)",
								"url": "/learning/ai/what-is-model-context-protocol-mcp/"
							}
						]
					},
					"column5Title": "Learning Center",
					"column5": {
						"contentTypeId": "learningCenterArticleLinkedList",
						"contentfulId": "5kj4ISBExxQfI9CaYifpU9",
						"name": "AI Learning Center footer",
						"displayText": "Learning Center Navigation",
						"links": [
							{
								"contentTypeId": "learningCenterArticleLink",
								"contentfulId": "1yItw6W6SsM2Y0Wuq6Y2c6",
								"displayText": "Security Learning Center",
								"url": "/learning/security/what-is-web-application-security/"
							},
							{
								"contentTypeId": "learningCenterArticleLink",
								"contentfulId": "6DGcnYIMM0eCGwqAKu2ECO",
								"displayText": "CDN Learning Center",
								"url": "/learning/cdn/what-is-a-cdn/"
							},
							{
								"contentTypeId": "learningCenterArticleLink",
								"contentfulId": "56Y6OytBNgNoNDWQL2ezlf",
								"displayText": "DDoS Learning Center",
								"url": "/learning/ddos/what-is-a-ddos-attack/"
							},
							{
								"contentTypeId": "learningCenterArticleLink",
								"contentfulId": "4Ra0qmFJ1uewk2MYscW8KI",
								"displayText": "DNS Learning Center",
								"url": "/learning/dns/what-is-dns/"
							},
							{
								"contentTypeId": "learningCenterArticleLink",
								"contentfulId": "Q67XLnKqKhfmDJR4q9hJT",
								"displayText": "Performance Learning Center",
								"url": "/learning/performance/why-site-speed-matters/"
							},
							{
								"contentTypeId": "learningCenterArticleLink",
								"contentfulId": "1z3UC9tP1Kjk85BX5zSGPY",
								"displayText": "Serverless Learning Center",
								"url": "/learning/serverless/what-is-serverless/"
							},
							{
								"contentTypeId": "learningCenterArticleLink",
								"contentfulId": "52V7iI9YriJ0En2SGmEXz5",
								"displayText": "SSL Learning Center",
								"url": "/learning/ssl/what-is-ssl/"
							},
							{
								"contentTypeId": "learningCenterArticleLink",
								"contentfulId": "7D22pOfjb98KXcnAJyJFCX",
								"displayText": "Bots Learning Center",
								"url": "/learning/bots/what-is-a-bot/"
							},
							{
								"contentTypeId": "learningCenterArticleLink",
								"contentfulId": "7jgqDUFFzaLuKrsaBTLdV0",
								"displayText": "Cloud Learning Center",
								"url": "/learning/cloud/what-is-the-cloud/"
							},
							{
								"contentTypeId": "learningCenterArticleLink",
								"contentfulId": "6PdBQjiFs98K6RQuG5TYNd",
								"displayText": "Access Management Learning Center",
								"url": "/learning/access-management/what-is-identity-and-access-management/"
							},
							{
								"contentTypeId": "learningCenterArticleLink",
								"contentfulId": "3jjEMuuLrCwhRidjG5kglM",
								"displayText": "Network Layer Learning Center",
								"url": "/learning/network-layer/what-is-the-network-layer/"
							},
							{
								"contentTypeId": "learningCenterArticleLink",
								"contentfulId": "6RvS0l0m86G1VISDrEvlJb",
								"displayText": "Privacy Learning Center",
								"url": "/learning/privacy/what-is-data-privacy/"
							},
							{
								"contentTypeId": "learningCenterArticleLink",
								"contentfulId": "MytksT5WU5mh863BQYEkj",
								"displayText": "Video Streaming Learning Center",
								"url": "/learning/video/what-is-streaming/"
							},
							{
								"contentTypeId": "learningCenterArticleLink",
								"contentfulId": "5ysNN6LRvlvlFj4ekk2xq4",
								"displayText": "Email Security Learning Center",
								"url": "/learning/email-security/what-is-email-security/"
							},
							{
								"contentTypeId": "learningCenterArticleLink",
								"contentfulId": "2hdSVoYA5asSIG0U2ae20c",
								"displayText": "Learning Center Home",
								"url": "/learning/"
							}
						]
					}
				},
				"header": "What are the OWASP Top 10 risks for LLMs?",
				"blurbSubHeader": "Large language model (LLM) applications are vulnerable to prompt injection, data poisoning, model denial of service, and more attacks.",
				"objectivesHeader": "OWASP Top 10 risks for LLMs",
				"objectivesList": [
					"Understand what OWASP is",
					"Summarize each of the OWASP Top 10 threats for LLMs",
					"Uncover ways to address LLM vulnerabilities"
				],
				"relatedContentLinkText": "Related Content",
				"relatedContent": [
					{
						"contentTypeId": "learningCenterArticleLink",
						"contentfulId": "4Nzjjp656PRDIGKSTWZT3j",
						"displayText": "What is the OWASP Top 10?",
						"url": "/learning/security/threats/owasp-top-10/"
					},
					{
						"contentTypeId": "learningCenterArticleLink",
						"contentfulId": "7HZD3CNwacCcZXLQxHtX8k",
						"displayText": "What is artificial intelligence (AI)?",
						"url": "/learning/ai/what-is-artificial-intelligence/"
					},
					{
						"contentTypeId": "learningCenterArticleLink",
						"contentfulId": "7lSEVfyAjPWCZDHNj1v27k",
						"displayText": "What is a large language model (LLM)?",
						"url": "/learning/ai/what-is-large-language-model/"
					},
					{
						"contentTypeId": "learningCenterArticleLink",
						"contentfulId": "6COFjJFWkujpQraRJD2KHy",
						"displayText": "What is generative AI?",
						"url": "/learning/ai/what-is-generative-ai/"
					},
					{
						"contentTypeId": "learningCenterArticleLink",
						"contentfulId": "2pLwK8cnRhtcrFR53cRAME",
						"displayText": "What is natural language processing (NLP)?",
						"url": "/learning/ai/natural-language-processing-nlp/"
					}
				],
				"enablementBlade": null,
				"desktopMainContent": "<h2 class=\"learning-content-h2 learning-content-h2--margin-top-16px\" itemprop=\"headline\">What is OWASP?</h2>\n\n<p>The Open Web Application Security Project (OWASP) is an international non-profit organization with <a href='/learning/security/what-is-web-application-security/'>web application</a> security as its core mission. OWASP strives to help other organizations improve their web application security by providing a range of free information through documents, tools, videos, conferences, and forums.</p>\n\n<p>The <a href='/learning/security/threats/owasp-top-10/'>OWASP Top 10 report</a> highlights the 10 most critical risks for application security, according to security experts. OWASP recommends that all organizations incorporate insights from this report into their <a href='https://www.cloudflare.com/the-net/state-application-security/'>web application security strategy</a>.</p>\n\n<p>In 2023, an OWASP working group launched a new project to create a similar report focusing on threats to <a href='/learning/ai/what-is-large-language-model/'>large language model (LLM)</a> applications. The OWASP Top 10 for Large Language Model Applications identifies threats, provides examples of vulnerabilities and real-work attack scenarios, and offers mitigation strategies. OWASP hopes to raise awareness among developers, designers, architects, and managers while also helping them defend against threats.</p>\n\n<p>Below are the vulnerabilities highlighted in the OWASP Top 10 for LLM Applications report from October 2023:</p>\n\n<h4 class='learning-content-h4'>1. Prompt injection</h4>\n<p>Prompt injection is a tactic in which attackers manipulate the prompts used for an LLM. Attackers might intend to steal sensitive information, affect decision-making processes guided by the LLM, or use the LLM in a <a href='/learning/security/threats/social-engineering-attack/'>social engineering</a> scheme.</p>\n\n<p>Attackers might manipulate prompts in two ways:</p>\n\n<ul>\n<li><strong>Direct prompt injection</strong> (also called \"jailbreaking\") is the process of overwriting the system prompt, which instructs the LLM on how to respond to user input. Through this tactic, the attacker might be able to access and exploit backend systems.</li>\n\n<li><strong>Indirect prompt injection</strong> is when an attacker controls external websites, files, or other external sources that are used as input for the LLM. The attacker could then exploit the systems that the LLM accesses or employ the model to manipulate the user. </li></ul>\n\n<p>There are multiple ways to prevent damage from prompt injections. For example, organizations can implement robust <a href='/learning/access-management/what-is-access-control/'>access control</a> policies for backend systems, integrate humans into LLM-directed processes, and ensure humans have the final say over LLM-driven decisions. </p>\n\n<h4 class='learning-content-h4'>2. Insecure output handling</h4>\n<p>When organizations fail to scrutinize LLM outputs, any outputs generated by malicious users could cause problems with downstream systems. The exploitation of insecure output handling could result in <a href='/learning/security/threats/cross-site-scripting/'>cross-site scripting (XSS)</a>, <a href='/learning/security/threats/cross-site-request-forgery/'>cross-site request forgery (CSRF)</a>, server-side request forgery (SSRF), <a href='/learning/security/what-is-remote-code-execution/'>remote code execution (RCE)</a>, and other types of attacks. For example, an attacker might cause an LLM to output a malicious script that is interpreted by a browser, resulting in an XSS attack.</p>\n\n<p>Organizations can prevent insecure output handling by applying a <a href='/learning/security/glossary/what-is-zero-trust/'>Zero Trust security</a> model and treating the LLM like any user or device. They would validate any output from the LLM before allowing them to drive other functions.</p>\n\n<h4 class='learning-content-h4'>3. Training data poisoning</h4>\n<p>Attackers might attempt to manipulate — or \"poison\" — data used for training an LLM model. <a href='https://www.cloudflare.com/learning/ai/data-poisoning/'>Data poisoning</a> can hinder the model's ability to deliver accurate results or support AI-driven decision making. This type of attack could be launched by malicious competitors who want to damage the reputation of the organization using the model.</p>\n\n<p>To reduce the likelihood of data poisoning, organizations must secure the data supply chain. As part of that work, they should verify the legitimacy of data sources — including any components of <a href='/learning/ai/big-data/'>big data</a> used for modeling. They should also prevent the model from scraping data from untrusted sources and sanitize data.</p>\n\n<h4 class='learning-content-h4'>4. Model denial of service</h4>\n<p>Similar to a <a href='/learning/ddos/what-is-a-ddos-attack/'>distributed denial-of-service (DDoS)</a> attack, attackers might run resource-heavy operations using an LLM in an attempt to degrade service quality, drive up costs, or otherwise disrupt operations. This type of attack might go undetected since LLMs often consume large amounts of resources, and resource demands can fluctuate depending on user inputs. </p>\n\n<p>To avoid this type of <a href='/learning/ddos/glossary/denial-of-service/'>denial-of-service</a> attack, organizations can enforce <a href='/learning/security/api/what-is-an-api/'>API</a> <a href='/learning/bots/what-is-rate-limiting/'>rate limits</a> for individual users or <a href='/learning/dns/glossary/what-is-my-ip-address/'>IP addresses</a>. They can also validate and sanitize inputs. And they should continuously monitor resource usage to identify any suspicious spikes. </p>\n\n<h4 class='learning-content-h4'>5. Supply chain vulnerabilities</h4>\n<p>Vulnerabilities in the supply chain for LLM applications can leave models exposed to security risks or yield inaccurate results. Several components used for LLM applications — including pre-trained models, the data used to train models, third-party data sets, and plugins — can set the groundwork for <a href='/learning/security/what-is-a-supply-chain-attack/'>an attack</a> or cause other problems with the LLM application's operation. </p> \n\n<p><a href='https://www.cloudflare.com/the-net/supply-chain-attacks/'>Addressing supply chain vulnerabilities</a> starts with carefully vetting suppliers and ensuring they have adequate security in place. Organizations should also maintain an up-to-date inventory of components, and scrutinize supplied data and models. </p>\n\n<h4 class='learning-content-h4'>6. Sensitive information disclosure</h4>\n<p>LLM applications might inadvertently reveal confidential data in responses, ranging from sensitive customer information to intellectual property. These types of disclosures could constitute compliance violations or lead to security breaches. </p>\n\n<p>Mitigation efforts should focus on preventing <a href='/learning/privacy/what-is-personal-information/'>confidential information</a> and malicious inputs from entering training models in the first place. Data sanitizing and scrubbing are essential for these efforts. </p>\n\n<p>Since building LLMs might involve cross-border data transfers, organizations should also implement automated <a href='/learning/privacy/what-is-data-localization/'>data localization</a> controls that keep certain sensitive data in specific regions. They can allow other data to be incorporated into LLMs.</p>\n\n<h4 class='learning-content-h4'>7. Insecure plugin design</h4>\n<p>LLM plugins can enhance model functionality and facilitate integration with third-party services. But some plugins might lack sufficient access controls, creating opportunities for attackers to inject malicious inputs. Those inputs could enable RCE or another type of attack. </p>\n\n<p>Preventing plugin exploitation requires more secure plugin design. Plugins should control inputs and perform input checks, making sure no malicious code gets through. In addition, plugins should implement authentication controls based on the <a href='/learning/access-management/principle-of-least-privilege/'>principle of least privilege</a>.</p>\n\n<h4 class='learning-content-h4'>8. Excessive agency</h4>\n<p>Developers often give LLM applications some degree of agency — the ability to take actions automatically in response to a prompt. Giving applications too much agency, however, can cause problems. If an LLM produces unexpected outputs (because of an attack, an AI hallucination, or some other error), the application could take potentially damaging actions, such as disclosing sensitive information or deleting files. </p>\n\n<p>The best way to prevent excessive agency is for developers to limit the functionality, permissions, and autonomy of plugins and other tools to the minimum levels necessary. Organizations running LLM applications with plugins can also require humans to authorize certain actions before they are taken.</p>\n\n<h4 class='learning-content-h4'>9. Overreliance</h4>\n<p>LLMs are not perfect. They can occasionally produce factually incorrect results, <a href='/learning/ai/what-are-ai-hallucinations/'>AI hallucinations</a>, or biased results, even though they might deliver those results in an authoritative way. When organizations or individuals rely on LLMs excessively, they can disseminate incorrect information that leads to regulatory violations, legal exposure, and damaged reputations.</p>\n\n<p>To avoid the problems of overreliance, organizations should implement LLM oversight policies. They also should regularly review outputs and compare them with information in other, trusted external sources to confirm their accuracy.</p>\n\n<h4 class='learning-content-h4'>10. Model theft</h4>\n<p>Attackers might attempt to access, copy, or steal proprietary LLM models. These attacks could result in the erosion of a company's competitive edge or the loss of sensitive information within the model. </p>\n\n<p>Applying strong access controls, including <a href='/learning/access-management/role-based-access-control-rbac/'>role-based access control (RBAC)</a> capabilities, can help prevent unauthorized access to LLM models. Organizations should also regularly monitor access logs and respond to any unauthorized behavior. <a href='/learning/access-management/what-is-dlp/'>Data loss prevention (DLP)</a> capabilities can help spot attempts to <a href='https://www.cloudflare.com/learning/security/what-is-data-exfiltration/'>exfiltrate</a> information from the application. </p>\n\n<h2 class=\"learning-content-h2\" itemprop=\"headline\">How can organizations secure LLMs?</h2>\n\n<p>As the OWASP document suggests, organizations need a multi-faceted strategy to protect LLM applications from threats. For example, they should: </p>\n\n<ul>\n<li>Analyze network traffic for patterns that might indicate a breached LLM, which could compromise applications. </li>\n\n<li>Establish real-time visibility into packets and data interacting with LLMs at the bit level.</li>\n\n<li>Apply DLP to secure sensitive data in transit.</li>\n\n<li>Verify, filter, and isolate traffic to protect applications from compromised LLMs.</li>\n\n<li>Employ <a href='/learning/access-management/what-is-browser-isolation/'>remote browser isolation (RBI)</a> to insulate users from models with injected malicious code.</li>\n\n<li>Use <a href='/learning/ddos/glossary/web-application-firewall-waf/'>web application firewall (WAF)</a>–managed rulesets to block LLM attacks based on SQL injection, XSS, and other web attack vectors.</li>\n\n<li><a href='https://www.cloudflare.com/the-net/roadmap-zerotrust/'>Employ a Zero Trust security model</a> to shrink their attack surface by granting only context-based, least-privilege access per resource. </li>\n</ul>\n\n<h2 class=\"learning-content-h2\" itemprop=\"headline\">How does Cloudflare help reduce LLM risks?</h2>\n\n<p>To help organizations address the risks threatening LLM applications, Cloudflare is developing Firewall for AI — an advanced WAF designed specifically for LLM applications. Organizations will be able to deploy <a href='https://blog.cloudflare.com/firewall-for-ai/'>Firewall for AI</a> in front of LLMs to detect vulnerabilities and identify abuses before they reach models. Taking advantage of Cloudflare's large global network, it will run close to users to spot attacks early and protect both users and models.</p>\n\n<p>In addition, <a href='https://blog.cloudflare.com/ai-gateway-is-generally-available'>Cloudflare AI Gateway</a> provides an AI ops platform for managing and scaling <a href='/learning/ai/what-is-generative-ai/'>generative AI</a> workloads from a unified interface. It acts as a proxy between an organization's service and their interface provider, helping the organization observe and control AI applications.</p> \n\n<p>For a more in-depth look at the OWASP Top 10 for LLMs, see the <a href='https://owasp.org/www-project-top-10-for-large-language-model-applications/assets/PDF/OWASP-Top-10-for-LLMs-2023-v1_1.pdf'>official report</a>.</p>\n\n<h2>FAQs</h2>\n<h4>What are the top 10 risks for large language models (LLMs) according to the Open Web Application Security Project (OWASP)?</h4>\n<p>The vulnerabilities OWASP highlights in their Top 10 for LLM report include:</p>\n<ol>\n  <li>Prompt injection</li>\n  <li>Insecure output handling</li>\n  <li>Training data poisoning</li>\n  <li>Model denial of service</li>\n  <li>Supply chain vulnerabilities</li>\n  <li>Sensitive information disclosure</li>\n  <li>Insecure plugin design</li>\n  <li>Excessive agency</li>\n  <li>Overreliance</li>\n  <li>Model theft</li>\n</ol>\n\n<h4>How do LLM security vulnerabilities differ from traditional application vulnerabilities</h4>\n<p><a href='https://www.cloudflare.com/the-net/vulnerable-llm-ai/'>LLM security vulnerabilities</a> involve unique attack vectors specifically targeting the language model's reasoning and processing capabilities. Traditional applications typically face threats like SQL injection or cross-site scripting (XSS), while LLMs face novel threats like prompt injection, model denial of service, and hallucination manipulation.</p>\n\n<h4>What is prompt injection?</h4>\n<p>Prompt injection occurs when an attacker manipulates an LLM by inserting malicious inputs that override the original instructions. These attacks can lead to data theft, system manipulation, and exposure of sensitive information.</p>\n\n<h4>What are the key governance concerns for LLM applications?</h4>\n<p>LLM governance concerns include ensuring proper model oversight, managing data privacy, and implementing responsible AI practices. Strong governance frameworks help organizations <a href='https://www.cloudflare.com/the-net/pursuing-privacy-first-security/data-localization/'>maintain compliance</a> while mitigating risks related to biased outputs, intellectual property violations, and downstream liability.</p>\n\n<h4>Which strategies are most effective for securing LLMs in organizations?</h4>\n<p>To protect LLM applications, organizations should use a layered security approach that includes monitoring traffic for threats, ensuring data visibility, applying data loss prevention (DLP), filtering and isolating risky activity, using web application firewalls (WAFs) to block common attacks, and adopting a Zero Trust security model with least-privilege access.</p>\n\n<script type=\"application/ld+json\">\n{\n  \"@context\": \"https://schema.org\",\n  \"@type\": \"FAQPage\",\n  \"mainEntity\": [\n    {\n      \"@type\": \"Question\",\n      \"name\": \"What are the top 10 risks for large language models (LLMs) according to the Open Web Application Security Project (OWASP)?\",\n      \"acceptedAnswer\": {\n        \"@type\": \"Answer\",\n        \"text\": \"The vulnerabilities OWASP highlights in their Top 10 for LLM report include: Prompt injection, Insecure output handling, Training data poisoning, Model denial of service, Supply chain vulnerabilities, Sensitive information disclosure, Insecure plugin design, Excessive agency, Overreliance, and Model theft.\"\n      }\n    },\n    {\n      \"@type\": \"Question\",\n      \"name\": \"How do LLM security vulnerabilities differ from traditional application vulnerabilities?\",\n      \"acceptedAnswer\": {\n        \"@type\": \"Answer\",\n        \"text\": \"LLM security vulnerabilities involve unique attack vectors specifically targeting the language model's reasoning and processing capabilities. Traditional applications typically face threats like SQL injection or cross-site scripting (XSS), while LLMs face novel threats like prompt injection, model denial of service, and hallucination manipulation.\"\n      }\n    },\n    {\n      \"@type\": \"Question\",\n      \"name\": \"What is prompt injection?\",\n      \"acceptedAnswer\": {\n        \"@type\": \"Answer\",\n        \"text\": \"Prompt injection occurs when an attacker manipulates an LLM by inserting malicious inputs that override the original instructions. These attacks can lead to data theft, system manipulation, and exposure of sensitive information.\"\n      }\n    },\n    {\n      \"@type\": \"Question\",\n      \"name\": \"What are the key governance concerns for LLM applications?\",\n      \"acceptedAnswer\": {\n        \"@type\": \"Answer\",\n        \"text\": \"LLM governance concerns include ensuring proper model oversight, managing data privacy, and implementing responsible AI practices. Strong governance frameworks help organizations maintain compliance while mitigating risks related to biased outputs, intellectual property violations, and downstream liability.\"\n      }\n    },\n    {\n      \"@type\": \"Question\",\n      \"name\": \"Which strategies are most effective for securing LLMs in organizations?\",\n      \"acceptedAnswer\": {\n        \"@type\": \"Answer\",\n        \"text\": \"To protect LLM applications, organizations should use a layered security approach that includes monitoring traffic for threats, ensuring data visibility, applying data loss prevention (DLP), filtering and isolating risky activity, using web application firewalls (WAFs) to block common attacks, and adopting a Zero Trust security model with least-privilege access.\"\n      }\n    }\n  ]\n}\n</script>",
				"availableLocales": null,
				"localized": null,
				"localeList": {
					"enUS": "English for Locale",
					"zhCN": "Translated for Locale",
					"zhTW": "Translated for Locale",
					"frFR": "Translated for Locale",
					"deDE": "Translated for Locale",
					"itIT": "English for Locale",
					"jaJP": "Translated for Locale",
					"koKR": "Translated for Locale",
					"ptBR": "Translated for Locale",
					"esES": "Translated for Locale",
					"esLA": "Translated for Locale",
					"enAU": "English for Locale",
					"enCA": "English for Locale",
					"enIN": "English for Locale",
					"enGB": "English for Locale",
					"nlNL": "English for Locale",
					"idID": "English for Locale",
					"thTH": "English for Locale",
					"ruRU": "English for Locale",
					"svSE": "English for Locale",
					"viVN": "English for Locale",
					"trTR": "English for Locale",
					"zhHansCN": "Translated for Locale",
					"plPL": "English for Locale"
				},
				"proactivePopup": null,
				"sidebarForm": {
					"contentTypeId": "learningCenterArticleSidebarForm",
					"contentfulId": "4ad3l3edBzH8KRefyi225g",
					"heading": "Want to keep learning?",
					"postSubmissionHeading": "Thank you for subscribing",
					"description": "Subscribe to theNET, Cloudflare's monthly recap of the Internet's most popular insights!",
					"postSubmissionDescription": "We're looking forward to sharing content with you.",
					"marketoForm": {
						"contentTypeId": "marketoEmbeddedForm",
						"contentfulId": "4N6NOUDQwA5AAjxuGTnCMu",
						"bladeName": "Form-Marketo Single Inline - Learning Center Newsletter Form",
						"backgroundColor": "blue",
						"marketoFormId": 2459,
						"marketoFormLeadSource": "Inbound - Blog Subscriber",
						"marketoFormLeadSourceDetail": "[BRD] Q4'24 WEB - GBL - Learning Center Subscription",
						"marketoFormHeaderText": null,
						"marketoFormDefaultComment": null,
						"showCommentBox": null,
						"marketoFormThankYouText": "Thank you for subscribing!",
						"marketoFormCustomSuccessMessage": "We're looking forward to sharing content with you.",
						"marketoFormPdfDownload": null,
						"marketoFormPdfDownloadI18n": [],
						"marketoFormAssetDownloadButtonText": null,
						"marketoFormSuccessRedirectUrl": null,
						"sendAdRoll": null,
						"meta_adRollCustomSegment": null,
						"enableSandbox": null,
						"marketoFormSubmitButtonText": "Subscribe  to theNET",
						"marketoFormSubmitButtonColor": null,
						"enableEmailVerification": null,
						"disableEnterpriseEmailNotification": null,
						"enableRingout": null
					}
				}
			}
		},
		"pageContext": {
			"locale": "en-US",
			"contentfulId": "64QIQQpqhRqgIwWd5aQlg6",
			"pathname": "/learning/ai/owasp-top-10-risks-for-llms/",
			"baseURL": "https://www.cloudflare.com",
			"allowedHrefLangs": [
				"en-US",
				"zh-CN",
				"zh-TW",
				"fr-FR",
				"de-DE",
				"ja-JP",
				"ko-KR",
				"pt-BR",
				"es-ES",
				"es-LA",
				"zh-Hans-CN"
			]
		}
	}
}