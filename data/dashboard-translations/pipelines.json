{
	"pipelines.404.back": "Back to Pipelines",
	"pipelines.product.title": "Pipelines",
	"pipelines.product.description": "Ingest, transform, and load streaming data into Iceberg or Parquet in R2.",
	"pipelines.product.documentation": "Documentation",
	"pipelines.pipeline.id": "Pipeline ID:",
	"pipelines.create.button": "+ Create Pipeline",
	"pipelines.create.heading": "Create Pipeline",
	"pipelines.create.subheading": "Create a pipeline to R2 buckets.",
	"pipelines.create.inputs.name.label": "Pipeline name",
	"pipelines.create.inputs.name.description": "<b>Example:</b> my-pipeline. Configuration names cannot be changed.",
	"pipelines.create.inputs.name.tooltip": "Must be between 1 and 63 characters and can only contain lowercase alphanumeric characters and hypens.",
	"pipelines.overview.search.placeholder": "Search pipelines",
	"pipelines.streams.search.placeholder": "Search streams",
	"pipelines.sinks.search.placeholder": "Search sinks",
	"pipelines.list.columns.name": "Name",
	"pipelines.list.columns.created_at": "Created At",
	"pipelines.sinks.list.columns.type": "Type",
	"pipelines.sinks.list.columns.destination": "Destination",
	"pipelines.streams.list.columns.ingest_sources": "Ingest Sources",
	"pipelines.streams.actions.copy_binding": "Copy binding",
	"pipelines.sinks.destination.unknown_table": "Unknown Table",
	"pipelines.sinks.destination.unknown_bucket": "Unknown Bucket",
	"pipelines.sinks.empty.title": "Create a sink",
	"pipelines.sinks.empty.description": "Sinks are data destinations where your pipelines write data.",
	"pipelines.streams.empty.title": "Create a stream",
	"pipelines.streams.empty.description": "Streams are data sources that feed into your pipelines.",
	"pipelines.table.search": "Search Pipelines...",
	"pipelines.table.search.empty": "Specified Pipeline not found.",
	"pipelines.table.name.header": "Pipeline",
	"pipelines.table.id.header": "Pipeline Id",
	"pipelines.table.ingested_records.header": "Records",
	"pipelines.table.ingested_bytes.header": "Ingested",
	"pipelines.table.delivered_bytes.header": "Written",
	"pipelines.table.actions.copy_endpoint": "Copy Endpoint URL",
	"pipelines.table.empty": "Get started by creating your first Pipeline.",
	"pipelines.empty.title": "Create a pipeline",
	"pipelines.empty.description": "Build data pipelines to ingest, transform, and load streaming data. Click Create Pipeline above or use Wrangler CLI to get started.",
	"pipelines.metrics.tab": "Metrics",
	"pipelines.metrics.ingested_records": "Records",
	"pipelines.metrics.ingested_bytes": "Data Ingested",
	"pipelines.metrics.delivered_bytes": "Data Written",
	"pipelines.metrics.data_in": "Data in",
	"pipelines.metrics.data_out": "Data out",
	"pipelines.metrics.events": "Events",
	"pipelines.metrics.data_in.tooltip": "Total bytes ingested across all pipelines",
	"pipelines.metrics.data_out.tooltip": "Total bytes emitted across all pipelines",
	"pipelines.metrics.events.tooltip": "Total events processed across all pipelines",
	"pipelines.pipeline.metrics.charts.dataIn.title": "Data In",
	"pipelines.pipeline.metrics.charts.dataIn.tooltip": "Volume of data ingested by pipeline",
	"pipelines.pipeline.metrics.charts.dataIn.series": "Total data in",
	"pipelines.pipeline.metrics.charts.dataOut.title": "Data Out",
	"pipelines.pipeline.metrics.charts.dataOut.tooltip": "Volume of data output by pipeline",
	"pipelines.pipeline.metrics.charts.dataOut.series": "Total data out",
	"pipelines.configuration.tab": "Configuration",
	"pipelines.delete_modal.title": "Delete Pipeline?",
	"pipelines.delete_modal.warning": "Deleting this Pipeline will permanently remove it.",
	"pipelines.delete_pipeline.toast": "Pipeline **%{pipelineId}** was deleted.",
	"pipelines.pipeline.tabs.metrics": "Metrics",
	"pipelines.pipeline.tabs.sql": "Pipeline SQL",
	"pipelines.pipeline.tabs.overview": "Overview",
	"pipelines.pipeline.tabs.sink": "Sink",
	"pipelines.pipeline.tabs.settings": "Settings",
	"pipelines.pipeline.sql.heading": "Pipeline SQL",
	"pipelines.pipeline.sql.description": "The SQL query that defines how data is transformed in this pipeline.",
	"pipelines.pipeline.sql.readonly": "This SQL is read-only and cannot be modified after pipeline creation.",
	"pipelines.pipeline.sql.error": "Failed to load pipeline SQL. Please try again.",
	"pipelines.pipeline.settings.delete.title": "Delete Pipeline",
	"pipelines.pipeline.settings.delete.description": "This will permanently delete the pipeline, including its configuration. This action cannot be undone.",
	"pipelines.pipeline.settings.delete.action": "Delete pipeline",
	"pipelines.pipeline.settings.delete.modal.title": "Delete pipeline",
	"pipelines.pipeline.settings.delete.modal.content.warning": "This will permanently delete the pipeline **%{pipelineName}**. This action cannot be undone.",
	"pipelines.pipeline.settings.delete.error": "Failed to delete pipeline. Please try again.",
	"pipelines.stream.tabs.settings": "Settings",
	"pipelines.stream.tabs.getting_started": "Getting Started",
	"pipelines.pipeline.overview.title": "Pipeline Overview",
	"pipelines.pipeline.overview.description": "View connected streams and sinks for your pipeline.",
	"pipelines.pipeline.connected_streams": "Connected Streams",
	"pipelines.pipeline.connected_sinks": "Connected Sinks",
	"pipelines.sink.settings.general.id.title": "Sink ID",
	"pipelines.stream.settings.general.title": "General",
	"pipelines.stream.settings.general.id.title": "Stream ID",
	"pipelines.stream.settings.general.created.title": "Created",
	"pipelines.stream.settings.general.delete.title": "This will permanently delete the stream, including its schema and configuration.",
	"pipelines.sink.settings.general.title": "General",
	"pipelines.sink.settings.general.delete.title": "This will permanently delete the sink, including its configuration.",
	"pipelines.stream.settings.general.delete.modal.title": "Delete stream",
	"pipelines.stream.settings.general.delete.modal.content.warning": "This will permanently delete the stream **%{streamName}**. This action cannot be undone.",
	"pipelines.resources.get_started": "Get Started with Pipelines",
	"pipelines.resources.developer_docs": "Developer documentation",
	"pipelines.resources.discord": "Discord server",
	"pipelines.resources.community_forum": "Community forum",
	"pipelines.errors.unauthorized": "Insufficient permissions. Please contact a Super Administrator to grant you access.",
	"pipelines.errors.unknown": "An unexpected error occurred. Please try again.",
	"pipelines.stream.navigation.http_ingest": "HTTP Ingest",
	"pipelines.stream.navigation.workers_ingest": "Workers Ingest",
	"pipelines.stream.navigation.input_schema": "Input Schema",
	"pipelines.stream.http_ingest.title": "HTTP Ingest",
	"pipelines.stream.http_ingest.description": "Configure HTTP endpoint settings for ingesting data into this stream.",
	"pipelines.stream.http_ingest.disabled.content": "HTTP ingestion is currently disabled for this stream. Enable it to allow HTTP requests to send data to your stream.",
	"pipelines.stream.http_ingest.endpoint": "HTTP Endpoint",
	"pipelines.stream.http_ingest.authentication": "Authentication",
	"pipelines.stream.http_ingest.authentication.not_required": "Not Required",
	"pipelines.stream.http_ingest.cors_origins": "CORS Origins",
	"pipelines.stream.http_ingest.cors_origins.none": "No origins configured",
	"pipelines.stream.http_ingest.drawer.title": "Edit HTTP Ingest Settings",
	"pipelines.stream.http_ingest.authentication.label": "Require Authentication",
	"pipelines.stream.http_ingest.authentication.description": "When enabled, requests must include valid authentication headers to send data to this stream.",
	"pipelines.stream.http_ingest.cors_origins.label": "CORS Origins",
	"pipelines.stream.http_ingest.cors_origins.description": "Specify origins that can send cross-origin requests to this stream. Leave empty to allow all origins.",
	"pipelines.stream.http_ingest.cors_origins.placeholder": "https://example.com",
	"pipelines.stream.http_ingest.cors_origins.error.invalid_url": "Please enter valid HTTP or HTTPS URLs",
	"pipelines.stream.workers_ingest.title": "Workers Ingest",
	"pipelines.stream.workers_ingest.description": "Workers can always send data to streams through bindings. No additional configuration is required.",
	"pipelines.stream.workers_ingest.content": "Copy binding configuration and code to use in your Worker.",
	"pipelines.stream.workers_ingest.copy_binding": "Copy Binding",
	"pipelines.stream.schema.title": "Input Schema",
	"pipelines.stream.schema.description": "The schema defines the structure of data accepted by this stream. This schema is read-only.",
	"pipelines.stream.schema.empty.content": "No schema is currently defined for this stream. Data will be accepted in any format.",
	"pipelines.stream.schema.field_name": "Field Name",
	"pipelines.stream.schema.type": "Type",
	"pipelines.stream.schema.unit_items": "Unit/Items",
	"pipelines.stream.schema.required": "Required",
	"pipelines.sink.navigation.destination": "Destination",
	"pipelines.sink.navigation.batching": "Batching",
	"pipelines.sink.navigation.format": "Format",
	"pipelines.sink.destination.title": "Destination",
	"pipelines.sink.destination.description": "Configuration for where data is stored in R2.",
	"pipelines.sink.destination.type": "Type",
	"pipelines.sink.destination.bucket": "Bucket",
	"pipelines.sink.destination.path": "Path",
	"pipelines.sink.destination.partitioning": "Partitioning",
	"pipelines.sink.destination.namespace": "Namespace",
	"pipelines.sink.destination.table_name": "Table Name",
	"pipelines.sink.batching.title": "Batching",
	"pipelines.sink.batching.description": "Defines how data is grouped into files based on size or time limits before being written.",
	"pipelines.sink.batching.file_size": "File Size",
	"pipelines.sink.batching.file_size.tooltip": "Target file size before creating a new file.",
	"pipelines.sink.batching.interval": "Interval",
	"pipelines.sink.batching.interval.tooltip": "Amount of time before creating a new file.",
	"pipelines.sink.format.title": "Format",
	"pipelines.sink.format.description": "Configuration for how data is formatted and compressed.",
	"pipelines.sink.format.type": "Format",
	"pipelines.sink.format.compression": "Compression",
	"pipelines.sink.format.target_row_group_size": "Target Row Group Size",
	"pipelines.legacy.label": "Legacy",
	"pipelines.legacy.tooltip": "This pipeline was created with the previous version of the Pipelines API and has limited functionality.",
	"pipelines.enable": "Enable Pipelines",
	"pipelines.pricing": "Pipelines is currently only available with the Workers Paid plan. Upgrade your plan to start building data pipelines.",
	"pipelines.stream.settings.delete.action": "Delete stream",
	"pipelines.stream.settings.delete.modal.title": "Delete stream",
	"pipelines.stream.settings.delete.modal.content.warning": "This will permanently delete the stream **%{streamName}**. This action cannot be undone.",
	"pipelines.stream.settings.delete.error": "Failed to delete stream. Please try again.",
	"pipelines.sink.settings.delete.action": "Delete sink",
	"pipelines.sink.settings.delete.modal.title": "Delete sink",
	"pipelines.sink.settings.delete.modal.content.warning": "This will permanently delete the sink **%{sinkName}**. This action cannot be undone.",
	"pipelines.sink.settings.delete.error": "Failed to delete sink. Please try again.",
	"pipelines.create.steps.connect_stream": "Connect to a Stream",
	"pipelines.create.steps.define_schema": "Define Input Schema",
	"pipelines.create.steps.define_sink": "Define Sink",
	"pipelines.create.steps.credentials": "Credentials",
	"pipelines.create.steps.pipeline_definition": "Pipeline Definition",
	"pipelines.create.connect_stream.title": "Create a new pipeline",
	"pipelines.create.connect_stream.description": "Configure a complete pipeline including a stream, sink, and SQL transformation.",
	"pipelines.create.inputs.pipeline_name.label": "Pipeline name",
	"pipelines.create.inputs.pipeline_name.placeholder": "analytics",
	"pipelines.create.inputs.pipeline_name.description": "This name will also be used as a prefix for your stream and sink (e.g., analytics_stream and analytics_sink).",
	"pipelines.create.http_endpoint.label": "Enable HTTP endpoint for sending data",
	"pipelines.create.http_auth.label": "HTTP Authentication",
	"pipelines.create.http_auth.tooltip": "Require authentication for HTTP endpoint",
	"pipelines.create.http_auth.placeholder": "Select authentication",
	"pipelines.create.http_auth.description": "When enabled, requests must include a Bearer token with 'Pipelines Edit' permission in the Authorization header. To learn more, refer to",
	"pipelines.create.http_auth.link_text": "Create API token",
	"pipelines.create.schema.title": "Define Input Schema",
	"pipelines.create.schema.description": "Define the structure of events coming from your data stream (Workers or HTTP endpoints).",
	"pipelines.create.schema.visual_editor": "Visual Editor",
	"pipelines.create.schema.json_editor": "JSON Editor",
	"pipelines.create.schema.unstructured": "Unstructured",
	"pipelines.create.schema.field_placeholder": "field_name",
	"pipelines.create.schema.add_nested_field": "Add nested field",
	"pipelines.create.schema.delete_field": "Delete field",
	"pipelines.create.schema.add_field": "Add Field",
	"pipelines.create.schema.list_struct_fields": "u2514 List Struct Fields",
	"pipelines.create.schema.unstructured.description": "Streams configured with an unstructured schema have a single value column with the JSON value.",
	"pipelines.create.sink.title": "Define Sink",
	"pipelines.create.sink.description": "Configure storage settings for your pipeline data.",
	"pipelines.create.sink.r2_bucket.label": "R2 Bucket",
	"pipelines.create.sink.r2_bucket.description": "Select an R2 bucket where your pipeline will write data",
	"pipelines.create.sink.storage_type.label": "Storage Type",
	"pipelines.create.sink.namespace.placeholder": "default",
	"pipelines.create.sink.namespace.description": "Catalog namespace for organizing tables",
	"pipelines.create.sink.table_name.placeholder": "my_table",
	"pipelines.create.sink.table_name.description": "Name of the table in the data catalog",
	"pipelines.create.sink.advanced_settings": "Advanced Settings",
	"pipelines.create.sink.parquet_options": "Parquet Options",
	"pipelines.create.sink.compression.placeholder": "Select compression",
	"pipelines.create.sink.row_group_size.placeholder": "128",
	"pipelines.create.sink.rolling_policy": "Rolling Policy",
	"pipelines.create.sink.max_file_size.label": "Maximum File Size",
	"pipelines.create.sink.max_file_size.placeholder": "100",
	"pipelines.create.sink.max_file_size.description": "Files close when reaching this size",
	"pipelines.create.sink.max_time_interval.label": "Maximum Time Interval",
	"pipelines.create.sink.max_time_interval.placeholder": "300",
	"pipelines.create.sink.max_time_interval.description": "Files close after this duration",
	"pipelines.create.sink.file_naming": "File Naming",
	"pipelines.create.sink.file_path.label": "File Path",
	"pipelines.create.sink.file_path.placeholder": "Optional path (e.g., my-path)",
	"pipelines.create.sink.file_path.description": "The base prefix in your bucket where data will be written",
	"pipelines.create.sink.time_partition.label": "Time Partition Pattern",
	"pipelines.create.sink.time_partition.placeholder": "year=%Y/month=%m/day=%d",
	"pipelines.create.sink.time_partition.description": "Use strftime format codes (e.g., %Y for year, %m for month)",
	"pipelines.create.credentials.title": "Configure Credentials",
	"pipelines.create.credentials.description": "Set up service credentials needed for your sink to write to R2.",
	"pipelines.create.credentials.description_catalog": "Set up service credentials needed for your sink to write to Data Catalog.",
	"pipelines.create.credentials.auto_create.label": "Automatically create an Account API token for your sink",
	"pipelines.create.credentials.auto_create.r2_description": "We'll create a token with write access to \"%{bucketName}\"",
	"pipelines.create.credentials.auto_create.catalog_description": "We'll create a token with Data Catalog write permissions",
	"pipelines.create.credentials.catalog_token.label": "Catalog Token",
	"pipelines.create.credentials.catalog_token.placeholder": "Enter your Data Catalog API token",
	"pipelines.create.credentials.catalog_token.description": "API token with R2 object write and Data Catalog write permissions",
	"pipelines.create.credentials.access_key_id.label": "Access Key ID",
	"pipelines.create.credentials.access_key_id.placeholder": "Enter your R2 Access Key ID",
	"pipelines.create.credentials.access_key_id.description": "The Access Key ID for your R2 credentials",
	"pipelines.create.credentials.secret_access_key.label": "Secret Access Key",
	"pipelines.create.credentials.secret_access_key.placeholder": "Enter your R2 Secret Access Key",
	"pipelines.create.credentials.secret_access_key.description": "The Secret Access Key for your R2 credentials",
	"pipelines.create.credentials.creating_token": "Creating token...",
	"pipelines.create.definition.title": "Pipeline Definition",
	"pipelines.create.definition.description": "Define how your pipeline will transform and route data.",
	"pipelines.create.definition.sink_schema_inferred": "Sink schema inferred by SQL query",
	"pipelines.create.definition.sql_placeholder": "Enter your SQL transformation...",
	"pipelines.create.definition.sql_validation_errors": "SQL Validation Errors:",
	"pipelines.create.definition.creating": "Creating...",
	"pipelines.create.definition.creating_stream": "Creating Stream...",
	"pipelines.create.definition.creating_sink": "Creating Sink...",
	"pipelines.create.definition.validating_sql": "Validating SQL...",
	"pipelines.create.definition.creating_pipeline": "Creating Pipeline...",
	"pipelines.create.r2_bucket.load_error": "Failed to load R2 buckets. Please try again.",
	"pipelines.create.r2_bucket.placeholder": "Select a bucket",
	"pipelines.create.r2_bucket.empty": "No R2 buckets found.",
	"pipelines.create.r2_bucket.required": "Please select an R2 bucket",
	"pipelines.create.storage_types.json.description": "Human-readable, widely supported",
	"pipelines.create.storage_types.parquet.description": "Compressed, optimized for analytics",
	"pipelines.create.storage_types.catalog.description": "Managed Iceberg tables",
	"pipelines.create.units.megabytes": "MB",
	"pipelines.create.units.seconds": "sec"
}